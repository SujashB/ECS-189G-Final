{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing Adversarial Robustness of Nano GPT\n",
        "\n",
        "### ECS 189G Final Project\n",
        "\n",
        "#### Sujash Barman, Jeffrey Wang, Dehui Chen"
      ],
      "metadata": {
        "id": "Ss09IxsiWFQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with DailyDialog"
      ],
      "metadata": {
        "id": "mmPZQ6Cp13Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace this line with your own directory\n",
        "cwd = \"/content/drive/MyDrive/ECS 189G/\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import math\n",
        "import inspect\n",
        "import tiktoken\n",
        "from contextlib import nullcontext\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "# settings\n",
        "out_dir = os.path.join(cwd, \"out\")\n",
        "eval_interval = 250 # keep frequent because we'll overfit\n",
        "eval_iters = 200\n",
        "log_interval = 10 # don't print too too often\n",
        "\n",
        "always_save_checkpoint = False # we expect to overfit on this small dataset, so only save when val improves\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "\n",
        "wandb_log = False # override via command line if you like\n",
        "wandb_project = 'dial-char'\n",
        "wandb_run_name = 'mini-gpt'\n",
        "\n",
        "dataset = 'daily-dialog'\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64\n",
        "block_size = 256 # context of up to 256 previous characters\n",
        "\n",
        "# baby GPT model :)\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "bias = False\n",
        "\n",
        "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
        "max_iters = 5000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 100 # not super necessary potentially\n",
        "lr_decay_iters = 5000 # make equal to max_iters usually\n",
        "min_lr = 1e-4 # learning_rate / 10 usually\n",
        "\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster"
      ],
      "metadata": {
        "id": "64jL5tQ8a-5U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architecture"
      ],
      "metadata": {
        "id": "9JWInvkl15Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "          wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "          wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "          drop = nn.Dropout(config.dropout),\n",
        "          # Use ARTM‐augmented blocks here:\n",
        "          h   = nn.ModuleList([TransformerBlockWithARTM(config) for _ in range(config.n_layer)]),\n",
        "          ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "class GPTForClassification(GPT):\n",
        "    \"\"\"\n",
        "    Extends NanoGPT so we can use it as a classifier (SST-2 / QQP / MNLI).\n",
        "    We simply take the hidden state at the final position (T-1) and\n",
        "    add a tiny linear head to predict num_labels classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: GPTConfig, num_labels: int):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.classifier = nn.Linear(config.n_embd, num_labels, bias=True)\n",
        "        # Initialize classification head\n",
        "        nn.init.normal_(self.classifier.weight, mean=0.0, std=0.02)\n",
        "        if self.classifier.bias is not None:\n",
        "            nn.init.zeros_(self.classifier.bias)\n",
        "\n",
        "    def forward(self, idx, labels=None):\n",
        "        \"\"\"\n",
        "        idx: LongTensor, shape (B, T)\n",
        "        labels: LongTensor (B,) or None\n",
        "        Returns:\n",
        "          - if labels is None: (logits, None)\n",
        "          - if labels is provided: (logits, loss)\n",
        "        \"\"\"\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        # 1) Token + Positional Embeddings\n",
        "        tok_emb = self.transformer.wte(idx)        # (B, T, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)        # (T, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        # 2) Pass through Transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)                            # (B, T, n_embd)\n",
        "\n",
        "        x = self.transformer.ln_f(x)               # (B, T, n_embd)\n",
        "\n",
        "        # 3) Pool hidden state at final position (T-1)\n",
        "        pooled = x[:, -1, :]                       # (B, n_embd)\n",
        "\n",
        "        # 4) Classification head\n",
        "        logits = self.classifier(pooled)           # (B, num_labels)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels),\n",
        "                            labels.view(-1))\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "# ----------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "D5Am5pty15Gi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_head_coherence_scores(attn_layer: CausalSelfAttention, x_norm: torch.Tensor, pos_idx: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Placeholder for per‐position “head coherence” features.\n",
        "    In practice, you would extract the raw attention weights inside attn_layer\n",
        "    (e.g. by modifying CausalSelfAttention to store 'att' before softmax). Here we return zeros.\n",
        "\n",
        "    Args:\n",
        "      attn_layer: instance of CausalSelfAttention (already invoked on x_norm)\n",
        "      x_norm:      normalized pre-attention input (shape: B, T, n_embd)\n",
        "      pos_idx:     integer index t for which we want features\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor of shape (B, head_feat_dim). We use head_feat_dim=1 for simplicity.\n",
        "    \"\"\"\n",
        "    B, T, C = x_norm.size()\n",
        "    # Example: just return a zero scalar per batch element\n",
        "    return torch.zeros(B, 1, device=x_norm.device)"
      ],
      "metadata": {
        "id": "JD-7HvTjYRlm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlockWithARTM(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer block that includes the Adversarial-Repair Thinking Module (ARTM)\n",
        "    right after the masked MHSA output.\n",
        "\n",
        "    ARTM inspects both:\n",
        "      - h_att = MHSA(LayerNorm(x_in))\n",
        "      - z_norm = LayerNorm(x_in + pos_emb)\n",
        "\n",
        "    and emits a correction Δh for each position.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # LayerNorm before attention\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        # Causal masked self-attention (possibly PAAF-patched already)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        # LayerNorm before MLP\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        # MLP feed-forward\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "        # Compute feature dimension for ARTM_FFN:\n",
        "        #   - z_norm[t] (n_embd)\n",
        "        #   - h_att[t]  (n_embd)\n",
        "        #   - d_prev (1)\n",
        "        #   - d_next (1)\n",
        "        #   - head_coh (1, or more if you extract per-head features)\n",
        "        feat_dim = 2 * config.n_embd + 3  # last “3” = (d_prev, d_next, head_coh_dim=1)\n",
        "        self.artm_ffn = ARTM_FFN(config.n_embd, feat_dim)\n",
        "\n",
        "        # Positional embedding is stored in GPT's `wpe`, so we will pull from there.\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, n_embd) input to this block\n",
        "        returns: x_out = x + h_corr + MLP(LN(x + h_corr))\n",
        "        \"\"\"\n",
        "        B, T, C = x.size()\n",
        "        device = x.device\n",
        "\n",
        "        # 1) Pre-attention LayerNorm\n",
        "        x_norm = self.ln_1(x)  # (B, T, C)\n",
        "\n",
        "        # 2) Masked self-attention\n",
        "        h_att = self.attn(x_norm)  # (B, T, C)\n",
        "\n",
        "        # 3) Positional-encoded input and its normalization\n",
        "        #    We need the positional embeddings from the parent GPT model.\n",
        "        #    Assuming the parent GPT made `pos_emb = self.wpe(pos_idx)` earlier,\n",
        "        #    we just reconstruct here:\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device)  # (T,)\n",
        "        pos_emb = self.attn.c_attn.weight.new_zeros(B, T, C)\n",
        "        # Actually, we need to reach back into the GPT module to get wpe:\n",
        "        # But in a typical usage, GPT’s forward has already done: z = tok_emb + pos_emb.\n",
        "        # So here we assume x (the input) already includes pos_emb. If not, you can accept pos_emb via closure.\n",
        "\n",
        "        # For clarity: assume x was computed as (tok_emb + pos_emb) before passing to this block.\n",
        "        z = x  # if x = (tok_emb + pos_emb) downstream from GPT’s embedding + dropout\n",
        "        z_norm = LayerNorm(C, bias=self.config.bias)(z)  # (B, T, C)\n",
        "\n",
        "        # 4) ARTM: compute Δh for each position\n",
        "        #    We will build a feature tensor of shape (B, T, feat_dim)\n",
        "        #    then apply artm_ffn in one shot.\n",
        "        #    To do so, we need to gather d_prev, d_next, and head_coh.\n",
        "\n",
        "        # 4a) Compute d_prev & d_next:\n",
        "        #     Create two tensors of shape (B, T, 1)\n",
        "        d_prev = torch.zeros(B, T, 1, device=device)\n",
        "        d_next = torch.zeros(B, T, 1, device=device)\n",
        "\n",
        "        if T > 1:\n",
        "            # distance between z_norm[:, t] and z_norm[:, t-1]\n",
        "            d_prev[:, 1:, 0] = torch.norm(\n",
        "                z_norm[:, 1:, :] - z_norm[:, :-1, :],\n",
        "                dim=-1\n",
        "            )\n",
        "            # distance between z_norm[:, t] and z_norm[:, t+1]\n",
        "            d_next[:, :-1, 0] = torch.norm(\n",
        "                z_norm[:, :-1, :] - z_norm[:, 1:, :],\n",
        "                dim=-1\n",
        "            )\n",
        "        # 4b) Compute head_coh for all positions at once\n",
        "        #     We will simply call compute_head_coherence_scores in a loop for each t,\n",
        "        #     because typically head_coh is scalar per position.\n",
        "\n",
        "        head_coh = torch.zeros(B, T, 1, device=device)\n",
        "        # If you want to vectorize, implement a batch-version of compute_head_coherence_scores.\n",
        "        for t in range(T):\n",
        "            # Pass both x_norm and x_norm (or Q,K) to extract head features at t\n",
        "            head_coh[:, t, :] = compute_head_coherence_scores(self.attn, x_norm, t)\n",
        "\n",
        "        # 4c) Concatenate features along last dim: (B, T, 2*C + 3)\n",
        "        feat = torch.cat([\n",
        "            z_norm,         # (B, T, C)\n",
        "            h_att,          # (B, T, C)\n",
        "            d_prev,         # (B, T, 1)\n",
        "            d_next,         # (B, T, 1)\n",
        "            head_coh        # (B, T, 1)\n",
        "        ], dim=-1)           # results in (B, T, 2*C + 3)\n",
        "\n",
        "        # 4d) Run through ARTM_FFN to get Δh: (B, T, C)\n",
        "        delta_h = self.artm_ffn(feat)  # (B, T, C)\n",
        "\n",
        "        # 4e) Corrected attention output\n",
        "        h_corr = h_att + delta_h  # (B, T, C)\n",
        "\n",
        "        # 5) Residual + MLP path\n",
        "        x1 = x + h_corr            # (B, T, C)\n",
        "        x2 = x1 + self.mlp(self.ln_2(x1))  # (B, T, C)\n",
        "\n",
        "        return x2"
      ],
      "metadata": {
        "id": "96QROaq7YSXl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing Training Data"
      ],
      "metadata": {
        "id": "ahPcRfhSI6d4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K6Zzj_6dnb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77ea5d5-ef71-4141-ba18-58eeda6babe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 5,319,059\n",
            "all the unique characters: \n",
            " !\"#$%&'()*+,-./0123456789:;=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ\\_abcdefghijklmnopqrstuvwxyz~£¥°–—‘’“”′、。\n",
            "vocab size: 100\n",
            "train has 4,787,153 tokens\n",
            "val has 531,906 tokens\n"
          ]
        }
      ],
      "source": [
        "# download the dataset\n",
        "input_file_path = os.path.join(cwd, 'dial.txt')\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(cwd, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(cwd, 'val.bin'))\n",
        "\n",
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open(os.path.join(cwd, 'meta.pkl'), 'wb') as f:\n",
        "    pickle.dump(meta, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulfAzOqZGSp9",
        "outputId": "f0bb0f63-d347-4ece-d6ed-9ab3a004c455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop"
      ],
      "metadata": {
        "id": "s3n7v7BicHeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# poor man's data loader\n",
        "data_dir = cwd\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * (it + 1) / (warmup_iters + 1)\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "gITA15grcHFz",
        "outputId": "91f52b17-1c51-4020-8bc5-5bb89116fe98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 16,384\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 32.60M\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-409b3010e75f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_block_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'block_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_size\u001b[0m \u001b[0;31m# so that the checkpoint will have the right value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# initialize a GradScaler. If enabled=False scaler is a no-op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating Result Samples"
      ],
      "metadata": {
        "id": "plNHeCZQdGRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "elif init_from.startswith('gpt2'):\n",
        "    # init from a given GPT-2 model\n",
        "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "\n",
        "# look for the meta pickle in case it is available in the dataset folder\n",
        "load_meta = False\n",
        "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
        "    meta_path = os.path.join(cwd, 'meta.pkl')\n",
        "    load_meta = os.path.exists(meta_path)\n",
        "if load_meta:\n",
        "    print(f\"Loading meta from {meta_path}...\")\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
        "    stoi, itos = meta['stoi'], meta['itos']\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "else:\n",
        "    # ok let's assume gpt-2 encodings by default\n",
        "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "    decode = lambda l: enc.decode(l)\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehGjxj16dIyf",
        "outputId": "d922defb-af31-4d6b-97a7-b7c529c367c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 10.66M\n",
            "Loading meta from /content/drive/MyDrive/ECS 189G/meta.pkl...\n",
            "\n",
            "What is the matter him going ?\n",
            "It is a big problem with my battroom .\n",
            "How unexpecting ?\n",
            "It ’ s the matter of the lottery .\n",
            "That ’ s a good idea . Can you come in ?\n",
            "I'd like to come in at the park at 2:00 .\n",
            "Is there a battery account ?\n",
            "I want a car in the bad location .\n",
            "You can call the table for lunch ?\n",
            "Yes , that would be fine .\n",
            "Good luck . Can I help you ?\n",
            "Yes , that would be great ! Thanks . Taxi .\n",
            "And no problem .\n",
            "That's great .\n",
            "I'm from China and I need a Promise .\n",
            "You don't mind saying tha\n",
            "---------------\n",
            "\n",
            "Really ? Really ? I must have been working for it .\n",
            "I appreciate that .\n",
            "That was a private cover .\n",
            "John , do you have any other covers ?\n",
            "Yes . For example , a single cover can give me a call from the next step , and I would appreciate you to make my country .\n",
            "Ok , I ’ ll teach you on . When do you think you ’ re awful now ?\n",
            "Well , I ’ ll change it .\n",
            "How long will it take you to the computer ?\n",
            "It is the highest new one .\n",
            "OK .\n",
            "Sorry , I can ’ t . I ’ ll take it .\n",
            "Thanks for your help .\n",
            "I ’ ll take\n",
            "---------------\n",
            "\n",
            "It seems that you have really expected ? Thank you for the position .\n",
            "How often do you like this one ?\n",
            "I like public transactions . I have to pay in an interview as you like .\n",
            "What can I do for you ?\n",
            "I am calling to tell you the entrance reading exam to pay the made .\n",
            "I am sorry to hear that . Can I help you ?\n",
            "Do you have anything to declare ?\n",
            "Yes . I made the decision where you want to get to that exactly .\n",
            "When can I pay for you ?\n",
            "I want to transfer you to the main restaurant as to which you w\n",
            "---------------\n",
            "\n",
            "Look at the new section .\n",
            "Yes , ma'am . After all , the interests of the section should be successful . I'll probably certainly open you our calendar .\n",
            "OK , thank you .\n",
            "Can I have my passport please ?\n",
            "Sure . Do you have any reservations ?\n",
            "Yes , I have . And this has a special reservation by the way of a reservation the vacation of Francis .\n",
            "OK . Here is the table , sir . Thanks for your help .\n",
            "How many tickets do I have ?\n",
            "2 hours . What is your name ? Where to ?\n",
            "My name is Brooke .\n",
            "I am so heady\n",
            "---------------\n",
            "\n",
            "What do I do for you ?\n",
            "Do you know if I can invite my bank ?\n",
            "I think I should be more interested in you .\n",
            "What about something to drink ?\n",
            "I don ’ t think so . The perfect new happy has a prentice private design on the subway traditional for many traditional industry .\n",
            "Why ?\n",
            "The most development is supposed to be a personal job . I think I should pay attention to the subway after that nightly . What do you think of mistake ?\n",
            "We like traditional profits , but as long as the match this afternoon is\n",
            "---------------\n",
            "\n",
            "The particular flight is much better than the conference of the novels .\n",
            "What if the particular just doesn ’ t make it to come up this afternoon ?\n",
            "Well , the documents are more than them . Perhaps for the film there is no attractive work , and then the mortgage more and work when you also have a good brand time-out the shop .\n",
            "Could you show me the next time ?\n",
            "Of course I can . It's just a shopping off on my way .\n",
            "I know . That way we can get a brain .\n",
            "The Olympic Games is a tour sick of mine lea\n",
            "---------------\n",
            "\n",
            "I'm afraid I haven't done yet .\n",
            "That's all right . What kind of personality do you like best ?\n",
            "I like it better than everybody .\n",
            "Can you tell me you the first time is ?\n",
            "Yes .\n",
            "I'll have this for my first time .\n",
            "I will take it .\n",
            "What do you pay for our company ?\n",
            "I want to provide the specialty of your money .\n",
            "Why don ’ t you come borrow ?\n",
            "Sure , I ’ ll give you a safe range .\n",
            "I could only go for dinner tonight .\n",
            "But then we ’ ll go just together .\n",
            "What do you think of him ?\n",
            "He's a salespect man .\n",
            "\n",
            "---------------\n",
            "\n",
            "So , sometimes the starting position is much better .\n",
            "I am just looking forward to seeing the main department take the rate of the rate .\n",
            "Great ! You are absolutely happy to be happy with that .\n",
            "I think it's true , isn't it ?\n",
            "Well , that's why you are lucky for that .\n",
            "Let me know what I need . I think I'll get the stop by the end of the way .\n",
            "So , I think you'll have to catch you on .\n",
            "Thank you very much .\n",
            "I ’ ll be back in the airport at the stop .\n",
            "It won ’ t be a good day . I ’ ll stay in the \n",
            "---------------\n",
            "\n",
            "She always really loves the sofa .\n",
            "My only thing has done some besides , and he doesn ’ t really match great too well .\n",
            "She is always encouraging her man .\n",
            "She is much more simple when she likes to get used to the spring man .\n",
            "You really like the weather in the seat , are you ? Peter , I mean that I can feel any things .\n",
            "I see . You ’ ll have to wait .\n",
            "That ’ s great . Everyone has gone a lot .\n",
            "I think you ’ re in such a football man .\n",
            "Are you kidding ?\n",
            "Yeah , I ’ m a fine deal , and I ’ ll be h\n",
            "---------------\n",
            "\n",
            "I ’ Ve got a regular price for half papers .\n",
            "What do you think of the job ?\n",
            "Oh , you ’ re right . I ’ m going to need a preparation .\n",
            "How about this one ?\n",
            "It ’ s really good .\n",
            "Let ’ s go to the office for a sec . You ’ ll have to go to the Station of the third States . I ’ ll also buy some information about the other programmers . Thank you very much .\n",
            "You are welcome . It'll be really expensive .\n",
            "What are you doing for ?\n",
            "I'm highly going to stay in with my mind , but I am not sure . I think you\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load AdvGLUE Dataset"
      ],
      "metadata": {
        "id": "-QVHCnS-MSoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "class AdvGLUEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads adversarial SST-2, QQP, and MNLI examples from a JSON of the form:\n",
        "      {\n",
        "        \"SST2\": [...],\n",
        "        \"QQP\":  [...],\n",
        "        \"MNLI\": [...]\n",
        "      }\n",
        "    Each example dict has fields:\n",
        "      - SST2: 'sentence', 'label'\n",
        "      - QQP:  'question1', 'question2', 'label'\n",
        "      - MNLI: 'premise', 'hypothesis', 'label'\n",
        "    We convert each to a single text string and a numeric label.\n",
        "    \"\"\"\n",
        "    def __init__(self, json_path, stoi, max_length=128):\n",
        "        super().__init__()\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = []\n",
        "        for task, examples in data.items():\n",
        "            for ex in examples:\n",
        "                if task.lower() == 'sst2':\n",
        "                    text = ex['sentence']\n",
        "                    label = ex['label']\n",
        "                elif task.lower() == 'qqp':\n",
        "                    text = ex['question1'] + \" [SEP] \" + ex['question2']\n",
        "                    label = ex['label']\n",
        "                elif task.lower() == 'mnli':\n",
        "                    text = ex['premise'] + \" [SEP] \" + ex['hypothesis']\n",
        "                    label = ex['label']  # 0,1,2\n",
        "                else:\n",
        "                    continue\n",
        "                self.samples.append({'text': text, 'label': label})\n",
        "\n",
        "        self.stoi = stoi\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.samples[idx]\n",
        "        text = ex['text']\n",
        "        label = ex['label']\n",
        "        # Convert to int IDs via the character‐level stoi (pad/truncate to max_length)\n",
        "        ids = [self.stoi.get(ch, self.stoi.get(' ', 0)) for ch in text][:self.max_length]\n",
        "        if len(ids) < self.max_length:\n",
        "            ids += [0] * (self.max_length - len(ids))\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# If cwd = \"/content/drive/MyDrive/YEAR 4/ECS 189G/\"\n",
        "# and your JSON is at \"/content/drive/MyDrive/YEAR 4/ECS 189G/dev_ann.json\", do:\n",
        "\n",
        "json_path = os.path.join(cwd, 'dev_ann.json')\n",
        "# e.g. \"/content/drive/MyDrive/YEAR 4/ECS 189G/dev_ann.json\"\n",
        "\n",
        "adv_dataset = AdvGLUEDataset(json_path, stoi, max_length=128)\n",
        "adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "ppM6BWsQMUuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shared Training Function"
      ],
      "metadata": {
        "id": "2fNN3rRXSNh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A: Shared training function to produce a checkpoint with optional LoRA/DAR/PAAF\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "def train_and_save_variant(\n",
        "    out_dir: str,\n",
        "    ckpt_filename: str,\n",
        "    use_lora: bool = False,\n",
        "    use_paaf: bool = False,\n",
        "    use_dar: bool = False,\n",
        "    dar_weight: float = 0.01,\n",
        "    lora_rank: int = 8,\n",
        "    lora_alpha: float = 16.0,\n",
        "    max_iters: int = 5000\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains NanoGPT from scratch (or from `init_from='scratch'`) with the given defenses,\n",
        "    and saves the final checkpoint to `os.path.join(out_dir, ckpt_filename)`.\n",
        "\n",
        "    - use_lora: wraps relevant Linear layers in LoRA adapters before training.\n",
        "    - use_paaf: assumes you have already applied the PAAF monkey-patch to CausalSelfAttention.forward earlier.\n",
        "    - use_dar: adds DAR penalty to the cross‐entropy loss.\n",
        "    - dar_weight: weight on DAR penalty.\n",
        "    - lora_rank / lora_alpha: LoRA hyperparameters.\n",
        "    - max_iters: number of training iterations.\n",
        "    \"\"\"\n",
        "    # 1) Create the model from scratch\n",
        "    model_args = dict(\n",
        "        n_layer=n_layer,\n",
        "        n_head=n_head,\n",
        "        n_embd=n_embd,\n",
        "        block_size=block_size,\n",
        "        bias=bias,\n",
        "        vocab_size=meta_vocab_size if meta_vocab_size is not None else 50304,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    model.to(device)\n",
        "\n",
        "    # 2) LoRA injection if requested\n",
        "    if use_lora:\n",
        "        print(f\"--> Injecting LoRA adapters (rank={lora_rank}, alpha={lora_alpha})\")\n",
        "        model = inject_lora(model, rank=lora_rank, alpha=lora_alpha)\n",
        "        model.to(device)   # <--- ensure LoRA parameters are also on GPU\n",
        "\n",
        "\n",
        "    # 3) If DDP, wrap in DDP (we assume single‐GPU here; adjust if multi‐GPU)\n",
        "    #    (Remove or comment-out if not using DDP)\n",
        "    ddp = False\n",
        "    if ddp:\n",
        "        init_process_group(backend=backend)\n",
        "        model = DDP(model, device_ids=[torch.cuda.current_device()])\n",
        "        master_process = (int(os.environ.get('RANK', -1)) == 0)\n",
        "    else:\n",
        "        master_process = True\n",
        "\n",
        "    # 4) Optimizer\n",
        "    optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "\n",
        "    # 5) GradScaler for fp16 (already defined as `scaler`)\n",
        "    scaler_local = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "    iter_num = 0\n",
        "    best_val_loss = 1e9\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 6) Training loop\n",
        "    while True:\n",
        "        # learning rate schedule\n",
        "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "        for pg in optimizer.param_groups:\n",
        "            pg['lr'] = lr\n",
        "\n",
        "        # Evaluate periodically\n",
        "        if iter_num % eval_interval == 0 and master_process:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"[{ckpt_filename}] iter {iter_num}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
        "            if losses['val'] < best_val_loss:\n",
        "                best_val_loss = losses['val']\n",
        "                if iter_num > 0:\n",
        "                    # Save intermediate checkpoint\n",
        "                    ckpt_path = os.path.join(out_dir, ckpt_filename)\n",
        "                    print(f\"[{ckpt_filename}] saving checkpoint at iter {iter_num} → {ckpt_path}\")\n",
        "                    torch.save({\n",
        "                        'model': (model.module if ddp else model).state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'model_args': model_args,\n",
        "                        'iter_num': iter_num,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                    }, ckpt_path)\n",
        "\n",
        "        if iter_num >= max_iters:\n",
        "            break\n",
        "\n",
        "        # 7) Forward/Backward\n",
        "        X, Y = get_batch('train')\n",
        "        for micro_step in range(gradient_accumulation_steps):\n",
        "            if ddp:\n",
        "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "            with ctx:\n",
        "                logits, _ = model(X, Y)\n",
        "                # compute loss: CE + optional DAR penalty\n",
        "                if use_dar:\n",
        "                    # DAR.attach_loss expects logits over entire vocab, so we need to re-forward if needed.\n",
        "                    # But here `model(X, Y)` already gives us logits over each position for LM loss.\n",
        "                    # Instead, do classification‐style DAR only on tokens: we keep it simple and\n",
        "                    # apply DAR penalty on LoRA A/B if present.\n",
        "                    ce = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
        "                    dar_pen = DAR.penalty(model)\n",
        "                    loss = ce + dar_weight * dar_pen\n",
        "                else:\n",
        "                    # standard LM cross-entropy\n",
        "                    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            X, Y = get_batch('train')\n",
        "            scaler_local.scale(loss).backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        if grad_clip != 0.0:\n",
        "            scaler_local.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_((model.module if ddp else model).parameters(), grad_clip)\n",
        "        scaler_local.step(optimizer)\n",
        "        scaler_local.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Logging\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "        t0 = t1\n",
        "        if iter_num % log_interval == 0 and master_process:\n",
        "            lossf = loss.item() * gradient_accumulation_steps\n",
        "            print(f\"[{ckpt_filename}] iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.1f}ms\")\n",
        "        iter_num += 1\n",
        "\n",
        "    # Final save (in case best checkpoint was never updated)\n",
        "    if master_process:\n",
        "        ckpt_path = os.path.join(out_dir, ckpt_filename)\n",
        "        print(f\"[{ckpt_filename}] final save at iter {iter_num} → {ckpt_path}\")\n",
        "        torch.save({\n",
        "            'model': (model.module if ddp else model).state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model_args': model_args,\n",
        "            'iter_num': iter_num,\n",
        "            'best_val_loss': best_val_loss,\n",
        "        }, ckpt_path)\n",
        "\n",
        "    if ddp:\n",
        "        destroy_process_group()\n",
        "    print(f\"[{ckpt_filename}] Training complete.\")\n"
      ],
      "metadata": {
        "id": "BqZA6jpsSQNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT (base)"
      ],
      "metadata": {
        "id": "ikuDofBvSR5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell B: NanoGPT (base) – no LoRA, no PAAF, no DAR\n",
        "\n",
        "train_and_save_variant(\n",
        "    out_dir    = out_dir,\n",
        "    ckpt_filename = 'ckpt_base.pt',\n",
        "    use_lora   = False,\n",
        "    use_paaf   = False,\n",
        "    use_dar    = False,\n",
        "    max_iters  = max_iters  # e.g. 5000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNeUUBWYSbr2",
        "outputId": "0baf1894-d1bb-4f4e-9d49-2c7c86e26f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 10.66M\n",
            "num decayed parameter tensors: 26, with 10,753,536 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-384b2adbcda7>:66: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_local = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ckpt_base.pt] iter 0: train 0.8653, val 0.9370\n",
            "[ckpt_base.pt] iter 0: loss 4.6271, time 12753.8ms\n",
            "[ckpt_base.pt] iter 10: loss 3.1541, time 96.4ms\n",
            "[ckpt_base.pt] iter 20: loss 2.7049, time 97.7ms\n",
            "[ckpt_base.pt] iter 30: loss 2.5169, time 96.9ms\n",
            "[ckpt_base.pt] iter 40: loss 2.4297, time 98.5ms\n",
            "[ckpt_base.pt] iter 50: loss 2.4171, time 98.2ms\n",
            "[ckpt_base.pt] iter 60: loss 2.3862, time 98.3ms\n",
            "[ckpt_base.pt] iter 70: loss 2.3493, time 96.5ms\n",
            "[ckpt_base.pt] iter 80: loss 2.3448, time 96.8ms\n",
            "[ckpt_base.pt] iter 90: loss 2.3766, time 96.4ms\n",
            "[ckpt_base.pt] iter 100: loss 2.3248, time 100.0ms\n",
            "[ckpt_base.pt] iter 110: loss 2.3424, time 97.5ms\n",
            "[ckpt_base.pt] iter 120: loss 2.3378, time 96.5ms\n",
            "[ckpt_base.pt] iter 130: loss 2.3284, time 100.2ms\n",
            "[ckpt_base.pt] iter 140: loss 2.2833, time 97.2ms\n",
            "[ckpt_base.pt] iter 150: loss 2.2480, time 98.6ms\n",
            "[ckpt_base.pt] iter 160: loss 2.2281, time 99.8ms\n",
            "[ckpt_base.pt] iter 170: loss 2.1584, time 99.4ms\n",
            "[ckpt_base.pt] iter 180: loss 2.0939, time 98.3ms\n",
            "[ckpt_base.pt] iter 190: loss 2.0512, time 97.5ms\n",
            "[ckpt_base.pt] iter 200: loss 1.9974, time 99.1ms\n",
            "[ckpt_base.pt] iter 210: loss 1.9567, time 98.5ms\n",
            "[ckpt_base.pt] iter 220: loss 1.9477, time 99.8ms\n",
            "[ckpt_base.pt] iter 230: loss 1.8942, time 98.7ms\n",
            "[ckpt_base.pt] iter 240: loss 1.8514, time 98.9ms\n",
            "[ckpt_base.pt] iter 250: train 0.8661, val 0.9365\n",
            "[ckpt_base.pt] saving checkpoint at iter 250 → /content/drive/MyDrive/ECS 189G/out/ckpt_base.pt\n",
            "[ckpt_base.pt] iter 250: loss 1.8419, time 12933.2ms\n",
            "[ckpt_base.pt] iter 260: loss 1.7941, time 96.7ms\n",
            "[ckpt_base.pt] iter 270: loss 1.7867, time 103.9ms\n",
            "[ckpt_base.pt] iter 280: loss 1.7917, time 100.4ms\n",
            "[ckpt_base.pt] iter 290: loss 1.7491, time 99.3ms\n",
            "[ckpt_base.pt] iter 300: loss 1.7675, time 99.9ms\n",
            "[ckpt_base.pt] iter 310: loss 1.7067, time 99.5ms\n",
            "[ckpt_base.pt] iter 320: loss 1.7020, time 102.6ms\n",
            "[ckpt_base.pt] iter 330: loss 1.6707, time 101.7ms\n",
            "[ckpt_base.pt] iter 340: loss 1.6633, time 99.6ms\n",
            "[ckpt_base.pt] iter 350: loss 1.6621, time 97.9ms\n",
            "[ckpt_base.pt] iter 360: loss 1.6106, time 98.1ms\n",
            "[ckpt_base.pt] iter 370: loss 1.6167, time 100.7ms\n",
            "[ckpt_base.pt] iter 380: loss 1.6164, time 81.4ms\n",
            "[ckpt_base.pt] iter 390: loss 1.6003, time 93.1ms\n",
            "[ckpt_base.pt] iter 400: loss 1.5749, time 68.7ms\n",
            "[ckpt_base.pt] iter 410: loss 1.5784, time 62.3ms\n",
            "[ckpt_base.pt] iter 420: loss 1.5484, time 100.5ms\n",
            "[ckpt_base.pt] iter 430: loss 1.5499, time 102.5ms\n",
            "[ckpt_base.pt] iter 440: loss 1.5878, time 100.5ms\n",
            "[ckpt_base.pt] iter 450: loss 1.5731, time 98.7ms\n",
            "[ckpt_base.pt] iter 460: loss 1.5244, time 101.2ms\n",
            "[ckpt_base.pt] iter 470: loss 1.5509, time 100.7ms\n",
            "[ckpt_base.pt] iter 480: loss 1.5276, time 101.8ms\n",
            "[ckpt_base.pt] iter 490: loss 1.4926, time 101.8ms\n",
            "[ckpt_base.pt] iter 500: train 0.8664, val 0.9359\n",
            "[ckpt_base.pt] saving checkpoint at iter 500 → /content/drive/MyDrive/ECS 189G/out/ckpt_base.pt\n",
            "[ckpt_base.pt] iter 500: loss 1.4901, time 13101.1ms\n",
            "[ckpt_base.pt] iter 510: loss 1.5068, time 100.3ms\n",
            "[ckpt_base.pt] iter 520: loss 1.4691, time 102.6ms\n",
            "[ckpt_base.pt] iter 530: loss 1.4586, time 99.8ms\n",
            "[ckpt_base.pt] iter 540: loss 1.4775, time 100.1ms\n",
            "[ckpt_base.pt] iter 550: loss 1.4563, time 102.3ms\n",
            "[ckpt_base.pt] iter 560: loss 1.4213, time 102.8ms\n",
            "[ckpt_base.pt] iter 570: loss 1.4468, time 102.4ms\n",
            "[ckpt_base.pt] iter 580: loss 1.4422, time 101.9ms\n",
            "[ckpt_base.pt] iter 590: loss 1.4478, time 101.9ms\n",
            "[ckpt_base.pt] iter 600: loss 1.4504, time 101.5ms\n",
            "[ckpt_base.pt] iter 610: loss 1.3977, time 102.0ms\n",
            "[ckpt_base.pt] iter 620: loss 1.4096, time 99.4ms\n",
            "[ckpt_base.pt] iter 630: loss 1.3999, time 97.5ms\n",
            "[ckpt_base.pt] iter 640: loss 1.3865, time 101.7ms\n",
            "[ckpt_base.pt] iter 650: loss 1.3965, time 101.9ms\n",
            "[ckpt_base.pt] iter 660: loss 1.3656, time 102.1ms\n",
            "[ckpt_base.pt] iter 670: loss 1.3854, time 101.0ms\n",
            "[ckpt_base.pt] iter 680: loss 1.4161, time 101.6ms\n",
            "[ckpt_base.pt] iter 690: loss 1.3771, time 102.0ms\n",
            "[ckpt_base.pt] iter 700: loss 1.3717, time 100.7ms\n",
            "[ckpt_base.pt] iter 710: loss 1.3519, time 101.9ms\n",
            "[ckpt_base.pt] iter 720: loss 1.3476, time 102.3ms\n",
            "[ckpt_base.pt] iter 730: loss 1.3282, time 101.2ms\n",
            "[ckpt_base.pt] iter 740: loss 1.3435, time 102.0ms\n",
            "[ckpt_base.pt] iter 750: train 0.8660, val 0.9346\n",
            "[ckpt_base.pt] saving checkpoint at iter 750 → /content/drive/MyDrive/ECS 189G/out/ckpt_base.pt\n",
            "[ckpt_base.pt] iter 750: loss 1.3647, time 13109.0ms\n",
            "[ckpt_base.pt] iter 760: loss 1.3334, time 102.1ms\n",
            "[ckpt_base.pt] iter 770: loss 1.3467, time 88.5ms\n",
            "[ckpt_base.pt] iter 780: loss 1.2987, time 102.8ms\n",
            "[ckpt_base.pt] iter 790: loss 1.3036, time 65.9ms\n",
            "[ckpt_base.pt] iter 800: loss 1.3464, time 101.3ms\n",
            "[ckpt_base.pt] iter 810: loss 1.3370, time 100.8ms\n",
            "[ckpt_base.pt] iter 820: loss 1.2842, time 102.4ms\n",
            "[ckpt_base.pt] iter 830: loss 1.2912, time 102.3ms\n",
            "[ckpt_base.pt] iter 840: loss 1.2988, time 101.7ms\n",
            "[ckpt_base.pt] iter 850: loss 1.2624, time 101.6ms\n",
            "[ckpt_base.pt] iter 860: loss 1.2924, time 101.4ms\n",
            "[ckpt_base.pt] iter 870: loss 1.2752, time 43.6ms\n",
            "[ckpt_base.pt] iter 880: loss 1.2820, time 100.9ms\n",
            "[ckpt_base.pt] iter 890: loss 1.2936, time 100.3ms\n",
            "[ckpt_base.pt] iter 900: loss 1.2782, time 101.2ms\n",
            "[ckpt_base.pt] iter 910: loss 1.2634, time 101.1ms\n",
            "[ckpt_base.pt] iter 920: loss 1.2673, time 101.6ms\n",
            "[ckpt_base.pt] iter 930: loss 1.2365, time 102.0ms\n",
            "[ckpt_base.pt] iter 940: loss 1.2431, time 102.0ms\n",
            "[ckpt_base.pt] iter 950: loss 1.2512, time 102.1ms\n",
            "[ckpt_base.pt] iter 960: loss 1.2437, time 102.3ms\n",
            "[ckpt_base.pt] iter 970: loss 1.2518, time 101.0ms\n",
            "[ckpt_base.pt] iter 980: loss 1.2649, time 101.5ms\n",
            "[ckpt_base.pt] iter 990: loss 1.2333, time 101.4ms\n",
            "[ckpt_base.pt] iter 1000: train 0.8655, val 0.9368\n",
            "[ckpt_base.pt] iter 1000: loss 1.2368, time 14093.8ms\n",
            "[ckpt_base.pt] iter 1010: loss 1.2541, time 103.5ms\n",
            "[ckpt_base.pt] iter 1020: loss 1.2601, time 101.8ms\n",
            "[ckpt_base.pt] iter 1030: loss 1.2707, time 101.9ms\n",
            "[ckpt_base.pt] iter 1040: loss 1.2475, time 101.8ms\n",
            "[ckpt_base.pt] iter 1050: loss 1.2157, time 102.7ms\n",
            "[ckpt_base.pt] iter 1060: loss 1.1895, time 101.7ms\n",
            "[ckpt_base.pt] iter 1070: loss 1.2164, time 100.2ms\n",
            "[ckpt_base.pt] iter 1080: loss 1.2345, time 100.8ms\n",
            "[ckpt_base.pt] iter 1090: loss 1.1994, time 102.4ms\n",
            "[ckpt_base.pt] iter 1100: loss 1.2104, time 103.3ms\n",
            "[ckpt_base.pt] iter 1110: loss 1.2313, time 103.4ms\n",
            "[ckpt_base.pt] iter 1120: loss 1.2195, time 102.1ms\n",
            "[ckpt_base.pt] iter 1130: loss 1.2145, time 102.6ms\n",
            "[ckpt_base.pt] iter 1140: loss 1.2409, time 102.5ms\n",
            "[ckpt_base.pt] iter 1150: loss 1.1997, time 103.2ms\n",
            "[ckpt_base.pt] iter 1160: loss 1.2011, time 101.9ms\n",
            "[ckpt_base.pt] iter 1170: loss 1.2438, time 102.8ms\n",
            "[ckpt_base.pt] iter 1180: loss 1.1733, time 102.9ms\n",
            "[ckpt_base.pt] iter 1190: loss 1.1857, time 104.8ms\n",
            "[ckpt_base.pt] iter 1200: loss 1.2083, time 102.3ms\n",
            "[ckpt_base.pt] iter 1210: loss 1.1815, time 107.0ms\n",
            "[ckpt_base.pt] iter 1220: loss 1.2325, time 103.5ms\n",
            "[ckpt_base.pt] iter 1230: loss 1.2049, time 103.9ms\n",
            "[ckpt_base.pt] iter 1240: loss 1.1666, time 103.8ms\n",
            "[ckpt_base.pt] iter 1250: train 0.8666, val 0.9343\n",
            "[ckpt_base.pt] saving checkpoint at iter 1250 → /content/drive/MyDrive/ECS 189G/out/ckpt_base.pt\n",
            "[ckpt_base.pt] iter 1250: loss 1.1609, time 13517.6ms\n",
            "[ckpt_base.pt] iter 1260: loss 1.1590, time 113.8ms\n",
            "[ckpt_base.pt] iter 1270: loss 1.1681, time 111.2ms\n",
            "[ckpt_base.pt] iter 1280: loss 1.1748, time 109.1ms\n",
            "[ckpt_base.pt] iter 1290: loss 1.1928, time 111.6ms\n",
            "[ckpt_base.pt] iter 1300: loss 1.1866, time 105.9ms\n",
            "[ckpt_base.pt] iter 1310: loss 1.1426, time 108.0ms\n",
            "[ckpt_base.pt] iter 1320: loss 1.1689, time 109.3ms\n",
            "[ckpt_base.pt] iter 1330: loss 1.1509, time 110.4ms\n",
            "[ckpt_base.pt] iter 1340: loss 1.1735, time 110.1ms\n",
            "[ckpt_base.pt] iter 1350: loss 1.1629, time 112.1ms\n",
            "[ckpt_base.pt] iter 1360: loss 1.1685, time 48.1ms\n",
            "[ckpt_base.pt] iter 1370: loss 1.1504, time 117.0ms\n",
            "[ckpt_base.pt] iter 1380: loss 1.1804, time 108.7ms\n",
            "[ckpt_base.pt] iter 1390: loss 1.1268, time 110.6ms\n",
            "[ckpt_base.pt] iter 1400: loss 1.1730, time 110.2ms\n",
            "[ckpt_base.pt] iter 1410: loss 1.1514, time 111.8ms\n",
            "[ckpt_base.pt] iter 1420: loss 1.1432, time 112.9ms\n",
            "[ckpt_base.pt] iter 1430: loss 1.1507, time 112.8ms\n",
            "[ckpt_base.pt] iter 1440: loss 1.1516, time 113.3ms\n",
            "[ckpt_base.pt] iter 1450: loss 1.1117, time 109.7ms\n",
            "[ckpt_base.pt] iter 1460: loss 1.1379, time 113.2ms\n",
            "[ckpt_base.pt] iter 1470: loss 1.1826, time 110.9ms\n",
            "[ckpt_base.pt] iter 1480: loss 1.1262, time 107.6ms\n",
            "[ckpt_base.pt] iter 1490: loss 1.1265, time 109.9ms\n",
            "[ckpt_base.pt] iter 1500: train 0.8658, val 0.9343\n",
            "[ckpt_base.pt] iter 1500: loss 1.1444, time 13346.5ms\n",
            "[ckpt_base.pt] iter 1510: loss 1.1275, time 108.9ms\n",
            "[ckpt_base.pt] iter 1520: loss 1.1702, time 105.4ms\n",
            "[ckpt_base.pt] iter 1530: loss 1.1589, time 107.6ms\n",
            "[ckpt_base.pt] iter 1540: loss 1.1399, time 109.6ms\n",
            "[ckpt_base.pt] iter 1550: loss 1.1320, time 108.4ms\n",
            "[ckpt_base.pt] iter 1560: loss 1.1292, time 107.1ms\n",
            "[ckpt_base.pt] iter 1570: loss 1.1283, time 105.3ms\n",
            "[ckpt_base.pt] iter 1580: loss 1.1259, time 107.9ms\n",
            "[ckpt_base.pt] iter 1590: loss 1.1438, time 105.2ms\n",
            "[ckpt_base.pt] iter 1600: loss 1.1334, time 108.4ms\n",
            "[ckpt_base.pt] iter 1610: loss 1.1447, time 108.5ms\n",
            "[ckpt_base.pt] iter 1620: loss 1.1168, time 105.9ms\n",
            "[ckpt_base.pt] iter 1630: loss 1.1209, time 105.1ms\n",
            "[ckpt_base.pt] iter 1640: loss 1.1464, time 104.7ms\n",
            "[ckpt_base.pt] iter 1650: loss 1.1359, time 104.4ms\n",
            "[ckpt_base.pt] iter 1660: loss 1.1526, time 104.8ms\n",
            "[ckpt_base.pt] iter 1670: loss 1.1284, time 106.7ms\n",
            "[ckpt_base.pt] iter 1680: loss 1.1185, time 104.9ms\n",
            "[ckpt_base.pt] iter 1690: loss 1.1250, time 104.5ms\n",
            "[ckpt_base.pt] iter 1700: loss 1.1253, time 104.6ms\n",
            "[ckpt_base.pt] iter 1710: loss 1.0889, time 103.9ms\n",
            "[ckpt_base.pt] iter 1720: loss 1.1057, time 103.7ms\n",
            "[ckpt_base.pt] iter 1730: loss 1.1179, time 106.2ms\n",
            "[ckpt_base.pt] iter 1740: loss 1.1017, time 106.4ms\n",
            "[ckpt_base.pt] iter 1750: train 0.8655, val 0.9330\n",
            "[ckpt_base.pt] saving checkpoint at iter 1750 → /content/drive/MyDrive/ECS 189G/out/ckpt_base.pt\n",
            "[ckpt_base.pt] iter 1750: loss 1.1275, time 13454.6ms\n",
            "[ckpt_base.pt] iter 1760: loss 1.1110, time 110.1ms\n",
            "[ckpt_base.pt] iter 1770: loss 1.1072, time 115.7ms\n",
            "[ckpt_base.pt] iter 1780: loss 1.0880, time 109.6ms\n",
            "[ckpt_base.pt] iter 1790: loss 1.1018, time 110.3ms\n",
            "[ckpt_base.pt] iter 1800: loss 1.1238, time 110.1ms\n",
            "[ckpt_base.pt] iter 1810: loss 1.0952, time 113.2ms\n",
            "[ckpt_base.pt] iter 1820: loss 1.0961, time 110.5ms\n",
            "[ckpt_base.pt] iter 1830: loss 1.0933, time 105.4ms\n",
            "[ckpt_base.pt] iter 1840: loss 1.1153, time 106.6ms\n",
            "[ckpt_base.pt] iter 1850: loss 1.1135, time 106.7ms\n",
            "[ckpt_base.pt] iter 1860: loss 1.1194, time 38.2ms\n",
            "[ckpt_base.pt] iter 1870: loss 1.0922, time 113.0ms\n",
            "[ckpt_base.pt] iter 1880: loss 1.0764, time 108.5ms\n",
            "[ckpt_base.pt] iter 1890: loss 1.1546, time 105.9ms\n",
            "[ckpt_base.pt] iter 1900: loss 1.0828, time 106.5ms\n",
            "[ckpt_base.pt] iter 1910: loss 1.1167, time 107.7ms\n",
            "[ckpt_base.pt] iter 1920: loss 1.0706, time 106.9ms\n",
            "[ckpt_base.pt] iter 1930: loss 1.0852, time 107.8ms\n",
            "[ckpt_base.pt] iter 1940: loss 1.1059, time 110.2ms\n",
            "[ckpt_base.pt] iter 1950: loss 1.0995, time 106.3ms\n",
            "[ckpt_base.pt] iter 1960: loss 1.0979, time 106.5ms\n",
            "[ckpt_base.pt] iter 1970: loss 1.0665, time 105.6ms\n",
            "[ckpt_base.pt] iter 1980: loss 1.0913, time 109.3ms\n",
            "[ckpt_base.pt] iter 1990: loss 1.0748, time 109.0ms\n",
            "[ckpt_base.pt] iter 2000: train 0.8667, val 0.9389\n",
            "[ckpt_base.pt] iter 2000: loss 1.0984, time 13052.7ms\n",
            "[ckpt_base.pt] iter 2010: loss 1.0906, time 107.4ms\n",
            "[ckpt_base.pt] iter 2020: loss 1.0852, time 104.9ms\n",
            "[ckpt_base.pt] iter 2030: loss 1.0748, time 108.1ms\n",
            "[ckpt_base.pt] iter 2040: loss 1.1099, time 104.2ms\n",
            "[ckpt_base.pt] iter 2050: loss 1.0861, time 104.3ms\n",
            "[ckpt_base.pt] iter 2060: loss 1.0879, time 103.9ms\n",
            "[ckpt_base.pt] iter 2070: loss 1.1003, time 104.2ms\n",
            "[ckpt_base.pt] iter 2080: loss 1.0361, time 104.3ms\n",
            "[ckpt_base.pt] iter 2090: loss 1.0689, time 103.5ms\n",
            "[ckpt_base.pt] iter 2100: loss 1.0572, time 104.0ms\n",
            "[ckpt_base.pt] iter 2110: loss 1.0822, time 103.3ms\n",
            "[ckpt_base.pt] iter 2120: loss 1.0725, time 103.4ms\n",
            "[ckpt_base.pt] iter 2130: loss 1.0890, time 103.8ms\n",
            "[ckpt_base.pt] iter 2140: loss 1.0590, time 102.9ms\n",
            "[ckpt_base.pt] iter 2150: loss 1.0891, time 104.0ms\n",
            "[ckpt_base.pt] iter 2160: loss 1.0604, time 103.1ms\n",
            "[ckpt_base.pt] iter 2170: loss 1.0866, time 101.6ms\n",
            "[ckpt_base.pt] iter 2180: loss 1.0810, time 100.7ms\n",
            "[ckpt_base.pt] iter 2190: loss 1.0613, time 103.0ms\n",
            "[ckpt_base.pt] iter 2200: loss 1.0701, time 104.2ms\n",
            "[ckpt_base.pt] iter 2210: loss 1.0799, time 103.0ms\n",
            "[ckpt_base.pt] iter 2220: loss 1.0654, time 105.3ms\n",
            "[ckpt_base.pt] iter 2230: loss 1.0799, time 104.1ms\n",
            "[ckpt_base.pt] iter 2240: loss 1.0798, time 103.2ms\n",
            "[ckpt_base.pt] iter 2250: train 0.8662, val 0.9344\n",
            "[ckpt_base.pt] iter 2250: loss 1.0732, time 12987.4ms\n",
            "[ckpt_base.pt] iter 2260: loss 1.0405, time 103.7ms\n",
            "[ckpt_base.pt] iter 2270: loss 1.0604, time 104.3ms\n",
            "[ckpt_base.pt] iter 2280: loss 1.0796, time 104.1ms\n",
            "[ckpt_base.pt] iter 2290: loss 1.0526, time 106.4ms\n",
            "[ckpt_base.pt] iter 2300: loss 1.0499, time 104.0ms\n",
            "[ckpt_base.pt] iter 2310: loss 1.0583, time 103.4ms\n",
            "[ckpt_base.pt] iter 2320: loss 1.0480, time 104.1ms\n",
            "[ckpt_base.pt] iter 2330: loss 1.0286, time 103.7ms\n",
            "[ckpt_base.pt] iter 2340: loss 1.0662, time 101.9ms\n",
            "[ckpt_base.pt] iter 2350: loss 1.0613, time 107.1ms\n",
            "[ckpt_base.pt] iter 2360: loss 1.0400, time 103.7ms\n",
            "[ckpt_base.pt] iter 2370: loss 1.0546, time 105.1ms\n",
            "[ckpt_base.pt] iter 2380: loss 1.0408, time 103.5ms\n",
            "[ckpt_base.pt] iter 2390: loss 1.0516, time 104.1ms\n",
            "[ckpt_base.pt] iter 2400: loss 1.0611, time 103.5ms\n",
            "[ckpt_base.pt] iter 2410: loss 1.0588, time 104.4ms\n",
            "[ckpt_base.pt] iter 2420: loss 1.0131, time 103.1ms\n",
            "[ckpt_base.pt] iter 2430: loss 1.0385, time 105.8ms\n",
            "[ckpt_base.pt] iter 2440: loss 1.0484, time 104.4ms\n",
            "[ckpt_base.pt] iter 2450: loss 1.0238, time 103.9ms\n",
            "[ckpt_base.pt] iter 2460: loss 1.0484, time 104.5ms\n",
            "[ckpt_base.pt] iter 2470: loss 1.0867, time 104.3ms\n",
            "[ckpt_base.pt] iter 2480: loss 1.0544, time 103.8ms\n",
            "[ckpt_base.pt] iter 2490: loss 1.0241, time 104.5ms\n",
            "[ckpt_base.pt] iter 2500: train 0.8656, val 0.9355\n",
            "[ckpt_base.pt] iter 2500: loss 1.0463, time 13371.1ms\n",
            "[ckpt_base.pt] iter 2510: loss 1.0500, time 106.9ms\n",
            "[ckpt_base.pt] iter 2520: loss 1.0230, time 104.9ms\n",
            "[ckpt_base.pt] iter 2530: loss 1.0432, time 102.8ms\n",
            "[ckpt_base.pt] iter 2540: loss 1.0608, time 100.8ms\n",
            "[ckpt_base.pt] iter 2550: loss 1.0474, time 103.3ms\n",
            "[ckpt_base.pt] iter 2560: loss 1.0621, time 103.2ms\n",
            "[ckpt_base.pt] iter 2570: loss 1.0364, time 102.0ms\n",
            "[ckpt_base.pt] iter 2580: loss 1.0098, time 103.6ms\n",
            "[ckpt_base.pt] iter 2590: loss 1.0473, time 103.9ms\n",
            "[ckpt_base.pt] iter 2600: loss 1.0158, time 103.2ms\n",
            "[ckpt_base.pt] iter 2610: loss 1.0366, time 102.3ms\n",
            "[ckpt_base.pt] iter 2620: loss 1.0452, time 104.3ms\n",
            "[ckpt_base.pt] iter 2630: loss 1.0297, time 103.9ms\n",
            "[ckpt_base.pt] iter 2640: loss 1.0351, time 101.8ms\n",
            "[ckpt_base.pt] iter 2650: loss 1.0159, time 104.4ms\n",
            "[ckpt_base.pt] iter 2660: loss 1.0650, time 103.6ms\n",
            "[ckpt_base.pt] iter 2670: loss 1.0259, time 104.1ms\n",
            "[ckpt_base.pt] iter 2680: loss 1.0068, time 103.4ms\n",
            "[ckpt_base.pt] iter 2690: loss 1.0422, time 105.8ms\n",
            "[ckpt_base.pt] iter 2700: loss 1.0308, time 101.9ms\n",
            "[ckpt_base.pt] iter 2710: loss 1.0712, time 105.1ms\n",
            "[ckpt_base.pt] iter 2720: loss 1.0182, time 105.2ms\n",
            "[ckpt_base.pt] iter 2730: loss 1.0315, time 102.7ms\n",
            "[ckpt_base.pt] iter 2740: loss 1.0312, time 103.0ms\n",
            "[ckpt_base.pt] iter 2750: train 0.8657, val 0.9364\n",
            "[ckpt_base.pt] iter 2750: loss 1.0155, time 13026.1ms\n",
            "[ckpt_base.pt] iter 2760: loss 1.0517, time 103.8ms\n",
            "[ckpt_base.pt] iter 2770: loss 1.0012, time 103.7ms\n",
            "[ckpt_base.pt] iter 2780: loss 1.0287, time 103.0ms\n",
            "[ckpt_base.pt] iter 2790: loss 1.0551, time 103.4ms\n",
            "[ckpt_base.pt] iter 2800: loss 1.0136, time 104.4ms\n",
            "[ckpt_base.pt] iter 2810: loss 0.9954, time 102.8ms\n",
            "[ckpt_base.pt] iter 2820: loss 0.9907, time 104.0ms\n",
            "[ckpt_base.pt] iter 2830: loss 0.9990, time 104.0ms\n",
            "[ckpt_base.pt] iter 2840: loss 1.0200, time 103.3ms\n",
            "[ckpt_base.pt] iter 2850: loss 1.0061, time 103.6ms\n",
            "[ckpt_base.pt] iter 2860: loss 0.9880, time 104.7ms\n",
            "[ckpt_base.pt] iter 2870: loss 1.0333, time 103.3ms\n",
            "[ckpt_base.pt] iter 2880: loss 1.0224, time 103.4ms\n",
            "[ckpt_base.pt] iter 2890: loss 0.9848, time 105.1ms\n",
            "[ckpt_base.pt] iter 2900: loss 0.9864, time 102.6ms\n",
            "[ckpt_base.pt] iter 2910: loss 1.0130, time 102.7ms\n",
            "[ckpt_base.pt] iter 2920: loss 1.0097, time 103.5ms\n",
            "[ckpt_base.pt] iter 2930: loss 1.0211, time 102.5ms\n",
            "[ckpt_base.pt] iter 2940: loss 1.0162, time 104.1ms\n",
            "[ckpt_base.pt] iter 2950: loss 1.0006, time 104.3ms\n",
            "[ckpt_base.pt] iter 2960: loss 0.9937, time 105.0ms\n",
            "[ckpt_base.pt] iter 2970: loss 1.0159, time 101.9ms\n",
            "[ckpt_base.pt] iter 2980: loss 0.9939, time 104.5ms\n",
            "[ckpt_base.pt] iter 2990: loss 0.9721, time 104.6ms\n",
            "[ckpt_base.pt] iter 3000: train 0.8657, val 0.9350\n",
            "[ckpt_base.pt] iter 3000: loss 1.0227, time 12913.0ms\n",
            "[ckpt_base.pt] iter 3010: loss 1.0136, time 105.0ms\n",
            "[ckpt_base.pt] iter 3020: loss 1.0048, time 103.9ms\n",
            "[ckpt_base.pt] iter 3030: loss 1.0129, time 102.7ms\n",
            "[ckpt_base.pt] iter 3040: loss 1.0142, time 103.5ms\n",
            "[ckpt_base.pt] iter 3050: loss 0.9881, time 101.9ms\n",
            "[ckpt_base.pt] iter 3060: loss 1.0204, time 103.4ms\n",
            "[ckpt_base.pt] iter 3070: loss 0.9916, time 102.9ms\n",
            "[ckpt_base.pt] iter 3080: loss 1.0085, time 104.5ms\n",
            "[ckpt_base.pt] iter 3090: loss 1.0043, time 101.7ms\n",
            "[ckpt_base.pt] iter 3100: loss 0.9638, time 103.8ms\n",
            "[ckpt_base.pt] iter 3110: loss 1.0129, time 103.7ms\n",
            "[ckpt_base.pt] iter 3120: loss 1.0145, time 103.6ms\n",
            "[ckpt_base.pt] iter 3130: loss 1.0109, time 103.0ms\n",
            "[ckpt_base.pt] iter 3140: loss 0.9665, time 102.7ms\n",
            "[ckpt_base.pt] iter 3150: loss 1.0081, time 102.9ms\n",
            "[ckpt_base.pt] iter 3160: loss 0.9885, time 101.2ms\n",
            "[ckpt_base.pt] iter 3170: loss 1.0184, time 104.6ms\n",
            "[ckpt_base.pt] iter 3180: loss 1.0218, time 102.7ms\n",
            "[ckpt_base.pt] iter 3190: loss 0.9878, time 102.7ms\n",
            "[ckpt_base.pt] iter 3200: loss 1.0269, time 101.7ms\n",
            "[ckpt_base.pt] iter 3210: loss 0.9904, time 102.3ms\n",
            "[ckpt_base.pt] iter 3220: loss 0.9622, time 103.5ms\n",
            "[ckpt_base.pt] iter 3230: loss 0.9981, time 104.5ms\n",
            "[ckpt_base.pt] iter 3240: loss 0.9706, time 101.8ms\n",
            "[ckpt_base.pt] iter 3250: train 0.8632, val 0.9353\n",
            "[ckpt_base.pt] iter 3250: loss 1.0345, time 12946.1ms\n",
            "[ckpt_base.pt] iter 3260: loss 0.9952, time 103.5ms\n",
            "[ckpt_base.pt] iter 3270: loss 0.9937, time 102.0ms\n",
            "[ckpt_base.pt] iter 3280: loss 0.9855, time 104.6ms\n",
            "[ckpt_base.pt] iter 3290: loss 0.9791, time 101.4ms\n",
            "[ckpt_base.pt] iter 3300: loss 0.9583, time 102.8ms\n",
            "[ckpt_base.pt] iter 3310: loss 1.0065, time 103.4ms\n",
            "[ckpt_base.pt] iter 3320: loss 0.9787, time 103.1ms\n",
            "[ckpt_base.pt] iter 3330: loss 1.0004, time 104.1ms\n",
            "[ckpt_base.pt] iter 3340: loss 0.9812, time 103.8ms\n",
            "[ckpt_base.pt] iter 3350: loss 0.9818, time 103.6ms\n",
            "[ckpt_base.pt] iter 3360: loss 0.9419, time 105.7ms\n",
            "[ckpt_base.pt] iter 3370: loss 0.9806, time 103.0ms\n",
            "[ckpt_base.pt] iter 3380: loss 0.9849, time 103.0ms\n",
            "[ckpt_base.pt] iter 3390: loss 0.9997, time 103.6ms\n",
            "[ckpt_base.pt] iter 3400: loss 1.0005, time 103.9ms\n",
            "[ckpt_base.pt] iter 3410: loss 1.0001, time 102.7ms\n",
            "[ckpt_base.pt] iter 3420: loss 0.9834, time 102.7ms\n",
            "[ckpt_base.pt] iter 3430: loss 0.9602, time 105.3ms\n",
            "[ckpt_base.pt] iter 3440: loss 0.9539, time 105.3ms\n",
            "[ckpt_base.pt] iter 3450: loss 0.9870, time 102.1ms\n",
            "[ckpt_base.pt] iter 3460: loss 1.0048, time 103.6ms\n",
            "[ckpt_base.pt] iter 3470: loss 0.9832, time 103.2ms\n",
            "[ckpt_base.pt] iter 3480: loss 0.9993, time 102.1ms\n",
            "[ckpt_base.pt] iter 3490: loss 0.9754, time 104.0ms\n",
            "[ckpt_base.pt] iter 3500: train 0.8625, val 0.9368\n",
            "[ckpt_base.pt] iter 3500: loss 0.9532, time 12944.6ms\n",
            "[ckpt_base.pt] iter 3510: loss 1.0103, time 102.3ms\n",
            "[ckpt_base.pt] iter 3520: loss 0.9511, time 106.1ms\n",
            "[ckpt_base.pt] iter 3530: loss 0.9668, time 102.6ms\n",
            "[ckpt_base.pt] iter 3540: loss 0.9346, time 103.6ms\n",
            "[ckpt_base.pt] iter 3550: loss 0.9646, time 103.7ms\n",
            "[ckpt_base.pt] iter 3560: loss 0.9920, time 103.2ms\n",
            "[ckpt_base.pt] iter 3570: loss 0.9769, time 104.8ms\n",
            "[ckpt_base.pt] iter 3580: loss 0.9884, time 104.5ms\n",
            "[ckpt_base.pt] iter 3590: loss 0.9743, time 103.5ms\n",
            "[ckpt_base.pt] iter 3600: loss 1.0033, time 106.7ms\n",
            "[ckpt_base.pt] iter 3610: loss 0.9485, time 104.1ms\n",
            "[ckpt_base.pt] iter 3620: loss 0.9649, time 104.3ms\n",
            "[ckpt_base.pt] iter 3630: loss 0.9594, time 102.1ms\n",
            "[ckpt_base.pt] iter 3640: loss 0.9725, time 103.8ms\n",
            "[ckpt_base.pt] iter 3650: loss 0.9792, time 103.2ms\n",
            "[ckpt_base.pt] iter 3660: loss 0.9507, time 103.1ms\n",
            "[ckpt_base.pt] iter 3670: loss 0.9693, time 102.4ms\n",
            "[ckpt_base.pt] iter 3680: loss 0.9916, time 102.0ms\n",
            "[ckpt_base.pt] iter 3690: loss 0.9672, time 104.7ms\n",
            "[ckpt_base.pt] iter 3700: loss 0.9653, time 102.5ms\n",
            "[ckpt_base.pt] iter 3710: loss 0.9564, time 103.5ms\n",
            "[ckpt_base.pt] iter 3720: loss 0.9480, time 102.1ms\n",
            "[ckpt_base.pt] iter 3730: loss 0.9468, time 104.4ms\n",
            "[ckpt_base.pt] iter 3740: loss 0.9425, time 102.3ms\n",
            "[ckpt_base.pt] iter 3750: train 0.8635, val 0.9353\n",
            "[ckpt_base.pt] iter 3750: loss 0.9551, time 12961.4ms\n",
            "[ckpt_base.pt] iter 3760: loss 0.9885, time 106.5ms\n",
            "[ckpt_base.pt] iter 3770: loss 0.9691, time 104.7ms\n",
            "[ckpt_base.pt] iter 3780: loss 0.9743, time 102.0ms\n",
            "[ckpt_base.pt] iter 3790: loss 0.9515, time 104.3ms\n",
            "[ckpt_base.pt] iter 3800: loss 0.9886, time 103.9ms\n",
            "[ckpt_base.pt] iter 3810: loss 0.9926, time 102.6ms\n",
            "[ckpt_base.pt] iter 3820: loss 0.9657, time 106.0ms\n",
            "[ckpt_base.pt] iter 3830: loss 0.9351, time 103.1ms\n",
            "[ckpt_base.pt] iter 3840: loss 0.9638, time 104.8ms\n",
            "[ckpt_base.pt] iter 3850: loss 0.9697, time 104.3ms\n",
            "[ckpt_base.pt] iter 3860: loss 0.9582, time 101.5ms\n",
            "[ckpt_base.pt] iter 3870: loss 0.9585, time 103.6ms\n",
            "[ckpt_base.pt] iter 3880: loss 0.9709, time 104.1ms\n",
            "[ckpt_base.pt] iter 3890: loss 0.9795, time 102.9ms\n",
            "[ckpt_base.pt] iter 3900: loss 0.9258, time 103.5ms\n",
            "[ckpt_base.pt] iter 3910: loss 0.9510, time 103.2ms\n",
            "[ckpt_base.pt] iter 3920: loss 0.9655, time 103.7ms\n",
            "[ckpt_base.pt] iter 3930: loss 0.9598, time 103.0ms\n",
            "[ckpt_base.pt] iter 3940: loss 0.9679, time 103.7ms\n",
            "[ckpt_base.pt] iter 3950: loss 1.0003, time 101.6ms\n",
            "[ckpt_base.pt] iter 3960: loss 0.9537, time 102.6ms\n",
            "[ckpt_base.pt] iter 3970: loss 0.9747, time 101.3ms\n",
            "[ckpt_base.pt] iter 3980: loss 0.9649, time 103.8ms\n",
            "[ckpt_base.pt] iter 3990: loss 0.9609, time 102.5ms\n",
            "[ckpt_base.pt] iter 4000: train 0.8641, val 0.9338\n",
            "[ckpt_base.pt] iter 4000: loss 0.9334, time 12947.6ms\n",
            "[ckpt_base.pt] iter 4010: loss 0.9734, time 105.9ms\n",
            "[ckpt_base.pt] iter 4020: loss 0.9614, time 105.0ms\n",
            "[ckpt_base.pt] iter 4030: loss 0.9323, time 104.0ms\n",
            "[ckpt_base.pt] iter 4040: loss 0.9433, time 102.4ms\n",
            "[ckpt_base.pt] iter 4050: loss 0.9579, time 102.0ms\n",
            "[ckpt_base.pt] iter 4060: loss 0.9530, time 103.7ms\n",
            "[ckpt_base.pt] iter 4070: loss 0.9340, time 102.4ms\n",
            "[ckpt_base.pt] iter 4080: loss 0.9335, time 103.3ms\n",
            "[ckpt_base.pt] iter 4090: loss 0.9467, time 105.5ms\n",
            "[ckpt_base.pt] iter 4100: loss 0.9235, time 105.8ms\n",
            "[ckpt_base.pt] iter 4110: loss 0.9537, time 102.5ms\n",
            "[ckpt_base.pt] iter 4120: loss 0.9418, time 102.8ms\n",
            "[ckpt_base.pt] iter 4130: loss 0.9489, time 103.0ms\n",
            "[ckpt_base.pt] iter 4140: loss 0.9684, time 102.3ms\n",
            "[ckpt_base.pt] iter 4150: loss 0.9332, time 104.3ms\n",
            "[ckpt_base.pt] iter 4160: loss 0.9423, time 101.5ms\n",
            "[ckpt_base.pt] iter 4170: loss 0.9398, time 103.7ms\n",
            "[ckpt_base.pt] iter 4180: loss 0.9525, time 104.1ms\n",
            "[ckpt_base.pt] iter 4190: loss 0.9699, time 103.3ms\n",
            "[ckpt_base.pt] iter 4200: loss 0.9641, time 103.2ms\n",
            "[ckpt_base.pt] iter 4210: loss 0.9648, time 102.2ms\n",
            "[ckpt_base.pt] iter 4220: loss 0.9461, time 104.0ms\n",
            "[ckpt_base.pt] iter 4230: loss 0.9280, time 103.9ms\n",
            "[ckpt_base.pt] iter 4240: loss 0.9292, time 103.0ms\n",
            "[ckpt_base.pt] iter 4250: train 0.8673, val 0.9326\n",
            "[ckpt_base.pt] saving checkpoint at iter 4250 → /content/drive/MyDrive/ECS 189G/out/ckpt_base.pt\n",
            "[ckpt_base.pt] iter 4250: loss 0.9370, time 13481.4ms\n",
            "[ckpt_base.pt] iter 4260: loss 0.9560, time 101.8ms\n",
            "[ckpt_base.pt] iter 4270: loss 0.9409, time 105.0ms\n",
            "[ckpt_base.pt] iter 4280: loss 0.9644, time 103.7ms\n",
            "[ckpt_base.pt] iter 4290: loss 0.9258, time 103.0ms\n",
            "[ckpt_base.pt] iter 4300: loss 0.9378, time 104.4ms\n",
            "[ckpt_base.pt] iter 4310: loss 0.9480, time 101.8ms\n",
            "[ckpt_base.pt] iter 4320: loss 0.9178, time 103.6ms\n",
            "[ckpt_base.pt] iter 4330: loss 0.9416, time 104.0ms\n",
            "[ckpt_base.pt] iter 4340: loss 0.9489, time 102.7ms\n",
            "[ckpt_base.pt] iter 4350: loss 0.9400, time 102.5ms\n",
            "[ckpt_base.pt] iter 4360: loss 0.9448, time 105.3ms\n",
            "[ckpt_base.pt] iter 4370: loss 0.9256, time 101.3ms\n",
            "[ckpt_base.pt] iter 4380: loss 0.9574, time 101.1ms\n",
            "[ckpt_base.pt] iter 4390: loss 0.9353, time 102.0ms\n",
            "[ckpt_base.pt] iter 4400: loss 0.9611, time 102.3ms\n",
            "[ckpt_base.pt] iter 4410: loss 0.9297, time 100.5ms\n",
            "[ckpt_base.pt] iter 4420: loss 0.9520, time 103.7ms\n",
            "[ckpt_base.pt] iter 4430: loss 0.9346, time 102.7ms\n",
            "[ckpt_base.pt] iter 4440: loss 0.9417, time 102.5ms\n",
            "[ckpt_base.pt] iter 4450: loss 0.9272, time 104.2ms\n",
            "[ckpt_base.pt] iter 4460: loss 0.9196, time 104.1ms\n",
            "[ckpt_base.pt] iter 4470: loss 0.9375, time 103.6ms\n",
            "[ckpt_base.pt] iter 4480: loss 0.9741, time 103.0ms\n",
            "[ckpt_base.pt] iter 4490: loss 0.9417, time 104.9ms\n",
            "[ckpt_base.pt] iter 4500: train 0.8637, val 0.9394\n",
            "[ckpt_base.pt] iter 4500: loss 0.9474, time 12901.9ms\n",
            "[ckpt_base.pt] iter 4510: loss 0.9369, time 105.2ms\n",
            "[ckpt_base.pt] iter 4520: loss 0.9248, time 102.9ms\n",
            "[ckpt_base.pt] iter 4530: loss 0.9415, time 104.7ms\n",
            "[ckpt_base.pt] iter 4540: loss 0.9672, time 103.2ms\n",
            "[ckpt_base.pt] iter 4550: loss 0.9348, time 102.3ms\n",
            "[ckpt_base.pt] iter 4560: loss 0.9192, time 102.9ms\n",
            "[ckpt_base.pt] iter 4570: loss 0.9499, time 103.7ms\n",
            "[ckpt_base.pt] iter 4580: loss 0.9088, time 104.6ms\n",
            "[ckpt_base.pt] iter 4590: loss 0.9405, time 103.1ms\n",
            "[ckpt_base.pt] iter 4600: loss 0.9369, time 103.6ms\n",
            "[ckpt_base.pt] iter 4610: loss 0.8817, time 103.8ms\n",
            "[ckpt_base.pt] iter 4620: loss 0.9348, time 104.1ms\n",
            "[ckpt_base.pt] iter 4630: loss 0.9201, time 101.3ms\n",
            "[ckpt_base.pt] iter 4640: loss 0.9118, time 104.3ms\n",
            "[ckpt_base.pt] iter 4650: loss 0.9267, time 103.0ms\n",
            "[ckpt_base.pt] iter 4660: loss 0.9466, time 103.6ms\n",
            "[ckpt_base.pt] iter 4670: loss 0.9466, time 100.7ms\n",
            "[ckpt_base.pt] iter 4680: loss 0.9478, time 102.9ms\n",
            "[ckpt_base.pt] iter 4690: loss 0.8868, time 103.5ms\n",
            "[ckpt_base.pt] iter 4700: loss 0.9325, time 101.2ms\n",
            "[ckpt_base.pt] iter 4710: loss 0.9453, time 103.8ms\n",
            "[ckpt_base.pt] iter 4720: loss 0.9433, time 104.1ms\n",
            "[ckpt_base.pt] iter 4730: loss 0.9582, time 103.1ms\n",
            "[ckpt_base.pt] iter 4740: loss 0.9347, time 102.4ms\n",
            "[ckpt_base.pt] iter 4750: train 0.8650, val 0.9363\n",
            "[ckpt_base.pt] iter 4750: loss 0.9176, time 12944.0ms\n",
            "[ckpt_base.pt] iter 4760: loss 0.9385, time 104.8ms\n",
            "[ckpt_base.pt] iter 4770: loss 0.9377, time 103.1ms\n",
            "[ckpt_base.pt] iter 4780: loss 0.9172, time 103.1ms\n",
            "[ckpt_base.pt] iter 4790: loss 0.9641, time 103.8ms\n",
            "[ckpt_base.pt] iter 4800: loss 0.9221, time 102.8ms\n",
            "[ckpt_base.pt] iter 4810: loss 0.9267, time 102.8ms\n",
            "[ckpt_base.pt] iter 4820: loss 0.9195, time 103.2ms\n",
            "[ckpt_base.pt] iter 4830: loss 0.9205, time 102.5ms\n",
            "[ckpt_base.pt] iter 4840: loss 0.9124, time 103.8ms\n",
            "[ckpt_base.pt] iter 4850: loss 0.9229, time 101.8ms\n",
            "[ckpt_base.pt] iter 4860: loss 0.9324, time 102.8ms\n",
            "[ckpt_base.pt] iter 4870: loss 0.9392, time 103.7ms\n",
            "[ckpt_base.pt] iter 4880: loss 0.9428, time 103.4ms\n",
            "[ckpt_base.pt] iter 4890: loss 0.9584, time 102.0ms\n",
            "[ckpt_base.pt] iter 4900: loss 0.9394, time 103.1ms\n",
            "[ckpt_base.pt] iter 4910: loss 0.9393, time 101.5ms\n",
            "[ckpt_base.pt] iter 4920: loss 0.9006, time 103.8ms\n",
            "[ckpt_base.pt] iter 4930: loss 0.9220, time 101.9ms\n",
            "[ckpt_base.pt] iter 4940: loss 0.9460, time 103.5ms\n",
            "[ckpt_base.pt] iter 4950: loss 0.9202, time 103.7ms\n",
            "[ckpt_base.pt] iter 4960: loss 0.9489, time 102.0ms\n",
            "[ckpt_base.pt] iter 4970: loss 0.9324, time 103.4ms\n",
            "[ckpt_base.pt] iter 4980: loss 0.9640, time 106.3ms\n",
            "[ckpt_base.pt] iter 4990: loss 0.9178, time 104.0ms\n",
            "[ckpt_base.pt] iter 5000: train 0.8653, val 0.9355\n",
            "[ckpt_base.pt] final save at iter 5000 → /content/drive/MyDrive/ECS 189G/out/ckpt_base.pt\n",
            "[ckpt_base.pt] Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT + LoRA"
      ],
      "metadata": {
        "id": "QYEeeOiuSc4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell C: NanoGPT + LoRA (no DAR, no PAAF)\n",
        "\n",
        "train_and_save_variant(\n",
        "    out_dir       = out_dir,\n",
        "    ckpt_filename = 'ckpt_lora.pt',\n",
        "    use_lora      = True,\n",
        "    use_paaf      = False,\n",
        "    use_dar       = False,\n",
        "    dar_weight    = 0.0,    # no DAR penalty\n",
        "    max_iters     = max_iters\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KWzS3QxSgkb",
        "outputId": "af0bb5a9-abd4-4cd0-cab6-1cb6696ba82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 10.66M\n",
            "--> Injecting LoRA adapters (rank=8, alpha=16.0)\n",
            "num decayed parameter tensors: 74, with 11,048,448 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-895d20065530>:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_local = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ckpt_lora.pt] iter 0: train 0.8660, val 0.9363\n",
            "[ckpt_lora.pt] iter 0: loss 4.6592, time 12400.3ms\n",
            "[ckpt_lora.pt] iter 10: loss 3.1473, time 142.6ms\n",
            "[ckpt_lora.pt] iter 20: loss 2.6685, time 144.5ms\n",
            "[ckpt_lora.pt] iter 30: loss 2.4949, time 143.5ms\n",
            "[ckpt_lora.pt] iter 40: loss 2.4083, time 143.1ms\n",
            "[ckpt_lora.pt] iter 50: loss 2.4210, time 143.7ms\n",
            "[ckpt_lora.pt] iter 60: loss 2.3888, time 142.5ms\n",
            "[ckpt_lora.pt] iter 70: loss 2.3575, time 144.4ms\n",
            "[ckpt_lora.pt] iter 80: loss 2.3733, time 142.5ms\n",
            "[ckpt_lora.pt] iter 90: loss 2.3552, time 143.9ms\n",
            "[ckpt_lora.pt] iter 100: loss 2.3494, time 143.3ms\n",
            "[ckpt_lora.pt] iter 110: loss 2.3219, time 143.6ms\n",
            "[ckpt_lora.pt] iter 120: loss 2.3035, time 143.8ms\n",
            "[ckpt_lora.pt] iter 130: loss 2.3168, time 144.5ms\n",
            "[ckpt_lora.pt] iter 140: loss 2.2621, time 144.3ms\n",
            "[ckpt_lora.pt] iter 150: loss 2.2166, time 140.5ms\n",
            "[ckpt_lora.pt] iter 160: loss 2.1042, time 141.9ms\n",
            "[ckpt_lora.pt] iter 170: loss 2.0190, time 142.9ms\n",
            "[ckpt_lora.pt] iter 180: loss 2.0220, time 143.0ms\n",
            "[ckpt_lora.pt] iter 190: loss 1.9431, time 145.7ms\n",
            "[ckpt_lora.pt] iter 200: loss 1.9227, time 144.8ms\n",
            "[ckpt_lora.pt] iter 210: loss 1.8870, time 144.4ms\n",
            "[ckpt_lora.pt] iter 220: loss 1.8450, time 143.2ms\n",
            "[ckpt_lora.pt] iter 230: loss 1.8345, time 144.9ms\n",
            "[ckpt_lora.pt] iter 240: loss 1.7798, time 144.1ms\n",
            "[ckpt_lora.pt] iter 250: train 0.8634, val 0.9346\n",
            "[ckpt_lora.pt] saving checkpoint at iter 250 → /content/drive/MyDrive/ECS 189G/out/ckpt_lora.pt\n",
            "[ckpt_lora.pt] iter 250: loss 1.8036, time 13522.6ms\n",
            "[ckpt_lora.pt] iter 260: loss 1.7583, time 143.8ms\n",
            "[ckpt_lora.pt] iter 270: loss 1.7546, time 144.6ms\n",
            "[ckpt_lora.pt] iter 280: loss 1.7235, time 145.8ms\n",
            "[ckpt_lora.pt] iter 290: loss 1.7179, time 146.2ms\n",
            "[ckpt_lora.pt] iter 300: loss 1.6683, time 146.3ms\n",
            "[ckpt_lora.pt] iter 310: loss 1.6969, time 144.5ms\n",
            "[ckpt_lora.pt] iter 320: loss 1.6941, time 146.3ms\n",
            "[ckpt_lora.pt] iter 330: loss 1.6386, time 145.0ms\n",
            "[ckpt_lora.pt] iter 340: loss 1.6519, time 145.6ms\n",
            "[ckpt_lora.pt] iter 350: loss 1.6356, time 148.3ms\n",
            "[ckpt_lora.pt] iter 360: loss 1.6141, time 145.9ms\n",
            "[ckpt_lora.pt] iter 370: loss 1.6074, time 146.7ms\n",
            "[ckpt_lora.pt] iter 380: loss 1.5998, time 146.3ms\n",
            "[ckpt_lora.pt] iter 390: loss 1.5583, time 145.9ms\n",
            "[ckpt_lora.pt] iter 400: loss 1.5837, time 146.7ms\n",
            "[ckpt_lora.pt] iter 410: loss 1.5532, time 145.0ms\n",
            "[ckpt_lora.pt] iter 420: loss 1.5774, time 147.5ms\n",
            "[ckpt_lora.pt] iter 430: loss 1.5466, time 144.5ms\n",
            "[ckpt_lora.pt] iter 440: loss 1.5387, time 144.1ms\n",
            "[ckpt_lora.pt] iter 450: loss 1.5378, time 146.0ms\n",
            "[ckpt_lora.pt] iter 460: loss 1.4998, time 144.7ms\n",
            "[ckpt_lora.pt] iter 470: loss 1.4960, time 145.9ms\n",
            "[ckpt_lora.pt] iter 480: loss 1.5129, time 146.7ms\n",
            "[ckpt_lora.pt] iter 490: loss 1.5433, time 145.4ms\n",
            "[ckpt_lora.pt] iter 500: train 0.8667, val 0.9349\n",
            "[ckpt_lora.pt] iter 500: loss 1.4910, time 12687.5ms\n",
            "[ckpt_lora.pt] iter 510: loss 1.5165, time 146.3ms\n",
            "[ckpt_lora.pt] iter 520: loss 1.4990, time 147.1ms\n",
            "[ckpt_lora.pt] iter 530: loss 1.5127, time 145.9ms\n",
            "[ckpt_lora.pt] iter 540: loss 1.4670, time 146.8ms\n",
            "[ckpt_lora.pt] iter 550: loss 1.4874, time 145.7ms\n",
            "[ckpt_lora.pt] iter 560: loss 1.4476, time 147.4ms\n",
            "[ckpt_lora.pt] iter 570: loss 1.4486, time 145.3ms\n",
            "[ckpt_lora.pt] iter 580: loss 1.4580, time 147.7ms\n",
            "[ckpt_lora.pt] iter 590: loss 1.4636, time 145.3ms\n",
            "[ckpt_lora.pt] iter 600: loss 1.4221, time 149.1ms\n",
            "[ckpt_lora.pt] iter 610: loss 1.4245, time 145.7ms\n",
            "[ckpt_lora.pt] iter 620: loss 1.4223, time 148.6ms\n",
            "[ckpt_lora.pt] iter 630: loss 1.4514, time 146.6ms\n",
            "[ckpt_lora.pt] iter 640: loss 1.4185, time 148.8ms\n",
            "[ckpt_lora.pt] iter 650: loss 1.4004, time 145.9ms\n",
            "[ckpt_lora.pt] iter 660: loss 1.3597, time 146.9ms\n",
            "[ckpt_lora.pt] iter 670: loss 1.3743, time 147.4ms\n",
            "[ckpt_lora.pt] iter 680: loss 1.4005, time 146.6ms\n",
            "[ckpt_lora.pt] iter 690: loss 1.4044, time 150.1ms\n",
            "[ckpt_lora.pt] iter 700: loss 1.3644, time 147.9ms\n",
            "[ckpt_lora.pt] iter 710: loss 1.3887, time 147.3ms\n",
            "[ckpt_lora.pt] iter 720: loss 1.3991, time 146.3ms\n",
            "[ckpt_lora.pt] iter 730: loss 1.3477, time 147.0ms\n",
            "[ckpt_lora.pt] iter 740: loss 1.3668, time 147.1ms\n",
            "[ckpt_lora.pt] iter 750: train 0.8663, val 0.9368\n",
            "[ckpt_lora.pt] iter 750: loss 1.3433, time 12686.1ms\n",
            "[ckpt_lora.pt] iter 760: loss 1.3678, time 146.5ms\n",
            "[ckpt_lora.pt] iter 770: loss 1.3343, time 148.3ms\n",
            "[ckpt_lora.pt] iter 780: loss 1.3234, time 148.3ms\n",
            "[ckpt_lora.pt] iter 790: loss 1.3261, time 146.9ms\n",
            "[ckpt_lora.pt] iter 800: loss 1.3579, time 148.2ms\n",
            "[ckpt_lora.pt] iter 810: loss 1.3000, time 148.7ms\n",
            "[ckpt_lora.pt] iter 820: loss 1.3523, time 147.1ms\n",
            "[ckpt_lora.pt] iter 830: loss 1.3097, time 147.4ms\n",
            "[ckpt_lora.pt] iter 840: loss 1.3261, time 146.4ms\n",
            "[ckpt_lora.pt] iter 850: loss 1.3349, time 148.2ms\n",
            "[ckpt_lora.pt] iter 860: loss 1.2886, time 148.8ms\n",
            "[ckpt_lora.pt] iter 870: loss 1.2995, time 146.9ms\n",
            "[ckpt_lora.pt] iter 880: loss 1.3176, time 146.8ms\n",
            "[ckpt_lora.pt] iter 890: loss 1.2506, time 148.2ms\n",
            "[ckpt_lora.pt] iter 900: loss 1.2929, time 148.3ms\n",
            "[ckpt_lora.pt] iter 910: loss 1.2372, time 147.0ms\n",
            "[ckpt_lora.pt] iter 920: loss 1.2687, time 147.9ms\n",
            "[ckpt_lora.pt] iter 930: loss 1.2766, time 148.1ms\n",
            "[ckpt_lora.pt] iter 940: loss 1.2762, time 147.7ms\n",
            "[ckpt_lora.pt] iter 950: loss 1.2956, time 148.0ms\n",
            "[ckpt_lora.pt] iter 960: loss 1.2808, time 149.6ms\n",
            "[ckpt_lora.pt] iter 970: loss 1.2685, time 147.7ms\n",
            "[ckpt_lora.pt] iter 980: loss 1.2305, time 146.9ms\n",
            "[ckpt_lora.pt] iter 990: loss 1.2486, time 147.9ms\n",
            "[ckpt_lora.pt] iter 1000: train 0.8637, val 0.9350\n",
            "[ckpt_lora.pt] iter 1000: loss 1.2444, time 12751.5ms\n",
            "[ckpt_lora.pt] iter 1010: loss 1.2508, time 147.6ms\n",
            "[ckpt_lora.pt] iter 1020: loss 1.2416, time 148.0ms\n",
            "[ckpt_lora.pt] iter 1030: loss 1.2686, time 148.0ms\n",
            "[ckpt_lora.pt] iter 1040: loss 1.2149, time 146.9ms\n",
            "[ckpt_lora.pt] iter 1050: loss 1.2306, time 149.1ms\n",
            "[ckpt_lora.pt] iter 1060: loss 1.2357, time 149.1ms\n",
            "[ckpt_lora.pt] iter 1070: loss 1.2628, time 150.1ms\n",
            "[ckpt_lora.pt] iter 1080: loss 1.2632, time 148.1ms\n",
            "[ckpt_lora.pt] iter 1090: loss 1.2460, time 149.1ms\n",
            "[ckpt_lora.pt] iter 1100: loss 1.2640, time 148.7ms\n",
            "[ckpt_lora.pt] iter 1110: loss 1.2235, time 149.8ms\n",
            "[ckpt_lora.pt] iter 1120: loss 1.2472, time 150.2ms\n",
            "[ckpt_lora.pt] iter 1130: loss 1.2545, time 150.3ms\n",
            "[ckpt_lora.pt] iter 1140: loss 1.2342, time 148.0ms\n",
            "[ckpt_lora.pt] iter 1150: loss 1.2109, time 147.3ms\n",
            "[ckpt_lora.pt] iter 1160: loss 1.2235, time 147.4ms\n",
            "[ckpt_lora.pt] iter 1170: loss 1.2410, time 147.1ms\n",
            "[ckpt_lora.pt] iter 1180: loss 1.2273, time 149.1ms\n",
            "[ckpt_lora.pt] iter 1190: loss 1.2200, time 148.9ms\n",
            "[ckpt_lora.pt] iter 1200: loss 1.2010, time 149.3ms\n",
            "[ckpt_lora.pt] iter 1210: loss 1.1787, time 146.6ms\n",
            "[ckpt_lora.pt] iter 1220: loss 1.1964, time 147.1ms\n",
            "[ckpt_lora.pt] iter 1230: loss 1.2166, time 148.7ms\n",
            "[ckpt_lora.pt] iter 1240: loss 1.1668, time 148.8ms\n",
            "[ckpt_lora.pt] iter 1250: train 0.8641, val 0.9366\n",
            "[ckpt_lora.pt] iter 1250: loss 1.1794, time 12733.0ms\n",
            "[ckpt_lora.pt] iter 1260: loss 1.1903, time 147.1ms\n",
            "[ckpt_lora.pt] iter 1270: loss 1.1868, time 149.7ms\n",
            "[ckpt_lora.pt] iter 1280: loss 1.1809, time 147.9ms\n",
            "[ckpt_lora.pt] iter 1290: loss 1.2088, time 148.1ms\n",
            "[ckpt_lora.pt] iter 1300: loss 1.1886, time 147.5ms\n",
            "[ckpt_lora.pt] iter 1310: loss 1.1830, time 149.8ms\n",
            "[ckpt_lora.pt] iter 1320: loss 1.1880, time 150.8ms\n",
            "[ckpt_lora.pt] iter 1330: loss 1.1734, time 150.2ms\n",
            "[ckpt_lora.pt] iter 1340: loss 1.1724, time 147.5ms\n",
            "[ckpt_lora.pt] iter 1350: loss 1.2057, time 147.8ms\n",
            "[ckpt_lora.pt] iter 1360: loss 1.2009, time 148.5ms\n",
            "[ckpt_lora.pt] iter 1370: loss 1.1688, time 147.5ms\n",
            "[ckpt_lora.pt] iter 1380: loss 1.1763, time 149.0ms\n",
            "[ckpt_lora.pt] iter 1390: loss 1.2027, time 148.2ms\n",
            "[ckpt_lora.pt] iter 1400: loss 1.1756, time 147.0ms\n",
            "[ckpt_lora.pt] iter 1410: loss 1.1861, time 148.6ms\n",
            "[ckpt_lora.pt] iter 1420: loss 1.1630, time 148.9ms\n",
            "[ckpt_lora.pt] iter 1430: loss 1.1749, time 148.7ms\n",
            "[ckpt_lora.pt] iter 1440: loss 1.1972, time 148.6ms\n",
            "[ckpt_lora.pt] iter 1450: loss 1.1588, time 149.1ms\n",
            "[ckpt_lora.pt] iter 1460: loss 1.1918, time 148.3ms\n",
            "[ckpt_lora.pt] iter 1470: loss 1.1845, time 148.6ms\n",
            "[ckpt_lora.pt] iter 1480: loss 1.1716, time 149.8ms\n",
            "[ckpt_lora.pt] iter 1490: loss 1.1547, time 147.8ms\n",
            "[ckpt_lora.pt] iter 1500: train 0.8658, val 0.9364\n",
            "[ckpt_lora.pt] iter 1500: loss 1.1733, time 12787.6ms\n",
            "[ckpt_lora.pt] iter 1510: loss 1.1634, time 148.0ms\n",
            "[ckpt_lora.pt] iter 1520: loss 1.1246, time 149.1ms\n",
            "[ckpt_lora.pt] iter 1530: loss 1.1498, time 150.2ms\n",
            "[ckpt_lora.pt] iter 1540: loss 1.1561, time 148.2ms\n",
            "[ckpt_lora.pt] iter 1550: loss 1.1465, time 147.4ms\n",
            "[ckpt_lora.pt] iter 1560: loss 1.1330, time 150.2ms\n",
            "[ckpt_lora.pt] iter 1570: loss 1.1579, time 150.8ms\n",
            "[ckpt_lora.pt] iter 1580: loss 1.1799, time 147.8ms\n",
            "[ckpt_lora.pt] iter 1590: loss 1.1835, time 147.3ms\n",
            "[ckpt_lora.pt] iter 1600: loss 1.1387, time 147.4ms\n",
            "[ckpt_lora.pt] iter 1610: loss 1.1425, time 149.3ms\n",
            "[ckpt_lora.pt] iter 1620: loss 1.1364, time 149.8ms\n",
            "[ckpt_lora.pt] iter 1630: loss 1.1531, time 149.2ms\n",
            "[ckpt_lora.pt] iter 1640: loss 1.1590, time 148.0ms\n",
            "[ckpt_lora.pt] iter 1650: loss 1.1363, time 148.8ms\n",
            "[ckpt_lora.pt] iter 1660: loss 1.1478, time 147.3ms\n",
            "[ckpt_lora.pt] iter 1670: loss 1.1135, time 151.2ms\n",
            "[ckpt_lora.pt] iter 1680: loss 1.1206, time 148.2ms\n",
            "[ckpt_lora.pt] iter 1690: loss 1.1159, time 148.6ms\n",
            "[ckpt_lora.pt] iter 1700: loss 1.1164, time 150.1ms\n",
            "[ckpt_lora.pt] iter 1710: loss 1.1338, time 147.2ms\n",
            "[ckpt_lora.pt] iter 1720: loss 1.1202, time 152.0ms\n",
            "[ckpt_lora.pt] iter 1730: loss 1.1302, time 149.0ms\n",
            "[ckpt_lora.pt] iter 1740: loss 1.1414, time 151.0ms\n",
            "[ckpt_lora.pt] iter 1750: train 0.8644, val 0.9371\n",
            "[ckpt_lora.pt] iter 1750: loss 1.1410, time 12786.5ms\n",
            "[ckpt_lora.pt] iter 1760: loss 1.1255, time 150.1ms\n",
            "[ckpt_lora.pt] iter 1770: loss 1.1353, time 147.9ms\n",
            "[ckpt_lora.pt] iter 1780: loss 1.0859, time 148.5ms\n",
            "[ckpt_lora.pt] iter 1790: loss 1.1199, time 147.7ms\n",
            "[ckpt_lora.pt] iter 1800: loss 1.1266, time 150.1ms\n",
            "[ckpt_lora.pt] iter 1810: loss 1.0790, time 149.0ms\n",
            "[ckpt_lora.pt] iter 1820: loss 1.1549, time 149.2ms\n",
            "[ckpt_lora.pt] iter 1830: loss 1.1062, time 148.2ms\n",
            "[ckpt_lora.pt] iter 1840: loss 1.1218, time 148.6ms\n",
            "[ckpt_lora.pt] iter 1850: loss 1.1169, time 149.8ms\n",
            "[ckpt_lora.pt] iter 1860: loss 1.1542, time 148.0ms\n",
            "[ckpt_lora.pt] iter 1870: loss 1.1468, time 148.0ms\n",
            "[ckpt_lora.pt] iter 1880: loss 1.1148, time 148.1ms\n",
            "[ckpt_lora.pt] iter 1890: loss 1.1008, time 149.9ms\n",
            "[ckpt_lora.pt] iter 1900: loss 1.0904, time 148.2ms\n",
            "[ckpt_lora.pt] iter 1910: loss 1.1358, time 146.8ms\n",
            "[ckpt_lora.pt] iter 1920: loss 1.1373, time 148.2ms\n",
            "[ckpt_lora.pt] iter 1930: loss 1.1003, time 150.2ms\n",
            "[ckpt_lora.pt] iter 1940: loss 1.1469, time 149.0ms\n",
            "[ckpt_lora.pt] iter 1950: loss 1.1149, time 148.6ms\n",
            "[ckpt_lora.pt] iter 1960: loss 1.0823, time 147.2ms\n",
            "[ckpt_lora.pt] iter 1970: loss 1.0720, time 150.6ms\n",
            "[ckpt_lora.pt] iter 1980: loss 1.1070, time 149.1ms\n",
            "[ckpt_lora.pt] iter 1990: loss 1.1048, time 151.1ms\n",
            "[ckpt_lora.pt] iter 2000: train 0.8661, val 0.9333\n",
            "[ckpt_lora.pt] saving checkpoint at iter 2000 → /content/drive/MyDrive/ECS 189G/out/ckpt_lora.pt\n",
            "[ckpt_lora.pt] iter 2000: loss 1.1247, time 13322.3ms\n",
            "[ckpt_lora.pt] iter 2010: loss 1.1151, time 148.5ms\n",
            "[ckpt_lora.pt] iter 2020: loss 1.1019, time 151.5ms\n",
            "[ckpt_lora.pt] iter 2030: loss 1.1190, time 148.7ms\n",
            "[ckpt_lora.pt] iter 2040: loss 1.0893, time 148.2ms\n",
            "[ckpt_lora.pt] iter 2050: loss 1.0741, time 149.3ms\n",
            "[ckpt_lora.pt] iter 2060: loss 1.1100, time 147.7ms\n",
            "[ckpt_lora.pt] iter 2070: loss 1.0762, time 151.9ms\n",
            "[ckpt_lora.pt] iter 2080: loss 1.0887, time 146.3ms\n",
            "[ckpt_lora.pt] iter 2090: loss 1.0769, time 148.3ms\n",
            "[ckpt_lora.pt] iter 2100: loss 1.1151, time 149.3ms\n",
            "[ckpt_lora.pt] iter 2110: loss 1.0829, time 148.0ms\n",
            "[ckpt_lora.pt] iter 2120: loss 1.0706, time 148.7ms\n",
            "[ckpt_lora.pt] iter 2130: loss 1.0841, time 151.1ms\n",
            "[ckpt_lora.pt] iter 2140: loss 1.0853, time 148.2ms\n",
            "[ckpt_lora.pt] iter 2150: loss 1.0951, time 149.9ms\n",
            "[ckpt_lora.pt] iter 2160: loss 1.0882, time 149.5ms\n",
            "[ckpt_lora.pt] iter 2170: loss 1.0505, time 147.0ms\n",
            "[ckpt_lora.pt] iter 2180: loss 1.0733, time 148.8ms\n",
            "[ckpt_lora.pt] iter 2190: loss 1.0610, time 148.2ms\n",
            "[ckpt_lora.pt] iter 2200: loss 1.1241, time 147.2ms\n",
            "[ckpt_lora.pt] iter 2210: loss 1.0786, time 148.9ms\n",
            "[ckpt_lora.pt] iter 2220: loss 1.1034, time 146.9ms\n",
            "[ckpt_lora.pt] iter 2230: loss 1.0605, time 148.7ms\n",
            "[ckpt_lora.pt] iter 2240: loss 1.0554, time 148.4ms\n",
            "[ckpt_lora.pt] iter 2250: train 0.8664, val 0.9369\n",
            "[ckpt_lora.pt] iter 2250: loss 1.0730, time 12776.0ms\n",
            "[ckpt_lora.pt] iter 2260: loss 1.1131, time 148.1ms\n",
            "[ckpt_lora.pt] iter 2270: loss 1.1023, time 148.7ms\n",
            "[ckpt_lora.pt] iter 2280: loss 1.0516, time 148.7ms\n",
            "[ckpt_lora.pt] iter 2290: loss 1.0741, time 151.0ms\n",
            "[ckpt_lora.pt] iter 2300: loss 1.1014, time 146.5ms\n",
            "[ckpt_lora.pt] iter 2310: loss 1.0642, time 149.0ms\n",
            "[ckpt_lora.pt] iter 2320: loss 1.0977, time 150.0ms\n",
            "[ckpt_lora.pt] iter 2330: loss 1.0605, time 149.0ms\n",
            "[ckpt_lora.pt] iter 2340: loss 1.0769, time 147.4ms\n",
            "[ckpt_lora.pt] iter 2350: loss 1.0696, time 150.2ms\n",
            "[ckpt_lora.pt] iter 2360: loss 1.0465, time 146.6ms\n",
            "[ckpt_lora.pt] iter 2370: loss 1.0741, time 150.6ms\n",
            "[ckpt_lora.pt] iter 2380: loss 1.0672, time 149.7ms\n",
            "[ckpt_lora.pt] iter 2390: loss 1.0724, time 147.2ms\n",
            "[ckpt_lora.pt] iter 2400: loss 1.0486, time 150.8ms\n",
            "[ckpt_lora.pt] iter 2410: loss 1.0659, time 149.7ms\n",
            "[ckpt_lora.pt] iter 2420: loss 1.0644, time 147.8ms\n",
            "[ckpt_lora.pt] iter 2430: loss 1.0674, time 147.9ms\n",
            "[ckpt_lora.pt] iter 2440: loss 1.0539, time 149.6ms\n",
            "[ckpt_lora.pt] iter 2450: loss 1.0709, time 150.0ms\n",
            "[ckpt_lora.pt] iter 2460: loss 1.0477, time 147.4ms\n",
            "[ckpt_lora.pt] iter 2470: loss 1.0794, time 149.4ms\n",
            "[ckpt_lora.pt] iter 2480: loss 1.0581, time 148.1ms\n",
            "[ckpt_lora.pt] iter 2490: loss 1.0382, time 150.9ms\n",
            "[ckpt_lora.pt] iter 2500: train 0.8644, val 0.9360\n",
            "[ckpt_lora.pt] iter 2500: loss 1.0773, time 12829.6ms\n",
            "[ckpt_lora.pt] iter 2510: loss 1.0531, time 149.6ms\n",
            "[ckpt_lora.pt] iter 2520: loss 1.0455, time 149.8ms\n",
            "[ckpt_lora.pt] iter 2530: loss 1.0343, time 146.9ms\n",
            "[ckpt_lora.pt] iter 2540: loss 1.0469, time 150.7ms\n",
            "[ckpt_lora.pt] iter 2550: loss 1.0288, time 148.0ms\n",
            "[ckpt_lora.pt] iter 2560: loss 1.0464, time 147.0ms\n",
            "[ckpt_lora.pt] iter 2570: loss 1.0371, time 151.8ms\n",
            "[ckpt_lora.pt] iter 2580: loss 1.0735, time 149.8ms\n",
            "[ckpt_lora.pt] iter 2590: loss 1.0413, time 148.7ms\n",
            "[ckpt_lora.pt] iter 2600: loss 1.0643, time 148.1ms\n",
            "[ckpt_lora.pt] iter 2610: loss 1.0397, time 148.8ms\n",
            "[ckpt_lora.pt] iter 2620: loss 1.0706, time 147.8ms\n",
            "[ckpt_lora.pt] iter 2630: loss 1.0461, time 148.0ms\n",
            "[ckpt_lora.pt] iter 2640: loss 1.0639, time 149.1ms\n",
            "[ckpt_lora.pt] iter 2650: loss 1.0544, time 147.4ms\n",
            "[ckpt_lora.pt] iter 2660: loss 1.0120, time 148.8ms\n",
            "[ckpt_lora.pt] iter 2670: loss 1.0654, time 147.6ms\n",
            "[ckpt_lora.pt] iter 2680: loss 1.0723, time 149.2ms\n",
            "[ckpt_lora.pt] iter 2690: loss 1.0624, time 148.2ms\n",
            "[ckpt_lora.pt] iter 2700: loss 1.0694, time 151.3ms\n",
            "[ckpt_lora.pt] iter 2710: loss 1.0364, time 146.6ms\n",
            "[ckpt_lora.pt] iter 2720: loss 1.0200, time 151.0ms\n",
            "[ckpt_lora.pt] iter 2730: loss 1.0312, time 148.6ms\n",
            "[ckpt_lora.pt] iter 2740: loss 1.0296, time 148.8ms\n",
            "[ckpt_lora.pt] iter 2750: train 0.8647, val 0.9357\n",
            "[ckpt_lora.pt] iter 2750: loss 1.0414, time 12806.4ms\n",
            "[ckpt_lora.pt] iter 2760: loss 1.0313, time 150.2ms\n",
            "[ckpt_lora.pt] iter 2770: loss 1.0289, time 149.3ms\n",
            "[ckpt_lora.pt] iter 2780: loss 1.0645, time 150.9ms\n",
            "[ckpt_lora.pt] iter 2790: loss 1.0719, time 149.0ms\n",
            "[ckpt_lora.pt] iter 2800: loss 1.0434, time 149.4ms\n",
            "[ckpt_lora.pt] iter 2810: loss 1.0125, time 148.7ms\n",
            "[ckpt_lora.pt] iter 2820: loss 1.0215, time 149.7ms\n",
            "[ckpt_lora.pt] iter 2830: loss 1.0308, time 150.2ms\n",
            "[ckpt_lora.pt] iter 2840: loss 1.0183, time 147.8ms\n",
            "[ckpt_lora.pt] iter 2850: loss 1.0352, time 151.4ms\n",
            "[ckpt_lora.pt] iter 2860: loss 1.0433, time 148.0ms\n",
            "[ckpt_lora.pt] iter 2870: loss 1.0623, time 147.4ms\n",
            "[ckpt_lora.pt] iter 2880: loss 1.0558, time 150.1ms\n",
            "[ckpt_lora.pt] iter 2890: loss 1.0318, time 146.7ms\n",
            "[ckpt_lora.pt] iter 2900: loss 1.0293, time 149.8ms\n",
            "[ckpt_lora.pt] iter 2910: loss 0.9997, time 148.7ms\n",
            "[ckpt_lora.pt] iter 2920: loss 1.0361, time 149.1ms\n",
            "[ckpt_lora.pt] iter 2930: loss 1.0425, time 147.9ms\n",
            "[ckpt_lora.pt] iter 2940: loss 1.0125, time 148.3ms\n",
            "[ckpt_lora.pt] iter 2950: loss 0.9962, time 148.1ms\n",
            "[ckpt_lora.pt] iter 2960: loss 1.0271, time 147.3ms\n",
            "[ckpt_lora.pt] iter 2970: loss 1.0059, time 148.4ms\n",
            "[ckpt_lora.pt] iter 2980: loss 1.0068, time 148.5ms\n",
            "[ckpt_lora.pt] iter 2990: loss 1.0328, time 147.7ms\n",
            "[ckpt_lora.pt] iter 3000: train 0.8635, val 0.9346\n",
            "[ckpt_lora.pt] iter 3000: loss 1.0461, time 12800.1ms\n",
            "[ckpt_lora.pt] iter 3010: loss 0.9900, time 152.1ms\n",
            "[ckpt_lora.pt] iter 3020: loss 1.0115, time 149.1ms\n",
            "[ckpt_lora.pt] iter 3030: loss 1.0065, time 150.7ms\n",
            "[ckpt_lora.pt] iter 3040: loss 1.0262, time 148.7ms\n",
            "[ckpt_lora.pt] iter 3050: loss 1.0341, time 147.8ms\n",
            "[ckpt_lora.pt] iter 3060: loss 1.0203, time 150.1ms\n",
            "[ckpt_lora.pt] iter 3070: loss 1.0226, time 148.4ms\n",
            "[ckpt_lora.pt] iter 3080: loss 1.0469, time 149.0ms\n",
            "[ckpt_lora.pt] iter 3090: loss 1.0374, time 148.5ms\n",
            "[ckpt_lora.pt] iter 3100: loss 1.0352, time 148.5ms\n",
            "[ckpt_lora.pt] iter 3110: loss 1.0097, time 150.2ms\n",
            "[ckpt_lora.pt] iter 3120: loss 1.0305, time 148.2ms\n",
            "[ckpt_lora.pt] iter 3130: loss 1.0326, time 149.2ms\n",
            "[ckpt_lora.pt] iter 3140: loss 1.0168, time 148.4ms\n",
            "[ckpt_lora.pt] iter 3150: loss 0.9828, time 149.3ms\n",
            "[ckpt_lora.pt] iter 3160: loss 1.0211, time 149.1ms\n",
            "[ckpt_lora.pt] iter 3170: loss 1.0263, time 147.8ms\n",
            "[ckpt_lora.pt] iter 3180: loss 1.0229, time 147.5ms\n",
            "[ckpt_lora.pt] iter 3190: loss 1.0153, time 148.6ms\n",
            "[ckpt_lora.pt] iter 3200: loss 1.0207, time 148.2ms\n",
            "[ckpt_lora.pt] iter 3210: loss 1.0086, time 148.8ms\n",
            "[ckpt_lora.pt] iter 3220: loss 1.0010, time 150.5ms\n",
            "[ckpt_lora.pt] iter 3230: loss 1.0235, time 148.8ms\n",
            "[ckpt_lora.pt] iter 3240: loss 1.0195, time 147.5ms\n",
            "[ckpt_lora.pt] iter 3250: train 0.8652, val 0.9338\n",
            "[ckpt_lora.pt] iter 3250: loss 1.0337, time 12865.6ms\n",
            "[ckpt_lora.pt] iter 3260: loss 1.0054, time 149.8ms\n",
            "[ckpt_lora.pt] iter 3270: loss 1.0137, time 147.3ms\n",
            "[ckpt_lora.pt] iter 3280: loss 1.0172, time 150.2ms\n",
            "[ckpt_lora.pt] iter 3290: loss 0.9997, time 149.0ms\n",
            "[ckpt_lora.pt] iter 3300: loss 1.0501, time 149.6ms\n",
            "[ckpt_lora.pt] iter 3310: loss 0.9890, time 148.3ms\n",
            "[ckpt_lora.pt] iter 3320: loss 0.9913, time 149.4ms\n",
            "[ckpt_lora.pt] iter 3330: loss 1.0270, time 147.0ms\n",
            "[ckpt_lora.pt] iter 3340: loss 0.9718, time 149.7ms\n",
            "[ckpt_lora.pt] iter 3350: loss 1.0098, time 148.4ms\n",
            "[ckpt_lora.pt] iter 3360: loss 0.9886, time 150.0ms\n",
            "[ckpt_lora.pt] iter 3370: loss 0.9993, time 150.0ms\n",
            "[ckpt_lora.pt] iter 3380: loss 1.0004, time 148.0ms\n",
            "[ckpt_lora.pt] iter 3390: loss 0.9909, time 150.1ms\n",
            "[ckpt_lora.pt] iter 3400: loss 0.9873, time 146.7ms\n",
            "[ckpt_lora.pt] iter 3410: loss 0.9991, time 150.7ms\n",
            "[ckpt_lora.pt] iter 3420: loss 0.9889, time 148.1ms\n",
            "[ckpt_lora.pt] iter 3430: loss 1.0130, time 148.2ms\n",
            "[ckpt_lora.pt] iter 3440: loss 0.9728, time 147.5ms\n",
            "[ckpt_lora.pt] iter 3450: loss 0.9866, time 149.7ms\n",
            "[ckpt_lora.pt] iter 3460: loss 1.0030, time 148.1ms\n",
            "[ckpt_lora.pt] iter 3470: loss 0.9969, time 149.7ms\n",
            "[ckpt_lora.pt] iter 3480: loss 1.0101, time 148.0ms\n",
            "[ckpt_lora.pt] iter 3490: loss 1.0176, time 149.2ms\n",
            "[ckpt_lora.pt] iter 3500: train 0.8653, val 0.9373\n",
            "[ckpt_lora.pt] iter 3500: loss 1.0128, time 12831.3ms\n",
            "[ckpt_lora.pt] iter 3510: loss 0.9928, time 147.7ms\n",
            "[ckpt_lora.pt] iter 3520: loss 0.9709, time 149.1ms\n",
            "[ckpt_lora.pt] iter 3530: loss 1.0043, time 149.4ms\n",
            "[ckpt_lora.pt] iter 3540: loss 0.9969, time 151.6ms\n",
            "[ckpt_lora.pt] iter 3550: loss 0.9818, time 150.8ms\n",
            "[ckpt_lora.pt] iter 3560: loss 0.9690, time 149.3ms\n",
            "[ckpt_lora.pt] iter 3570: loss 0.9663, time 150.8ms\n",
            "[ckpt_lora.pt] iter 3580: loss 0.9898, time 148.9ms\n",
            "[ckpt_lora.pt] iter 3590: loss 0.9887, time 148.5ms\n",
            "[ckpt_lora.pt] iter 3600: loss 0.9752, time 149.2ms\n",
            "[ckpt_lora.pt] iter 3610: loss 0.9757, time 149.1ms\n",
            "[ckpt_lora.pt] iter 3620: loss 0.9994, time 151.3ms\n",
            "[ckpt_lora.pt] iter 3630: loss 0.9818, time 147.9ms\n",
            "[ckpt_lora.pt] iter 3640: loss 1.0017, time 147.9ms\n",
            "[ckpt_lora.pt] iter 3650: loss 0.9780, time 148.4ms\n",
            "[ckpt_lora.pt] iter 3660: loss 0.9828, time 146.8ms\n",
            "[ckpt_lora.pt] iter 3670: loss 0.9662, time 151.0ms\n",
            "[ckpt_lora.pt] iter 3680: loss 0.9818, time 149.9ms\n",
            "[ckpt_lora.pt] iter 3690: loss 1.0199, time 148.9ms\n",
            "[ckpt_lora.pt] iter 3700: loss 0.9811, time 147.0ms\n",
            "[ckpt_lora.pt] iter 3710: loss 0.9541, time 148.5ms\n",
            "[ckpt_lora.pt] iter 3720: loss 0.9665, time 150.0ms\n",
            "[ckpt_lora.pt] iter 3730: loss 0.9941, time 149.1ms\n",
            "[ckpt_lora.pt] iter 3740: loss 0.9804, time 149.9ms\n",
            "[ckpt_lora.pt] iter 3750: train 0.8660, val 0.9375\n",
            "[ckpt_lora.pt] iter 3750: loss 0.9868, time 12818.2ms\n",
            "[ckpt_lora.pt] iter 3760: loss 0.9594, time 148.0ms\n",
            "[ckpt_lora.pt] iter 3770: loss 1.0058, time 153.7ms\n",
            "[ckpt_lora.pt] iter 3780: loss 0.9957, time 147.9ms\n",
            "[ckpt_lora.pt] iter 3790: loss 0.9648, time 151.2ms\n",
            "[ckpt_lora.pt] iter 3800: loss 0.9763, time 148.8ms\n",
            "[ckpt_lora.pt] iter 3810: loss 0.9707, time 149.8ms\n",
            "[ckpt_lora.pt] iter 3820: loss 0.9827, time 148.0ms\n",
            "[ckpt_lora.pt] iter 3830: loss 0.9803, time 147.7ms\n",
            "[ckpt_lora.pt] iter 3840: loss 0.9811, time 150.0ms\n",
            "[ckpt_lora.pt] iter 3850: loss 0.9716, time 147.4ms\n",
            "[ckpt_lora.pt] iter 3860: loss 0.9730, time 150.8ms\n",
            "[ckpt_lora.pt] iter 3870: loss 0.9912, time 149.2ms\n",
            "[ckpt_lora.pt] iter 3880: loss 0.9878, time 148.4ms\n",
            "[ckpt_lora.pt] iter 3890: loss 0.9757, time 148.9ms\n",
            "[ckpt_lora.pt] iter 3900: loss 0.9521, time 147.5ms\n",
            "[ckpt_lora.pt] iter 3910: loss 0.9719, time 150.2ms\n",
            "[ckpt_lora.pt] iter 3920: loss 0.9725, time 149.3ms\n",
            "[ckpt_lora.pt] iter 3930: loss 0.9640, time 149.2ms\n",
            "[ckpt_lora.pt] iter 3940: loss 0.9763, time 149.3ms\n",
            "[ckpt_lora.pt] iter 3950: loss 0.9862, time 148.0ms\n",
            "[ckpt_lora.pt] iter 3960: loss 0.9870, time 151.0ms\n",
            "[ckpt_lora.pt] iter 3970: loss 1.0171, time 148.5ms\n",
            "[ckpt_lora.pt] iter 3980: loss 0.9723, time 150.0ms\n",
            "[ckpt_lora.pt] iter 3990: loss 0.9686, time 150.0ms\n",
            "[ckpt_lora.pt] iter 4000: train 0.8643, val 0.9367\n",
            "[ckpt_lora.pt] iter 4000: loss 0.9670, time 12828.5ms\n",
            "[ckpt_lora.pt] iter 4010: loss 0.9587, time 149.7ms\n",
            "[ckpt_lora.pt] iter 4020: loss 0.9853, time 148.1ms\n",
            "[ckpt_lora.pt] iter 4030: loss 0.9848, time 147.8ms\n",
            "[ckpt_lora.pt] iter 4040: loss 0.9700, time 149.0ms\n",
            "[ckpt_lora.pt] iter 4050: loss 0.9645, time 150.0ms\n",
            "[ckpt_lora.pt] iter 4060: loss 0.9778, time 147.9ms\n",
            "[ckpt_lora.pt] iter 4070: loss 0.9583, time 151.1ms\n",
            "[ckpt_lora.pt] iter 4080: loss 0.9593, time 148.4ms\n",
            "[ckpt_lora.pt] iter 4090: loss 0.9426, time 140.3ms\n",
            "[ckpt_lora.pt] iter 4100: loss 0.9833, time 147.1ms\n",
            "[ckpt_lora.pt] iter 4110: loss 0.9875, time 151.2ms\n",
            "[ckpt_lora.pt] iter 4120: loss 0.9606, time 147.9ms\n",
            "[ckpt_lora.pt] iter 4130: loss 0.9510, time 150.0ms\n",
            "[ckpt_lora.pt] iter 4140: loss 0.9797, time 150.9ms\n",
            "[ckpt_lora.pt] iter 4150: loss 0.9744, time 149.2ms\n",
            "[ckpt_lora.pt] iter 4160: loss 0.9675, time 150.9ms\n",
            "[ckpt_lora.pt] iter 4170: loss 0.9712, time 148.8ms\n",
            "[ckpt_lora.pt] iter 4180: loss 0.9642, time 150.2ms\n",
            "[ckpt_lora.pt] iter 4190: loss 0.9769, time 149.1ms\n",
            "[ckpt_lora.pt] iter 4200: loss 0.9695, time 149.6ms\n",
            "[ckpt_lora.pt] iter 4210: loss 0.9974, time 148.7ms\n",
            "[ckpt_lora.pt] iter 4220: loss 0.9565, time 148.3ms\n",
            "[ckpt_lora.pt] iter 4230: loss 0.9707, time 151.1ms\n",
            "[ckpt_lora.pt] iter 4240: loss 0.9774, time 147.9ms\n",
            "[ckpt_lora.pt] iter 4250: train 0.8636, val 0.9352\n",
            "[ckpt_lora.pt] iter 4250: loss 0.9360, time 12780.4ms\n",
            "[ckpt_lora.pt] iter 4260: loss 0.9410, time 148.7ms\n",
            "[ckpt_lora.pt] iter 4270: loss 0.9604, time 148.2ms\n",
            "[ckpt_lora.pt] iter 4280: loss 0.9682, time 147.4ms\n",
            "[ckpt_lora.pt] iter 4290: loss 0.9678, time 149.1ms\n",
            "[ckpt_lora.pt] iter 4300: loss 0.9829, time 147.8ms\n",
            "[ckpt_lora.pt] iter 4310: loss 0.9902, time 148.0ms\n",
            "[ckpt_lora.pt] iter 4320: loss 0.9555, time 149.5ms\n",
            "[ckpt_lora.pt] iter 4330: loss 0.9600, time 149.9ms\n",
            "[ckpt_lora.pt] iter 4340: loss 0.9353, time 150.3ms\n",
            "[ckpt_lora.pt] iter 4350: loss 0.9756, time 148.1ms\n",
            "[ckpt_lora.pt] iter 4360: loss 0.9678, time 148.6ms\n",
            "[ckpt_lora.pt] iter 4370: loss 0.9771, time 149.6ms\n",
            "[ckpt_lora.pt] iter 4380: loss 0.9426, time 148.9ms\n",
            "[ckpt_lora.pt] iter 4390: loss 0.9705, time 148.5ms\n",
            "[ckpt_lora.pt] iter 4400: loss 0.9629, time 148.9ms\n",
            "[ckpt_lora.pt] iter 4410: loss 0.9954, time 148.7ms\n",
            "[ckpt_lora.pt] iter 4420: loss 0.9659, time 149.2ms\n",
            "[ckpt_lora.pt] iter 4430: loss 0.9694, time 148.5ms\n",
            "[ckpt_lora.pt] iter 4440: loss 0.9724, time 148.5ms\n",
            "[ckpt_lora.pt] iter 4450: loss 0.9762, time 148.4ms\n",
            "[ckpt_lora.pt] iter 4460: loss 0.9175, time 148.0ms\n",
            "[ckpt_lora.pt] iter 4470: loss 0.9597, time 149.3ms\n",
            "[ckpt_lora.pt] iter 4480: loss 0.9398, time 149.9ms\n",
            "[ckpt_lora.pt] iter 4490: loss 0.9391, time 148.9ms\n",
            "[ckpt_lora.pt] iter 4500: train 0.8638, val 0.9367\n",
            "[ckpt_lora.pt] iter 4500: loss 0.9612, time 13100.7ms\n",
            "[ckpt_lora.pt] iter 4510: loss 0.9374, time 147.2ms\n",
            "[ckpt_lora.pt] iter 4520: loss 0.9633, time 146.7ms\n",
            "[ckpt_lora.pt] iter 4530: loss 0.9864, time 149.9ms\n",
            "[ckpt_lora.pt] iter 4540: loss 0.9854, time 148.0ms\n",
            "[ckpt_lora.pt] iter 4550: loss 0.9597, time 148.9ms\n",
            "[ckpt_lora.pt] iter 4560: loss 0.9777, time 148.5ms\n",
            "[ckpt_lora.pt] iter 4570: loss 0.9604, time 149.0ms\n",
            "[ckpt_lora.pt] iter 4580: loss 0.9453, time 149.8ms\n",
            "[ckpt_lora.pt] iter 4590: loss 0.9554, time 148.4ms\n",
            "[ckpt_lora.pt] iter 4600: loss 0.9679, time 148.8ms\n",
            "[ckpt_lora.pt] iter 4610: loss 0.9470, time 149.9ms\n",
            "[ckpt_lora.pt] iter 4620: loss 0.9412, time 146.8ms\n",
            "[ckpt_lora.pt] iter 4630: loss 0.9315, time 149.5ms\n",
            "[ckpt_lora.pt] iter 4640: loss 0.9607, time 148.5ms\n",
            "[ckpt_lora.pt] iter 4650: loss 0.9833, time 148.7ms\n",
            "[ckpt_lora.pt] iter 4660: loss 0.9375, time 148.1ms\n",
            "[ckpt_lora.pt] iter 4670: loss 0.9471, time 148.5ms\n",
            "[ckpt_lora.pt] iter 4680: loss 0.9169, time 148.1ms\n",
            "[ckpt_lora.pt] iter 4690: loss 0.9120, time 148.6ms\n",
            "[ckpt_lora.pt] iter 4700: loss 0.9455, time 150.1ms\n",
            "[ckpt_lora.pt] iter 4710: loss 0.9578, time 149.0ms\n",
            "[ckpt_lora.pt] iter 4720: loss 0.9399, time 149.0ms\n",
            "[ckpt_lora.pt] iter 4730: loss 0.9461, time 148.4ms\n",
            "[ckpt_lora.pt] iter 4740: loss 0.9375, time 147.8ms\n",
            "[ckpt_lora.pt] iter 4750: train 0.8643, val 0.9335\n",
            "[ckpt_lora.pt] iter 4750: loss 0.9844, time 12810.7ms\n",
            "[ckpt_lora.pt] iter 4760: loss 0.9523, time 149.6ms\n",
            "[ckpt_lora.pt] iter 4770: loss 0.9471, time 149.6ms\n",
            "[ckpt_lora.pt] iter 4780: loss 0.9513, time 149.0ms\n",
            "[ckpt_lora.pt] iter 4790: loss 0.9542, time 149.4ms\n",
            "[ckpt_lora.pt] iter 4800: loss 0.9600, time 147.9ms\n",
            "[ckpt_lora.pt] iter 4810: loss 0.9604, time 148.5ms\n",
            "[ckpt_lora.pt] iter 4820: loss 0.9477, time 147.5ms\n",
            "[ckpt_lora.pt] iter 4830: loss 0.9646, time 148.2ms\n",
            "[ckpt_lora.pt] iter 4840: loss 0.9582, time 149.1ms\n",
            "[ckpt_lora.pt] iter 4850: loss 0.9540, time 150.4ms\n",
            "[ckpt_lora.pt] iter 4860: loss 0.9433, time 149.2ms\n",
            "[ckpt_lora.pt] iter 4870: loss 0.9119, time 149.6ms\n",
            "[ckpt_lora.pt] iter 4880: loss 0.9784, time 149.6ms\n",
            "[ckpt_lora.pt] iter 4890: loss 0.9486, time 147.2ms\n",
            "[ckpt_lora.pt] iter 4900: loss 0.9500, time 148.7ms\n",
            "[ckpt_lora.pt] iter 4910: loss 0.9489, time 148.6ms\n",
            "[ckpt_lora.pt] iter 4920: loss 0.9718, time 149.7ms\n",
            "[ckpt_lora.pt] iter 4930: loss 0.9592, time 148.6ms\n",
            "[ckpt_lora.pt] iter 4940: loss 0.9437, time 150.1ms\n",
            "[ckpt_lora.pt] iter 4950: loss 0.9484, time 148.6ms\n",
            "[ckpt_lora.pt] iter 4960: loss 0.9706, time 148.4ms\n",
            "[ckpt_lora.pt] iter 4970: loss 0.9317, time 150.6ms\n",
            "[ckpt_lora.pt] iter 4980: loss 0.9672, time 148.3ms\n",
            "[ckpt_lora.pt] iter 4990: loss 0.9170, time 149.3ms\n",
            "[ckpt_lora.pt] iter 5000: train 0.8665, val 0.9344\n",
            "[ckpt_lora.pt] final save at iter 5000 → /content/drive/MyDrive/ECS 189G/out/ckpt_lora.pt\n",
            "[ckpt_lora.pt] Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT + DAR"
      ],
      "metadata": {
        "id": "09PqVZyMSjvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell F: NanoGPT + DAR (no LoRA, no PAAF)\n",
        "\n",
        "train_and_save_variant(\n",
        "    out_dir       = out_dir,\n",
        "    ckpt_filename = 'ckpt_dar.pt',\n",
        "    use_lora      = False,\n",
        "    use_paaf      = False,\n",
        "    use_dar       = True,\n",
        "    dar_weight    = 0.01,    # you can change this penalty if desired\n",
        "    max_iters     = max_iters\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycnAL1ROSlkV",
        "outputId": "cebad509-d571-4a53-a20e-ed752c0b914f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 10.66M\n",
            "num decayed parameter tensors: 26, with 10,753,536 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-895d20065530>:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_local = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ckpt_dar.pt] iter 0: train 0.8672, val 0.9334\n",
            "[ckpt_dar.pt] iter 0: loss 4.6268, time 12573.0ms\n",
            "[ckpt_dar.pt] iter 10: loss 3.2125, time 97.1ms\n",
            "[ckpt_dar.pt] iter 20: loss 2.7086, time 99.5ms\n",
            "[ckpt_dar.pt] iter 30: loss 2.4963, time 96.8ms\n",
            "[ckpt_dar.pt] iter 40: loss 2.4014, time 97.3ms\n",
            "[ckpt_dar.pt] iter 50: loss 2.4060, time 99.3ms\n",
            "[ckpt_dar.pt] iter 60: loss 2.3763, time 96.2ms\n",
            "[ckpt_dar.pt] iter 70: loss 2.3420, time 99.8ms\n",
            "[ckpt_dar.pt] iter 80: loss 2.3444, time 99.6ms\n",
            "[ckpt_dar.pt] iter 90: loss 2.3541, time 98.0ms\n",
            "[ckpt_dar.pt] iter 100: loss 2.3433, time 96.1ms\n",
            "[ckpt_dar.pt] iter 110: loss 2.3411, time 99.7ms\n",
            "[ckpt_dar.pt] iter 120: loss 2.3224, time 100.0ms\n",
            "[ckpt_dar.pt] iter 130: loss 2.2983, time 101.5ms\n",
            "[ckpt_dar.pt] iter 140: loss 2.2860, time 99.4ms\n",
            "[ckpt_dar.pt] iter 150: loss 2.2526, time 99.6ms\n",
            "[ckpt_dar.pt] iter 160: loss 2.1917, time 97.5ms\n",
            "[ckpt_dar.pt] iter 170: loss 2.1624, time 97.4ms\n",
            "[ckpt_dar.pt] iter 180: loss 2.0892, time 100.6ms\n",
            "[ckpt_dar.pt] iter 190: loss 2.0384, time 96.8ms\n",
            "[ckpt_dar.pt] iter 200: loss 1.9906, time 98.5ms\n",
            "[ckpt_dar.pt] iter 210: loss 1.9828, time 100.2ms\n",
            "[ckpt_dar.pt] iter 220: loss 1.9305, time 99.9ms\n",
            "[ckpt_dar.pt] iter 230: loss 1.9125, time 99.3ms\n",
            "[ckpt_dar.pt] iter 240: loss 1.8745, time 99.7ms\n",
            "[ckpt_dar.pt] iter 250: train 0.8647, val 0.9341\n",
            "[ckpt_dar.pt] iter 250: loss 1.8449, time 12544.8ms\n",
            "[ckpt_dar.pt] iter 260: loss 1.8444, time 100.2ms\n",
            "[ckpt_dar.pt] iter 270: loss 1.7592, time 99.9ms\n",
            "[ckpt_dar.pt] iter 280: loss 1.7688, time 100.6ms\n",
            "[ckpt_dar.pt] iter 290: loss 1.7770, time 99.5ms\n",
            "[ckpt_dar.pt] iter 300: loss 1.7695, time 100.4ms\n",
            "[ckpt_dar.pt] iter 310: loss 1.7377, time 99.3ms\n",
            "[ckpt_dar.pt] iter 320: loss 1.6750, time 99.6ms\n",
            "[ckpt_dar.pt] iter 330: loss 1.6724, time 100.1ms\n",
            "[ckpt_dar.pt] iter 340: loss 1.6587, time 100.5ms\n",
            "[ckpt_dar.pt] iter 350: loss 1.6276, time 100.1ms\n",
            "[ckpt_dar.pt] iter 360: loss 1.6707, time 100.2ms\n",
            "[ckpt_dar.pt] iter 370: loss 1.6434, time 99.8ms\n",
            "[ckpt_dar.pt] iter 380: loss 1.6105, time 100.2ms\n",
            "[ckpt_dar.pt] iter 390: loss 1.5897, time 100.0ms\n",
            "[ckpt_dar.pt] iter 400: loss 1.5610, time 99.3ms\n",
            "[ckpt_dar.pt] iter 410: loss 1.6060, time 100.3ms\n",
            "[ckpt_dar.pt] iter 420: loss 1.5485, time 100.4ms\n",
            "[ckpt_dar.pt] iter 430: loss 1.5871, time 101.1ms\n",
            "[ckpt_dar.pt] iter 440: loss 1.5190, time 98.0ms\n",
            "[ckpt_dar.pt] iter 450: loss 1.5597, time 102.0ms\n",
            "[ckpt_dar.pt] iter 460: loss 1.5230, time 98.3ms\n",
            "[ckpt_dar.pt] iter 470: loss 1.5186, time 102.3ms\n",
            "[ckpt_dar.pt] iter 480: loss 1.5009, time 100.4ms\n",
            "[ckpt_dar.pt] iter 490: loss 1.4935, time 100.9ms\n",
            "[ckpt_dar.pt] iter 500: train 0.8643, val 0.9358\n",
            "[ckpt_dar.pt] iter 500: loss 1.5249, time 12754.4ms\n",
            "[ckpt_dar.pt] iter 510: loss 1.4639, time 102.9ms\n",
            "[ckpt_dar.pt] iter 520: loss 1.4851, time 103.5ms\n",
            "[ckpt_dar.pt] iter 530: loss 1.4713, time 101.9ms\n",
            "[ckpt_dar.pt] iter 540: loss 1.4678, time 102.6ms\n",
            "[ckpt_dar.pt] iter 550: loss 1.4392, time 102.2ms\n",
            "[ckpt_dar.pt] iter 560: loss 1.4401, time 100.7ms\n",
            "[ckpt_dar.pt] iter 570: loss 1.4304, time 102.4ms\n",
            "[ckpt_dar.pt] iter 580: loss 1.3993, time 102.6ms\n",
            "[ckpt_dar.pt] iter 590: loss 1.4249, time 103.0ms\n",
            "[ckpt_dar.pt] iter 600: loss 1.3979, time 103.5ms\n",
            "[ckpt_dar.pt] iter 610: loss 1.3995, time 103.8ms\n",
            "[ckpt_dar.pt] iter 620: loss 1.4047, time 102.2ms\n",
            "[ckpt_dar.pt] iter 630: loss 1.3783, time 102.8ms\n",
            "[ckpt_dar.pt] iter 640: loss 1.3877, time 102.5ms\n",
            "[ckpt_dar.pt] iter 650: loss 1.3778, time 103.3ms\n",
            "[ckpt_dar.pt] iter 660: loss 1.3694, time 102.5ms\n",
            "[ckpt_dar.pt] iter 670: loss 1.3768, time 103.1ms\n",
            "[ckpt_dar.pt] iter 680: loss 1.3527, time 104.7ms\n",
            "[ckpt_dar.pt] iter 690: loss 1.3669, time 102.5ms\n",
            "[ckpt_dar.pt] iter 700: loss 1.3772, time 104.0ms\n",
            "[ckpt_dar.pt] iter 710: loss 1.3488, time 103.5ms\n",
            "[ckpt_dar.pt] iter 720: loss 1.3457, time 104.0ms\n",
            "[ckpt_dar.pt] iter 730: loss 1.3635, time 105.7ms\n",
            "[ckpt_dar.pt] iter 740: loss 1.3290, time 104.4ms\n",
            "[ckpt_dar.pt] iter 750: train 0.8637, val 0.9367\n",
            "[ckpt_dar.pt] iter 750: loss 1.3505, time 12999.2ms\n",
            "[ckpt_dar.pt] iter 760: loss 1.3175, time 110.4ms\n",
            "[ckpt_dar.pt] iter 770: loss 1.2929, time 109.1ms\n",
            "[ckpt_dar.pt] iter 780: loss 1.3176, time 107.9ms\n",
            "[ckpt_dar.pt] iter 790: loss 1.2999, time 110.6ms\n",
            "[ckpt_dar.pt] iter 800: loss 1.2786, time 107.9ms\n",
            "[ckpt_dar.pt] iter 810: loss 1.3338, time 108.2ms\n",
            "[ckpt_dar.pt] iter 820: loss 1.2903, time 107.4ms\n",
            "[ckpt_dar.pt] iter 830: loss 1.2660, time 108.1ms\n",
            "[ckpt_dar.pt] iter 840: loss 1.2780, time 106.1ms\n",
            "[ckpt_dar.pt] iter 850: loss 1.2820, time 108.0ms\n",
            "[ckpt_dar.pt] iter 860: loss 1.2994, time 105.7ms\n",
            "[ckpt_dar.pt] iter 870: loss 1.2854, time 106.5ms\n",
            "[ckpt_dar.pt] iter 880: loss 1.2606, time 104.8ms\n",
            "[ckpt_dar.pt] iter 890: loss 1.2809, time 105.7ms\n",
            "[ckpt_dar.pt] iter 900: loss 1.2396, time 106.1ms\n",
            "[ckpt_dar.pt] iter 910: loss 1.2645, time 104.8ms\n",
            "[ckpt_dar.pt] iter 920: loss 1.2849, time 104.6ms\n",
            "[ckpt_dar.pt] iter 930: loss 1.2918, time 104.9ms\n",
            "[ckpt_dar.pt] iter 940: loss 1.2473, time 101.5ms\n",
            "[ckpt_dar.pt] iter 950: loss 1.2453, time 102.1ms\n",
            "[ckpt_dar.pt] iter 960: loss 1.2489, time 104.0ms\n",
            "[ckpt_dar.pt] iter 970: loss 1.2524, time 103.7ms\n",
            "[ckpt_dar.pt] iter 980: loss 1.2194, time 103.0ms\n",
            "[ckpt_dar.pt] iter 990: loss 1.2607, time 103.7ms\n",
            "[ckpt_dar.pt] iter 1000: train 0.8656, val 0.9349\n",
            "[ckpt_dar.pt] iter 1000: loss 1.2417, time 12955.3ms\n",
            "[ckpt_dar.pt] iter 1010: loss 1.2364, time 103.9ms\n",
            "[ckpt_dar.pt] iter 1020: loss 1.2549, time 103.2ms\n",
            "[ckpt_dar.pt] iter 1030: loss 1.2060, time 102.0ms\n",
            "[ckpt_dar.pt] iter 1040: loss 1.2124, time 103.0ms\n",
            "[ckpt_dar.pt] iter 1050: loss 1.1801, time 101.8ms\n",
            "[ckpt_dar.pt] iter 1060: loss 1.2242, time 101.9ms\n",
            "[ckpt_dar.pt] iter 1070: loss 1.2197, time 102.4ms\n",
            "[ckpt_dar.pt] iter 1080: loss 1.2366, time 101.6ms\n",
            "[ckpt_dar.pt] iter 1090: loss 1.2091, time 102.4ms\n",
            "[ckpt_dar.pt] iter 1100: loss 1.2068, time 102.0ms\n",
            "[ckpt_dar.pt] iter 1110: loss 1.2043, time 101.4ms\n",
            "[ckpt_dar.pt] iter 1120: loss 1.2219, time 101.9ms\n",
            "[ckpt_dar.pt] iter 1130: loss 1.1844, time 102.5ms\n",
            "[ckpt_dar.pt] iter 1140: loss 1.2132, time 102.2ms\n",
            "[ckpt_dar.pt] iter 1150: loss 1.2521, time 102.9ms\n",
            "[ckpt_dar.pt] iter 1160: loss 1.2104, time 101.1ms\n",
            "[ckpt_dar.pt] iter 1170: loss 1.1945, time 103.1ms\n",
            "[ckpt_dar.pt] iter 1180: loss 1.2034, time 105.5ms\n",
            "[ckpt_dar.pt] iter 1190: loss 1.1718, time 101.2ms\n",
            "[ckpt_dar.pt] iter 1200: loss 1.2259, time 103.1ms\n",
            "[ckpt_dar.pt] iter 1210: loss 1.2010, time 102.5ms\n",
            "[ckpt_dar.pt] iter 1220: loss 1.2096, time 102.0ms\n",
            "[ckpt_dar.pt] iter 1230: loss 1.2046, time 103.3ms\n",
            "[ckpt_dar.pt] iter 1240: loss 1.1864, time 103.2ms\n",
            "[ckpt_dar.pt] iter 1250: train 0.8652, val 0.9341\n",
            "[ckpt_dar.pt] iter 1250: loss 1.1746, time 13070.3ms\n",
            "[ckpt_dar.pt] iter 1260: loss 1.1994, time 105.4ms\n",
            "[ckpt_dar.pt] iter 1270: loss 1.1572, time 104.3ms\n",
            "[ckpt_dar.pt] iter 1280: loss 1.1970, time 104.0ms\n",
            "[ckpt_dar.pt] iter 1290: loss 1.1791, time 102.7ms\n",
            "[ckpt_dar.pt] iter 1300: loss 1.1949, time 103.8ms\n",
            "[ckpt_dar.pt] iter 1310: loss 1.1952, time 103.9ms\n",
            "[ckpt_dar.pt] iter 1320: loss 1.2040, time 102.4ms\n",
            "[ckpt_dar.pt] iter 1330: loss 1.1609, time 103.9ms\n",
            "[ckpt_dar.pt] iter 1340: loss 1.1627, time 103.5ms\n",
            "[ckpt_dar.pt] iter 1350: loss 1.1765, time 104.0ms\n",
            "[ckpt_dar.pt] iter 1360: loss 1.1787, time 101.5ms\n",
            "[ckpt_dar.pt] iter 1370: loss 1.1522, time 104.0ms\n",
            "[ckpt_dar.pt] iter 1380: loss 1.1842, time 103.0ms\n",
            "[ckpt_dar.pt] iter 1390: loss 1.1590, time 102.9ms\n",
            "[ckpt_dar.pt] iter 1400: loss 1.1565, time 103.6ms\n",
            "[ckpt_dar.pt] iter 1410: loss 1.1619, time 101.8ms\n",
            "[ckpt_dar.pt] iter 1420: loss 1.1349, time 103.6ms\n",
            "[ckpt_dar.pt] iter 1430: loss 1.1712, time 104.3ms\n",
            "[ckpt_dar.pt] iter 1440: loss 1.1426, time 103.6ms\n",
            "[ckpt_dar.pt] iter 1450: loss 1.1382, time 104.4ms\n",
            "[ckpt_dar.pt] iter 1460: loss 1.1691, time 102.9ms\n",
            "[ckpt_dar.pt] iter 1470: loss 1.1715, time 101.9ms\n",
            "[ckpt_dar.pt] iter 1480: loss 1.1307, time 103.9ms\n",
            "[ckpt_dar.pt] iter 1490: loss 1.1405, time 103.7ms\n",
            "[ckpt_dar.pt] iter 1500: train 0.8646, val 0.9370\n",
            "[ckpt_dar.pt] iter 1500: loss 1.1397, time 12840.7ms\n",
            "[ckpt_dar.pt] iter 1510: loss 1.1252, time 104.8ms\n",
            "[ckpt_dar.pt] iter 1520: loss 1.1147, time 101.8ms\n",
            "[ckpt_dar.pt] iter 1530: loss 1.1268, time 103.7ms\n",
            "[ckpt_dar.pt] iter 1540: loss 1.1400, time 102.6ms\n",
            "[ckpt_dar.pt] iter 1550: loss 1.1255, time 102.8ms\n",
            "[ckpt_dar.pt] iter 1560: loss 1.1220, time 102.6ms\n",
            "[ckpt_dar.pt] iter 1570: loss 1.1467, time 101.4ms\n",
            "[ckpt_dar.pt] iter 1580: loss 1.1417, time 102.5ms\n",
            "[ckpt_dar.pt] iter 1590: loss 1.1197, time 101.1ms\n",
            "[ckpt_dar.pt] iter 1600: loss 1.1193, time 102.7ms\n",
            "[ckpt_dar.pt] iter 1610: loss 1.1283, time 103.2ms\n",
            "[ckpt_dar.pt] iter 1620: loss 1.1228, time 104.2ms\n",
            "[ckpt_dar.pt] iter 1630: loss 1.1564, time 103.3ms\n",
            "[ckpt_dar.pt] iter 1640: loss 1.1293, time 101.9ms\n",
            "[ckpt_dar.pt] iter 1650: loss 1.1070, time 103.2ms\n",
            "[ckpt_dar.pt] iter 1660: loss 1.1204, time 104.0ms\n",
            "[ckpt_dar.pt] iter 1670: loss 1.1156, time 102.8ms\n",
            "[ckpt_dar.pt] iter 1680: loss 1.1082, time 105.2ms\n",
            "[ckpt_dar.pt] iter 1690: loss 1.1095, time 102.2ms\n",
            "[ckpt_dar.pt] iter 1700: loss 1.1390, time 102.8ms\n",
            "[ckpt_dar.pt] iter 1710: loss 1.1018, time 103.2ms\n",
            "[ckpt_dar.pt] iter 1720: loss 1.1026, time 100.4ms\n",
            "[ckpt_dar.pt] iter 1730: loss 1.1111, time 102.8ms\n",
            "[ckpt_dar.pt] iter 1740: loss 1.1120, time 103.6ms\n",
            "[ckpt_dar.pt] iter 1750: train 0.8658, val 0.9352\n",
            "[ckpt_dar.pt] iter 1750: loss 1.1211, time 12861.8ms\n",
            "[ckpt_dar.pt] iter 1760: loss 1.1319, time 102.9ms\n",
            "[ckpt_dar.pt] iter 1770: loss 1.1195, time 104.2ms\n",
            "[ckpt_dar.pt] iter 1780: loss 1.0912, time 105.4ms\n",
            "[ckpt_dar.pt] iter 1790: loss 1.1104, time 101.8ms\n",
            "[ckpt_dar.pt] iter 1800: loss 1.0823, time 103.7ms\n",
            "[ckpt_dar.pt] iter 1810: loss 1.1241, time 104.5ms\n",
            "[ckpt_dar.pt] iter 1820: loss 1.1338, time 104.0ms\n",
            "[ckpt_dar.pt] iter 1830: loss 1.0976, time 104.0ms\n",
            "[ckpt_dar.pt] iter 1840: loss 1.1141, time 102.8ms\n",
            "[ckpt_dar.pt] iter 1850: loss 1.0956, time 102.7ms\n",
            "[ckpt_dar.pt] iter 1860: loss 1.1062, time 102.8ms\n",
            "[ckpt_dar.pt] iter 1870: loss 1.0910, time 102.5ms\n",
            "[ckpt_dar.pt] iter 1880: loss 1.0861, time 102.8ms\n",
            "[ckpt_dar.pt] iter 1890: loss 1.1064, time 103.0ms\n",
            "[ckpt_dar.pt] iter 1900: loss 1.0951, time 103.6ms\n",
            "[ckpt_dar.pt] iter 1910: loss 1.1115, time 104.0ms\n",
            "[ckpt_dar.pt] iter 1920: loss 1.0918, time 104.0ms\n",
            "[ckpt_dar.pt] iter 1930: loss 1.0660, time 103.7ms\n",
            "[ckpt_dar.pt] iter 1940: loss 1.0718, time 102.5ms\n",
            "[ckpt_dar.pt] iter 1950: loss 1.1062, time 106.2ms\n",
            "[ckpt_dar.pt] iter 1960: loss 1.1020, time 103.1ms\n",
            "[ckpt_dar.pt] iter 1970: loss 1.0930, time 102.1ms\n",
            "[ckpt_dar.pt] iter 1980: loss 1.1023, time 103.5ms\n",
            "[ckpt_dar.pt] iter 1990: loss 1.0593, time 101.3ms\n",
            "[ckpt_dar.pt] iter 2000: train 0.8668, val 0.9369\n",
            "[ckpt_dar.pt] iter 2000: loss 1.0841, time 12842.2ms\n",
            "[ckpt_dar.pt] iter 2010: loss 1.0669, time 103.9ms\n",
            "[ckpt_dar.pt] iter 2020: loss 1.0923, time 102.8ms\n",
            "[ckpt_dar.pt] iter 2030: loss 1.0975, time 103.9ms\n",
            "[ckpt_dar.pt] iter 2040: loss 1.0876, time 102.7ms\n",
            "[ckpt_dar.pt] iter 2050: loss 1.1037, time 103.7ms\n",
            "[ckpt_dar.pt] iter 2060: loss 1.0706, time 102.1ms\n",
            "[ckpt_dar.pt] iter 2070: loss 1.0692, time 102.9ms\n",
            "[ckpt_dar.pt] iter 2080: loss 1.0894, time 102.2ms\n",
            "[ckpt_dar.pt] iter 2090: loss 1.0988, time 103.6ms\n",
            "[ckpt_dar.pt] iter 2100: loss 1.1127, time 102.6ms\n",
            "[ckpt_dar.pt] iter 2110: loss 1.0782, time 103.4ms\n",
            "[ckpt_dar.pt] iter 2120: loss 1.0854, time 104.6ms\n",
            "[ckpt_dar.pt] iter 2130: loss 1.0906, time 103.7ms\n",
            "[ckpt_dar.pt] iter 2140: loss 1.0667, time 102.3ms\n",
            "[ckpt_dar.pt] iter 2150: loss 1.0592, time 105.9ms\n",
            "[ckpt_dar.pt] iter 2160: loss 1.0541, time 102.1ms\n",
            "[ckpt_dar.pt] iter 2170: loss 1.0628, time 104.0ms\n",
            "[ckpt_dar.pt] iter 2180: loss 1.0798, time 104.6ms\n",
            "[ckpt_dar.pt] iter 2190: loss 1.0640, time 103.7ms\n",
            "[ckpt_dar.pt] iter 2200: loss 1.0786, time 102.1ms\n",
            "[ckpt_dar.pt] iter 2210: loss 1.0592, time 102.6ms\n",
            "[ckpt_dar.pt] iter 2220: loss 1.0757, time 100.7ms\n",
            "[ckpt_dar.pt] iter 2230: loss 1.0535, time 104.3ms\n",
            "[ckpt_dar.pt] iter 2240: loss 1.0561, time 103.0ms\n",
            "[ckpt_dar.pt] iter 2250: train 0.8641, val 0.9352\n",
            "[ckpt_dar.pt] iter 2250: loss 1.0512, time 12832.9ms\n",
            "[ckpt_dar.pt] iter 2260: loss 1.0534, time 104.9ms\n",
            "[ckpt_dar.pt] iter 2270: loss 1.0519, time 103.4ms\n",
            "[ckpt_dar.pt] iter 2280: loss 1.0313, time 103.7ms\n",
            "[ckpt_dar.pt] iter 2290: loss 1.0563, time 102.0ms\n",
            "[ckpt_dar.pt] iter 2300: loss 1.0696, time 101.9ms\n",
            "[ckpt_dar.pt] iter 2310: loss 1.0396, time 102.3ms\n",
            "[ckpt_dar.pt] iter 2320: loss 1.0757, time 102.6ms\n",
            "[ckpt_dar.pt] iter 2330: loss 1.0815, time 103.3ms\n",
            "[ckpt_dar.pt] iter 2340: loss 1.0154, time 103.3ms\n",
            "[ckpt_dar.pt] iter 2350: loss 1.0798, time 103.7ms\n",
            "[ckpt_dar.pt] iter 2360: loss 1.0944, time 103.0ms\n",
            "[ckpt_dar.pt] iter 2370: loss 1.0404, time 103.2ms\n",
            "[ckpt_dar.pt] iter 2380: loss 1.0400, time 104.1ms\n",
            "[ckpt_dar.pt] iter 2390: loss 1.0284, time 103.0ms\n",
            "[ckpt_dar.pt] iter 2400: loss 1.0512, time 103.3ms\n",
            "[ckpt_dar.pt] iter 2410: loss 1.0496, time 101.3ms\n",
            "[ckpt_dar.pt] iter 2420: loss 1.0256, time 103.3ms\n",
            "[ckpt_dar.pt] iter 2430: loss 1.0371, time 103.1ms\n",
            "[ckpt_dar.pt] iter 2440: loss 1.0118, time 103.5ms\n",
            "[ckpt_dar.pt] iter 2450: loss 1.0637, time 103.4ms\n",
            "[ckpt_dar.pt] iter 2460: loss 1.0429, time 101.4ms\n",
            "[ckpt_dar.pt] iter 2470: loss 1.0368, time 101.8ms\n",
            "[ckpt_dar.pt] iter 2480: loss 1.0305, time 103.7ms\n",
            "[ckpt_dar.pt] iter 2490: loss 1.0386, time 101.4ms\n",
            "[ckpt_dar.pt] iter 2500: train 0.8622, val 0.9338\n",
            "[ckpt_dar.pt] iter 2500: loss 0.9953, time 13145.3ms\n",
            "[ckpt_dar.pt] iter 2510: loss 1.0284, time 103.9ms\n",
            "[ckpt_dar.pt] iter 2520: loss 1.0166, time 103.4ms\n",
            "[ckpt_dar.pt] iter 2530: loss 1.0396, time 103.4ms\n",
            "[ckpt_dar.pt] iter 2540: loss 1.0356, time 101.8ms\n",
            "[ckpt_dar.pt] iter 2550: loss 1.0284, time 102.8ms\n",
            "[ckpt_dar.pt] iter 2560: loss 1.0246, time 101.8ms\n",
            "[ckpt_dar.pt] iter 2570: loss 1.0607, time 103.3ms\n",
            "[ckpt_dar.pt] iter 2580: loss 1.0445, time 104.8ms\n",
            "[ckpt_dar.pt] iter 2590: loss 1.0388, time 103.2ms\n",
            "[ckpt_dar.pt] iter 2600: loss 1.0462, time 104.3ms\n",
            "[ckpt_dar.pt] iter 2610: loss 1.0474, time 104.0ms\n",
            "[ckpt_dar.pt] iter 2620: loss 1.0245, time 102.6ms\n",
            "[ckpt_dar.pt] iter 2630: loss 1.0172, time 104.5ms\n",
            "[ckpt_dar.pt] iter 2640: loss 1.0553, time 103.6ms\n",
            "[ckpt_dar.pt] iter 2650: loss 1.0325, time 102.4ms\n",
            "[ckpt_dar.pt] iter 2660: loss 1.0411, time 103.1ms\n",
            "[ckpt_dar.pt] iter 2670: loss 1.0429, time 101.7ms\n",
            "[ckpt_dar.pt] iter 2680: loss 1.0494, time 102.1ms\n",
            "[ckpt_dar.pt] iter 2690: loss 0.9920, time 102.3ms\n",
            "[ckpt_dar.pt] iter 2700: loss 1.0413, time 104.7ms\n",
            "[ckpt_dar.pt] iter 2710: loss 0.9886, time 101.9ms\n",
            "[ckpt_dar.pt] iter 2720: loss 1.0160, time 104.4ms\n",
            "[ckpt_dar.pt] iter 2730: loss 1.0041, time 103.6ms\n",
            "[ckpt_dar.pt] iter 2740: loss 1.0174, time 103.3ms\n",
            "[ckpt_dar.pt] iter 2750: train 0.8640, val 0.9369\n",
            "[ckpt_dar.pt] iter 2750: loss 1.0073, time 12902.5ms\n",
            "[ckpt_dar.pt] iter 2760: loss 1.0422, time 103.1ms\n",
            "[ckpt_dar.pt] iter 2770: loss 1.0465, time 104.3ms\n",
            "[ckpt_dar.pt] iter 2780: loss 1.0361, time 102.4ms\n",
            "[ckpt_dar.pt] iter 2790: loss 1.0493, time 103.8ms\n",
            "[ckpt_dar.pt] iter 2800: loss 1.0050, time 106.0ms\n",
            "[ckpt_dar.pt] iter 2810: loss 1.0343, time 101.0ms\n",
            "[ckpt_dar.pt] iter 2820: loss 0.9762, time 102.5ms\n",
            "[ckpt_dar.pt] iter 2830: loss 1.0084, time 104.0ms\n",
            "[ckpt_dar.pt] iter 2840: loss 1.0258, time 104.0ms\n",
            "[ckpt_dar.pt] iter 2850: loss 1.0493, time 103.2ms\n",
            "[ckpt_dar.pt] iter 2860: loss 1.0160, time 103.3ms\n",
            "[ckpt_dar.pt] iter 2870: loss 0.9983, time 104.1ms\n",
            "[ckpt_dar.pt] iter 2880: loss 1.0227, time 105.8ms\n",
            "[ckpt_dar.pt] iter 2890: loss 1.0164, time 102.3ms\n",
            "[ckpt_dar.pt] iter 2900: loss 0.9992, time 104.1ms\n",
            "[ckpt_dar.pt] iter 2910: loss 1.0028, time 103.1ms\n",
            "[ckpt_dar.pt] iter 2920: loss 1.0196, time 104.0ms\n",
            "[ckpt_dar.pt] iter 2930: loss 1.0106, time 103.1ms\n",
            "[ckpt_dar.pt] iter 2940: loss 1.0055, time 101.8ms\n",
            "[ckpt_dar.pt] iter 2950: loss 0.9722, time 102.9ms\n",
            "[ckpt_dar.pt] iter 2960: loss 1.0163, time 102.6ms\n",
            "[ckpt_dar.pt] iter 2970: loss 1.0239, time 102.5ms\n",
            "[ckpt_dar.pt] iter 2980: loss 0.9882, time 103.5ms\n",
            "[ckpt_dar.pt] iter 2990: loss 1.0030, time 104.5ms\n",
            "[ckpt_dar.pt] iter 3000: train 0.8639, val 0.9353\n",
            "[ckpt_dar.pt] iter 3000: loss 0.9941, time 12915.2ms\n",
            "[ckpt_dar.pt] iter 3010: loss 1.0066, time 102.8ms\n",
            "[ckpt_dar.pt] iter 3020: loss 0.9874, time 103.4ms\n",
            "[ckpt_dar.pt] iter 3030: loss 0.9848, time 101.8ms\n",
            "[ckpt_dar.pt] iter 3040: loss 1.0105, time 103.5ms\n",
            "[ckpt_dar.pt] iter 3050: loss 0.9991, time 103.0ms\n",
            "[ckpt_dar.pt] iter 3060: loss 1.0145, time 102.7ms\n",
            "[ckpt_dar.pt] iter 3070: loss 0.9857, time 103.6ms\n",
            "[ckpt_dar.pt] iter 3080: loss 1.0082, time 101.9ms\n",
            "[ckpt_dar.pt] iter 3090: loss 0.9707, time 102.9ms\n",
            "[ckpt_dar.pt] iter 3100: loss 1.0097, time 103.3ms\n",
            "[ckpt_dar.pt] iter 3110: loss 0.9712, time 102.6ms\n",
            "[ckpt_dar.pt] iter 3120: loss 0.9542, time 101.3ms\n",
            "[ckpt_dar.pt] iter 3130: loss 1.0092, time 103.5ms\n",
            "[ckpt_dar.pt] iter 3140: loss 0.9752, time 102.7ms\n",
            "[ckpt_dar.pt] iter 3150: loss 0.9728, time 102.2ms\n",
            "[ckpt_dar.pt] iter 3160: loss 0.9843, time 103.9ms\n",
            "[ckpt_dar.pt] iter 3170: loss 0.9948, time 101.9ms\n",
            "[ckpt_dar.pt] iter 3180: loss 0.9792, time 104.8ms\n",
            "[ckpt_dar.pt] iter 3190: loss 0.9935, time 103.9ms\n",
            "[ckpt_dar.pt] iter 3200: loss 0.9867, time 103.0ms\n",
            "[ckpt_dar.pt] iter 3210: loss 0.9867, time 104.1ms\n",
            "[ckpt_dar.pt] iter 3220: loss 0.9911, time 103.6ms\n",
            "[ckpt_dar.pt] iter 3230: loss 1.0157, time 103.8ms\n",
            "[ckpt_dar.pt] iter 3240: loss 1.0086, time 102.9ms\n",
            "[ckpt_dar.pt] iter 3250: train 0.8650, val 0.9345\n",
            "[ckpt_dar.pt] iter 3250: loss 1.0100, time 12814.6ms\n",
            "[ckpt_dar.pt] iter 3260: loss 1.0093, time 103.7ms\n",
            "[ckpt_dar.pt] iter 3270: loss 0.9733, time 103.2ms\n",
            "[ckpt_dar.pt] iter 3280: loss 0.9773, time 103.7ms\n",
            "[ckpt_dar.pt] iter 3290: loss 0.9980, time 101.8ms\n",
            "[ckpt_dar.pt] iter 3300: loss 0.9948, time 105.0ms\n",
            "[ckpt_dar.pt] iter 3310: loss 0.9944, time 101.2ms\n",
            "[ckpt_dar.pt] iter 3320: loss 0.9875, time 103.9ms\n",
            "[ckpt_dar.pt] iter 3330: loss 1.0199, time 103.3ms\n",
            "[ckpt_dar.pt] iter 3340: loss 0.9959, time 101.1ms\n",
            "[ckpt_dar.pt] iter 3350: loss 0.9817, time 102.7ms\n",
            "[ckpt_dar.pt] iter 3360: loss 0.9562, time 102.9ms\n",
            "[ckpt_dar.pt] iter 3370: loss 0.9849, time 102.9ms\n",
            "[ckpt_dar.pt] iter 3380: loss 0.9953, time 103.2ms\n",
            "[ckpt_dar.pt] iter 3390: loss 0.9825, time 103.3ms\n",
            "[ckpt_dar.pt] iter 3400: loss 0.9774, time 103.7ms\n",
            "[ckpt_dar.pt] iter 3410: loss 0.9953, time 103.6ms\n",
            "[ckpt_dar.pt] iter 3420: loss 0.9772, time 104.1ms\n",
            "[ckpt_dar.pt] iter 3430: loss 0.9920, time 103.8ms\n",
            "[ckpt_dar.pt] iter 3440: loss 0.9756, time 103.3ms\n",
            "[ckpt_dar.pt] iter 3450: loss 0.9715, time 102.4ms\n",
            "[ckpt_dar.pt] iter 3460: loss 0.9916, time 102.9ms\n",
            "[ckpt_dar.pt] iter 3470: loss 0.9454, time 103.3ms\n",
            "[ckpt_dar.pt] iter 3480: loss 0.9611, time 102.2ms\n",
            "[ckpt_dar.pt] iter 3490: loss 0.9459, time 103.6ms\n",
            "[ckpt_dar.pt] iter 3500: train 0.8643, val 0.9354\n",
            "[ckpt_dar.pt] iter 3500: loss 0.9527, time 13018.4ms\n",
            "[ckpt_dar.pt] iter 3510: loss 0.9968, time 103.0ms\n",
            "[ckpt_dar.pt] iter 3520: loss 0.9789, time 105.4ms\n",
            "[ckpt_dar.pt] iter 3530: loss 0.9706, time 102.6ms\n",
            "[ckpt_dar.pt] iter 3540: loss 0.9384, time 103.8ms\n",
            "[ckpt_dar.pt] iter 3550: loss 0.9896, time 103.7ms\n",
            "[ckpt_dar.pt] iter 3560: loss 0.9500, time 103.1ms\n",
            "[ckpt_dar.pt] iter 3570: loss 0.9637, time 103.9ms\n",
            "[ckpt_dar.pt] iter 3580: loss 0.9681, time 102.0ms\n",
            "[ckpt_dar.pt] iter 3590: loss 0.9506, time 102.9ms\n",
            "[ckpt_dar.pt] iter 3600: loss 0.9881, time 103.2ms\n",
            "[ckpt_dar.pt] iter 3610: loss 0.9663, time 103.5ms\n",
            "[ckpt_dar.pt] iter 3620: loss 0.9771, time 101.9ms\n",
            "[ckpt_dar.pt] iter 3630: loss 0.9857, time 103.4ms\n",
            "[ckpt_dar.pt] iter 3640: loss 0.9687, time 104.5ms\n",
            "[ckpt_dar.pt] iter 3650: loss 1.0204, time 102.8ms\n",
            "[ckpt_dar.pt] iter 3660: loss 0.9396, time 102.3ms\n",
            "[ckpt_dar.pt] iter 3670: loss 0.9720, time 103.7ms\n",
            "[ckpt_dar.pt] iter 3680: loss 0.9597, time 104.1ms\n",
            "[ckpt_dar.pt] iter 3690: loss 0.9787, time 102.5ms\n",
            "[ckpt_dar.pt] iter 3700: loss 0.9785, time 102.3ms\n",
            "[ckpt_dar.pt] iter 3710: loss 0.9514, time 102.2ms\n",
            "[ckpt_dar.pt] iter 3720: loss 0.9524, time 104.4ms\n",
            "[ckpt_dar.pt] iter 3730: loss 0.9848, time 103.4ms\n",
            "[ckpt_dar.pt] iter 3740: loss 0.9514, time 103.4ms\n",
            "[ckpt_dar.pt] iter 3750: train 0.8656, val 0.9334\n",
            "[ckpt_dar.pt] saving checkpoint at iter 3750 → /content/drive/MyDrive/ECS 189G/out/ckpt_dar.pt\n",
            "[ckpt_dar.pt] iter 3750: loss 0.9528, time 13567.8ms\n",
            "[ckpt_dar.pt] iter 3760: loss 0.9542, time 102.6ms\n",
            "[ckpt_dar.pt] iter 3770: loss 0.9671, time 106.1ms\n",
            "[ckpt_dar.pt] iter 3780: loss 0.9679, time 105.1ms\n",
            "[ckpt_dar.pt] iter 3790: loss 0.9799, time 101.4ms\n",
            "[ckpt_dar.pt] iter 3800: loss 0.9824, time 103.1ms\n",
            "[ckpt_dar.pt] iter 3810: loss 0.9696, time 101.8ms\n",
            "[ckpt_dar.pt] iter 3820: loss 0.9607, time 102.4ms\n",
            "[ckpt_dar.pt] iter 3830: loss 0.9741, time 102.3ms\n",
            "[ckpt_dar.pt] iter 3840: loss 0.9837, time 103.0ms\n",
            "[ckpt_dar.pt] iter 3850: loss 0.9574, time 101.7ms\n",
            "[ckpt_dar.pt] iter 3860: loss 0.9641, time 89.2ms\n",
            "[ckpt_dar.pt] iter 3870: loss 0.9467, time 102.7ms\n",
            "[ckpt_dar.pt] iter 3880: loss 0.9700, time 103.0ms\n",
            "[ckpt_dar.pt] iter 3890: loss 0.9671, time 104.5ms\n",
            "[ckpt_dar.pt] iter 3900: loss 0.9450, time 101.0ms\n",
            "[ckpt_dar.pt] iter 3910: loss 0.9749, time 103.0ms\n",
            "[ckpt_dar.pt] iter 3920: loss 0.9791, time 103.6ms\n",
            "[ckpt_dar.pt] iter 3930: loss 0.9108, time 102.9ms\n",
            "[ckpt_dar.pt] iter 3940: loss 0.9556, time 103.2ms\n",
            "[ckpt_dar.pt] iter 3950: loss 0.9268, time 103.0ms\n",
            "[ckpt_dar.pt] iter 3960: loss 0.9875, time 102.7ms\n",
            "[ckpt_dar.pt] iter 3970: loss 0.9331, time 103.5ms\n",
            "[ckpt_dar.pt] iter 3980: loss 0.9595, time 103.4ms\n",
            "[ckpt_dar.pt] iter 3990: loss 0.9571, time 102.3ms\n",
            "[ckpt_dar.pt] iter 4000: train 0.8641, val 0.9375\n",
            "[ckpt_dar.pt] iter 4000: loss 0.9453, time 12837.4ms\n",
            "[ckpt_dar.pt] iter 4010: loss 0.9620, time 104.5ms\n",
            "[ckpt_dar.pt] iter 4020: loss 0.9278, time 101.3ms\n",
            "[ckpt_dar.pt] iter 4030: loss 0.9288, time 104.3ms\n",
            "[ckpt_dar.pt] iter 4040: loss 0.9315, time 101.6ms\n",
            "[ckpt_dar.pt] iter 4050: loss 0.9248, time 102.9ms\n",
            "[ckpt_dar.pt] iter 4060: loss 0.9277, time 102.8ms\n",
            "[ckpt_dar.pt] iter 4070: loss 0.9233, time 103.0ms\n",
            "[ckpt_dar.pt] iter 4080: loss 0.9406, time 103.7ms\n",
            "[ckpt_dar.pt] iter 4090: loss 0.9463, time 102.5ms\n",
            "[ckpt_dar.pt] iter 4100: loss 0.9430, time 103.7ms\n",
            "[ckpt_dar.pt] iter 4110: loss 0.9315, time 102.9ms\n",
            "[ckpt_dar.pt] iter 4120: loss 0.9255, time 103.2ms\n",
            "[ckpt_dar.pt] iter 4130: loss 0.9601, time 103.5ms\n",
            "[ckpt_dar.pt] iter 4140: loss 0.9407, time 101.9ms\n",
            "[ckpt_dar.pt] iter 4150: loss 0.9757, time 101.5ms\n",
            "[ckpt_dar.pt] iter 4160: loss 0.9639, time 103.6ms\n",
            "[ckpt_dar.pt] iter 4170: loss 0.9619, time 102.3ms\n",
            "[ckpt_dar.pt] iter 4180: loss 0.9730, time 102.9ms\n",
            "[ckpt_dar.pt] iter 4190: loss 0.9290, time 105.0ms\n",
            "[ckpt_dar.pt] iter 4200: loss 0.9406, time 101.9ms\n",
            "[ckpt_dar.pt] iter 4210: loss 0.9254, time 99.9ms\n",
            "[ckpt_dar.pt] iter 4220: loss 0.9279, time 102.5ms\n",
            "[ckpt_dar.pt] iter 4230: loss 0.9393, time 100.8ms\n",
            "[ckpt_dar.pt] iter 4240: loss 0.9452, time 103.1ms\n",
            "[ckpt_dar.pt] iter 4250: train 0.8625, val 0.9366\n",
            "[ckpt_dar.pt] iter 4250: loss 0.9543, time 13107.7ms\n",
            "[ckpt_dar.pt] iter 4260: loss 0.9390, time 104.6ms\n",
            "[ckpt_dar.pt] iter 4270: loss 0.9435, time 104.3ms\n",
            "[ckpt_dar.pt] iter 4280: loss 0.9353, time 103.0ms\n",
            "[ckpt_dar.pt] iter 4290: loss 0.8990, time 104.9ms\n",
            "[ckpt_dar.pt] iter 4300: loss 0.9461, time 104.5ms\n",
            "[ckpt_dar.pt] iter 4310: loss 0.9492, time 104.8ms\n",
            "[ckpt_dar.pt] iter 4320: loss 0.9654, time 104.7ms\n",
            "[ckpt_dar.pt] iter 4330: loss 0.9416, time 105.0ms\n",
            "[ckpt_dar.pt] iter 4340: loss 0.9452, time 103.2ms\n",
            "[ckpt_dar.pt] iter 4350: loss 0.9438, time 109.7ms\n",
            "[ckpt_dar.pt] iter 4360: loss 0.9523, time 106.0ms\n",
            "[ckpt_dar.pt] iter 4370: loss 0.9528, time 106.6ms\n",
            "[ckpt_dar.pt] iter 4380: loss 0.9122, time 107.1ms\n",
            "[ckpt_dar.pt] iter 4390: loss 0.9527, time 111.1ms\n",
            "[ckpt_dar.pt] iter 4400: loss 0.9754, time 105.7ms\n",
            "[ckpt_dar.pt] iter 4410: loss 0.9302, time 110.2ms\n",
            "[ckpt_dar.pt] iter 4420: loss 0.9524, time 112.9ms\n",
            "[ckpt_dar.pt] iter 4430: loss 0.9397, time 110.5ms\n",
            "[ckpt_dar.pt] iter 4440: loss 0.9314, time 112.5ms\n",
            "[ckpt_dar.pt] iter 4450: loss 0.9459, time 111.6ms\n",
            "[ckpt_dar.pt] iter 4460: loss 0.9551, time 111.4ms\n",
            "[ckpt_dar.pt] iter 4470: loss 0.9385, time 113.8ms\n",
            "[ckpt_dar.pt] iter 4480: loss 0.9300, time 110.0ms\n",
            "[ckpt_dar.pt] iter 4490: loss 0.9159, time 114.1ms\n",
            "[ckpt_dar.pt] iter 4500: train 0.8656, val 0.9350\n",
            "[ckpt_dar.pt] iter 4500: loss 0.9294, time 13284.6ms\n",
            "[ckpt_dar.pt] iter 4510: loss 0.9338, time 113.4ms\n",
            "[ckpt_dar.pt] iter 4520: loss 0.9266, time 109.9ms\n",
            "[ckpt_dar.pt] iter 4530: loss 0.9414, time 107.9ms\n",
            "[ckpt_dar.pt] iter 4540: loss 0.9416, time 109.2ms\n",
            "[ckpt_dar.pt] iter 4550: loss 0.9352, time 110.6ms\n",
            "[ckpt_dar.pt] iter 4560: loss 0.9460, time 105.6ms\n",
            "[ckpt_dar.pt] iter 4570: loss 0.9388, time 108.8ms\n",
            "[ckpt_dar.pt] iter 4580: loss 0.9323, time 108.6ms\n",
            "[ckpt_dar.pt] iter 4590: loss 0.9154, time 105.1ms\n",
            "[ckpt_dar.pt] iter 4600: loss 0.9340, time 104.7ms\n",
            "[ckpt_dar.pt] iter 4610: loss 0.9427, time 106.0ms\n",
            "[ckpt_dar.pt] iter 4620: loss 0.9427, time 114.2ms\n",
            "[ckpt_dar.pt] iter 4630: loss 0.9227, time 112.0ms\n",
            "[ckpt_dar.pt] iter 4640: loss 0.9280, time 108.9ms\n",
            "[ckpt_dar.pt] iter 4650: loss 0.9194, time 109.8ms\n",
            "[ckpt_dar.pt] iter 4660: loss 0.9227, time 107.8ms\n",
            "[ckpt_dar.pt] iter 4670: loss 0.9505, time 106.8ms\n",
            "[ckpt_dar.pt] iter 4680: loss 0.9092, time 109.7ms\n",
            "[ckpt_dar.pt] iter 4690: loss 0.9351, time 105.8ms\n",
            "[ckpt_dar.pt] iter 4700: loss 0.9048, time 108.0ms\n",
            "[ckpt_dar.pt] iter 4710: loss 0.9707, time 105.9ms\n",
            "[ckpt_dar.pt] iter 4720: loss 0.9296, time 108.9ms\n",
            "[ckpt_dar.pt] iter 4730: loss 0.9088, time 106.4ms\n",
            "[ckpt_dar.pt] iter 4740: loss 0.9549, time 105.1ms\n",
            "[ckpt_dar.pt] iter 4750: train 0.8649, val 0.9329\n",
            "[ckpt_dar.pt] saving checkpoint at iter 4750 → /content/drive/MyDrive/ECS 189G/out/ckpt_dar.pt\n",
            "[ckpt_dar.pt] iter 4750: loss 0.9336, time 13666.4ms\n",
            "[ckpt_dar.pt] iter 4760: loss 0.9555, time 107.0ms\n",
            "[ckpt_dar.pt] iter 4770: loss 0.9340, time 112.0ms\n",
            "[ckpt_dar.pt] iter 4780: loss 0.9459, time 105.7ms\n",
            "[ckpt_dar.pt] iter 4790: loss 0.9172, time 109.2ms\n",
            "[ckpt_dar.pt] iter 4800: loss 0.9208, time 105.5ms\n",
            "[ckpt_dar.pt] iter 4810: loss 0.9199, time 108.3ms\n",
            "[ckpt_dar.pt] iter 4820: loss 0.9088, time 104.8ms\n",
            "[ckpt_dar.pt] iter 4830: loss 0.9472, time 107.1ms\n",
            "[ckpt_dar.pt] iter 4840: loss 0.9302, time 110.0ms\n",
            "[ckpt_dar.pt] iter 4850: loss 0.9326, time 102.5ms\n",
            "[ckpt_dar.pt] iter 4860: loss 0.9052, time 100.9ms\n",
            "[ckpt_dar.pt] iter 4870: loss 0.9066, time 105.1ms\n",
            "[ckpt_dar.pt] iter 4880: loss 0.9409, time 106.5ms\n",
            "[ckpt_dar.pt] iter 4890: loss 0.9263, time 108.2ms\n",
            "[ckpt_dar.pt] iter 4900: loss 0.9125, time 105.6ms\n",
            "[ckpt_dar.pt] iter 4910: loss 0.9250, time 109.6ms\n",
            "[ckpt_dar.pt] iter 4920: loss 0.9079, time 108.5ms\n",
            "[ckpt_dar.pt] iter 4930: loss 0.9741, time 105.5ms\n",
            "[ckpt_dar.pt] iter 4940: loss 0.9504, time 106.7ms\n",
            "[ckpt_dar.pt] iter 4950: loss 0.9332, time 108.7ms\n",
            "[ckpt_dar.pt] iter 4960: loss 0.9181, time 106.0ms\n",
            "[ckpt_dar.pt] iter 4970: loss 0.9290, time 109.9ms\n",
            "[ckpt_dar.pt] iter 4980: loss 0.9325, time 108.3ms\n",
            "[ckpt_dar.pt] iter 4990: loss 0.9246, time 109.9ms\n",
            "[ckpt_dar.pt] iter 5000: train 0.8660, val 0.9358\n",
            "[ckpt_dar.pt] final save at iter 5000 → /content/drive/MyDrive/ECS 189G/out/ckpt_dar.pt\n",
            "[ckpt_dar.pt] Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT + DAR + LoRA"
      ],
      "metadata": {
        "id": "Y1NfPQpNSqWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell G: NanoGPT + LoRA + DAR (no PAAF)\n",
        "\n",
        "train_and_save_variant(\n",
        "    out_dir       = out_dir,\n",
        "    ckpt_filename = 'ckpt_lora_dar.pt',\n",
        "    use_lora      = True,\n",
        "    use_paaf      = False,\n",
        "    use_dar       = True,\n",
        "    dar_weight    = 0.01,\n",
        "    max_iters     = max_iters\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-uNsRsFSt8E",
        "outputId": "e0cd7eca-68d3-4a4c-9ea2-f404a4172632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 10.66M\n",
            "--> Injecting LoRA adapters (rank=8, alpha=16.0)\n",
            "num decayed parameter tensors: 74, with 11,048,448 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-895d20065530>:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_local = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ckpt_lora_dar.pt] iter 0: train 0.8636, val 0.9353\n",
            "[ckpt_lora_dar.pt] iter 0: loss 5.1199, time 12409.9ms\n",
            "[ckpt_lora_dar.pt] iter 10: loss 3.6971, time 143.3ms\n",
            "[ckpt_lora_dar.pt] iter 20: loss 3.2122, time 144.8ms\n",
            "[ckpt_lora_dar.pt] iter 30: loss 3.0199, time 144.2ms\n",
            "[ckpt_lora_dar.pt] iter 40: loss 2.9235, time 143.5ms\n",
            "[ckpt_lora_dar.pt] iter 50: loss 2.8649, time 144.5ms\n",
            "[ckpt_lora_dar.pt] iter 60: loss 2.8545, time 143.0ms\n",
            "[ckpt_lora_dar.pt] iter 70: loss 2.7886, time 142.9ms\n",
            "[ckpt_lora_dar.pt] iter 80: loss 2.7784, time 142.8ms\n",
            "[ckpt_lora_dar.pt] iter 90: loss 2.7112, time 144.0ms\n",
            "[ckpt_lora_dar.pt] iter 100: loss 2.6712, time 142.7ms\n",
            "[ckpt_lora_dar.pt] iter 110: loss 2.6342, time 145.0ms\n",
            "[ckpt_lora_dar.pt] iter 120: loss 2.6083, time 143.9ms\n",
            "[ckpt_lora_dar.pt] iter 130: loss 2.5563, time 144.2ms\n",
            "[ckpt_lora_dar.pt] iter 140: loss 2.5078, time 144.5ms\n",
            "[ckpt_lora_dar.pt] iter 150: loss 2.4360, time 144.5ms\n",
            "[ckpt_lora_dar.pt] iter 160: loss 2.3443, time 145.6ms\n",
            "[ckpt_lora_dar.pt] iter 170: loss 2.2777, time 143.9ms\n",
            "[ckpt_lora_dar.pt] iter 180: loss 2.2494, time 145.4ms\n",
            "[ckpt_lora_dar.pt] iter 190: loss 2.1248, time 146.0ms\n",
            "[ckpt_lora_dar.pt] iter 200: loss 2.0755, time 144.8ms\n",
            "[ckpt_lora_dar.pt] iter 210: loss 2.0434, time 146.3ms\n",
            "[ckpt_lora_dar.pt] iter 220: loss 2.0050, time 146.7ms\n",
            "[ckpt_lora_dar.pt] iter 230: loss 1.9813, time 143.6ms\n",
            "[ckpt_lora_dar.pt] iter 240: loss 1.9220, time 146.9ms\n",
            "[ckpt_lora_dar.pt] iter 250: train 0.8652, val 0.9364\n",
            "[ckpt_lora_dar.pt] iter 250: loss 1.8900, time 12464.7ms\n",
            "[ckpt_lora_dar.pt] iter 260: loss 1.8432, time 146.3ms\n",
            "[ckpt_lora_dar.pt] iter 270: loss 1.8615, time 146.9ms\n",
            "[ckpt_lora_dar.pt] iter 280: loss 1.8115, time 145.8ms\n",
            "[ckpt_lora_dar.pt] iter 290: loss 1.7898, time 147.2ms\n",
            "[ckpt_lora_dar.pt] iter 300: loss 1.7774, time 146.3ms\n",
            "[ckpt_lora_dar.pt] iter 310: loss 1.7625, time 146.6ms\n",
            "[ckpt_lora_dar.pt] iter 320: loss 1.7337, time 146.9ms\n",
            "[ckpt_lora_dar.pt] iter 330: loss 1.7500, time 145.7ms\n",
            "[ckpt_lora_dar.pt] iter 340: loss 1.7108, time 145.4ms\n",
            "[ckpt_lora_dar.pt] iter 350: loss 1.6726, time 147.6ms\n",
            "[ckpt_lora_dar.pt] iter 360: loss 1.6657, time 148.1ms\n",
            "[ckpt_lora_dar.pt] iter 370: loss 1.6339, time 147.6ms\n",
            "[ckpt_lora_dar.pt] iter 380: loss 1.6301, time 146.6ms\n",
            "[ckpt_lora_dar.pt] iter 390: loss 1.5840, time 146.4ms\n",
            "[ckpt_lora_dar.pt] iter 400: loss 1.6206, time 147.5ms\n",
            "[ckpt_lora_dar.pt] iter 410: loss 1.5926, time 148.8ms\n",
            "[ckpt_lora_dar.pt] iter 420: loss 1.5805, time 146.3ms\n",
            "[ckpt_lora_dar.pt] iter 430: loss 1.5692, time 146.7ms\n",
            "[ckpt_lora_dar.pt] iter 440: loss 1.5623, time 147.6ms\n",
            "[ckpt_lora_dar.pt] iter 450: loss 1.5667, time 147.9ms\n",
            "[ckpt_lora_dar.pt] iter 460: loss 1.5420, time 147.0ms\n",
            "[ckpt_lora_dar.pt] iter 470: loss 1.5322, time 148.1ms\n",
            "[ckpt_lora_dar.pt] iter 480: loss 1.5188, time 147.0ms\n",
            "[ckpt_lora_dar.pt] iter 490: loss 1.5261, time 148.1ms\n",
            "[ckpt_lora_dar.pt] iter 500: train 0.8652, val 0.9355\n",
            "[ckpt_lora_dar.pt] iter 500: loss 1.4824, time 12622.6ms\n",
            "[ckpt_lora_dar.pt] iter 510: loss 1.5029, time 146.9ms\n",
            "[ckpt_lora_dar.pt] iter 520: loss 1.4654, time 149.8ms\n",
            "[ckpt_lora_dar.pt] iter 530: loss 1.4993, time 148.1ms\n",
            "[ckpt_lora_dar.pt] iter 540: loss 1.4621, time 148.9ms\n",
            "[ckpt_lora_dar.pt] iter 550: loss 1.4500, time 147.7ms\n",
            "[ckpt_lora_dar.pt] iter 560: loss 1.4483, time 147.5ms\n",
            "[ckpt_lora_dar.pt] iter 570: loss 1.4311, time 148.4ms\n",
            "[ckpt_lora_dar.pt] iter 580: loss 1.4596, time 147.6ms\n",
            "[ckpt_lora_dar.pt] iter 590: loss 1.4472, time 147.7ms\n",
            "[ckpt_lora_dar.pt] iter 600: loss 1.4235, time 148.7ms\n",
            "[ckpt_lora_dar.pt] iter 610: loss 1.4207, time 147.3ms\n",
            "[ckpt_lora_dar.pt] iter 620: loss 1.4113, time 148.5ms\n",
            "[ckpt_lora_dar.pt] iter 630: loss 1.3941, time 148.1ms\n",
            "[ckpt_lora_dar.pt] iter 640: loss 1.3718, time 146.8ms\n",
            "[ckpt_lora_dar.pt] iter 650: loss 1.3559, time 147.1ms\n",
            "[ckpt_lora_dar.pt] iter 660: loss 1.3937, time 148.2ms\n",
            "[ckpt_lora_dar.pt] iter 670: loss 1.3946, time 147.0ms\n",
            "[ckpt_lora_dar.pt] iter 680: loss 1.3597, time 148.0ms\n",
            "[ckpt_lora_dar.pt] iter 690: loss 1.3730, time 148.0ms\n",
            "[ckpt_lora_dar.pt] iter 700: loss 1.3771, time 147.0ms\n",
            "[ckpt_lora_dar.pt] iter 710: loss 1.3490, time 148.3ms\n",
            "[ckpt_lora_dar.pt] iter 720: loss 1.3440, time 148.0ms\n",
            "[ckpt_lora_dar.pt] iter 730: loss 1.3392, time 148.4ms\n",
            "[ckpt_lora_dar.pt] iter 740: loss 1.3063, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 750: train 0.8641, val 0.9368\n",
            "[ckpt_lora_dar.pt] iter 750: loss 1.3664, time 12929.1ms\n",
            "[ckpt_lora_dar.pt] iter 760: loss 1.3085, time 148.3ms\n",
            "[ckpt_lora_dar.pt] iter 770: loss 1.3150, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 780: loss 1.3222, time 148.4ms\n",
            "[ckpt_lora_dar.pt] iter 790: loss 1.2963, time 148.6ms\n",
            "[ckpt_lora_dar.pt] iter 800: loss 1.2947, time 147.5ms\n",
            "[ckpt_lora_dar.pt] iter 810: loss 1.3154, time 148.1ms\n",
            "[ckpt_lora_dar.pt] iter 820: loss 1.3016, time 148.5ms\n",
            "[ckpt_lora_dar.pt] iter 830: loss 1.3054, time 148.9ms\n",
            "[ckpt_lora_dar.pt] iter 840: loss 1.3232, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 850: loss 1.3040, time 148.9ms\n",
            "[ckpt_lora_dar.pt] iter 860: loss 1.3102, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 870: loss 1.2822, time 147.1ms\n",
            "[ckpt_lora_dar.pt] iter 880: loss 1.3017, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 890: loss 1.2735, time 148.2ms\n",
            "[ckpt_lora_dar.pt] iter 900: loss 1.2764, time 148.3ms\n",
            "[ckpt_lora_dar.pt] iter 910: loss 1.2720, time 148.3ms\n",
            "[ckpt_lora_dar.pt] iter 920: loss 1.2815, time 147.9ms\n",
            "[ckpt_lora_dar.pt] iter 930: loss 1.3010, time 148.6ms\n",
            "[ckpt_lora_dar.pt] iter 940: loss 1.2560, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 950: loss 1.3022, time 147.8ms\n",
            "[ckpt_lora_dar.pt] iter 960: loss 1.2804, time 148.2ms\n",
            "[ckpt_lora_dar.pt] iter 970: loss 1.2504, time 147.4ms\n",
            "[ckpt_lora_dar.pt] iter 980: loss 1.2673, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 990: loss 1.2594, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 1000: train 0.8647, val 0.9350\n",
            "[ckpt_lora_dar.pt] saving checkpoint at iter 1000 → /content/drive/MyDrive/ECS 189G/out/ckpt_lora_dar.pt\n",
            "[ckpt_lora_dar.pt] iter 1000: loss 1.2705, time 13772.8ms\n",
            "[ckpt_lora_dar.pt] iter 1010: loss 1.2421, time 150.5ms\n",
            "[ckpt_lora_dar.pt] iter 1020: loss 1.2459, time 152.2ms\n",
            "[ckpt_lora_dar.pt] iter 1030: loss 1.2342, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 1040: loss 1.2767, time 152.1ms\n",
            "[ckpt_lora_dar.pt] iter 1050: loss 1.2256, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 1060: loss 1.2347, time 152.4ms\n",
            "[ckpt_lora_dar.pt] iter 1070: loss 1.2378, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 1080: loss 1.2140, time 151.4ms\n",
            "[ckpt_lora_dar.pt] iter 1090: loss 1.2441, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 1100: loss 1.2080, time 153.6ms\n",
            "[ckpt_lora_dar.pt] iter 1110: loss 1.2193, time 151.8ms\n",
            "[ckpt_lora_dar.pt] iter 1120: loss 1.2109, time 153.1ms\n",
            "[ckpt_lora_dar.pt] iter 1130: loss 1.2149, time 151.9ms\n",
            "[ckpt_lora_dar.pt] iter 1140: loss 1.2221, time 153.9ms\n",
            "[ckpt_lora_dar.pt] iter 1150: loss 1.1980, time 155.8ms\n",
            "[ckpt_lora_dar.pt] iter 1160: loss 1.2015, time 153.5ms\n",
            "[ckpt_lora_dar.pt] iter 1170: loss 1.2075, time 155.7ms\n",
            "[ckpt_lora_dar.pt] iter 1180: loss 1.2247, time 152.3ms\n",
            "[ckpt_lora_dar.pt] iter 1190: loss 1.2185, time 155.6ms\n",
            "[ckpt_lora_dar.pt] iter 1200: loss 1.2090, time 152.1ms\n",
            "[ckpt_lora_dar.pt] iter 1210: loss 1.2154, time 153.5ms\n",
            "[ckpt_lora_dar.pt] iter 1220: loss 1.2159, time 153.8ms\n",
            "[ckpt_lora_dar.pt] iter 1230: loss 1.1799, time 153.3ms\n",
            "[ckpt_lora_dar.pt] iter 1240: loss 1.1654, time 153.8ms\n",
            "[ckpt_lora_dar.pt] iter 1250: train 0.8669, val 0.9361\n",
            "[ckpt_lora_dar.pt] iter 1250: loss 1.1803, time 13103.3ms\n",
            "[ckpt_lora_dar.pt] iter 1260: loss 1.1926, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 1270: loss 1.1755, time 152.0ms\n",
            "[ckpt_lora_dar.pt] iter 1280: loss 1.1856, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 1290: loss 1.1905, time 152.1ms\n",
            "[ckpt_lora_dar.pt] iter 1300: loss 1.1831, time 151.4ms\n",
            "[ckpt_lora_dar.pt] iter 1310: loss 1.1752, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 1320: loss 1.1912, time 150.2ms\n",
            "[ckpt_lora_dar.pt] iter 1330: loss 1.1757, time 153.8ms\n",
            "[ckpt_lora_dar.pt] iter 1340: loss 1.2028, time 150.2ms\n",
            "[ckpt_lora_dar.pt] iter 1350: loss 1.1849, time 151.4ms\n",
            "[ckpt_lora_dar.pt] iter 1360: loss 1.1814, time 151.4ms\n",
            "[ckpt_lora_dar.pt] iter 1370: loss 1.1677, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 1380: loss 1.1896, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 1390: loss 1.1816, time 151.6ms\n",
            "[ckpt_lora_dar.pt] iter 1400: loss 1.1940, time 152.3ms\n",
            "[ckpt_lora_dar.pt] iter 1410: loss 1.1574, time 152.3ms\n",
            "[ckpt_lora_dar.pt] iter 1420: loss 1.1727, time 151.4ms\n",
            "[ckpt_lora_dar.pt] iter 1430: loss 1.1436, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 1440: loss 1.1545, time 151.8ms\n",
            "[ckpt_lora_dar.pt] iter 1450: loss 1.1738, time 151.3ms\n",
            "[ckpt_lora_dar.pt] iter 1460: loss 1.1230, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 1470: loss 1.1477, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 1480: loss 1.1641, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 1490: loss 1.1279, time 151.5ms\n",
            "[ckpt_lora_dar.pt] iter 1500: train 0.8642, val 0.9349\n",
            "[ckpt_lora_dar.pt] saving checkpoint at iter 1500 → /content/drive/MyDrive/ECS 189G/out/ckpt_lora_dar.pt\n",
            "[ckpt_lora_dar.pt] iter 1500: loss 1.1348, time 13517.7ms\n",
            "[ckpt_lora_dar.pt] iter 1510: loss 1.1176, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 1520: loss 1.1330, time 153.7ms\n",
            "[ckpt_lora_dar.pt] iter 1530: loss 1.1414, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 1540: loss 1.1612, time 152.6ms\n",
            "[ckpt_lora_dar.pt] iter 1550: loss 1.1246, time 152.7ms\n",
            "[ckpt_lora_dar.pt] iter 1560: loss 1.1575, time 149.4ms\n",
            "[ckpt_lora_dar.pt] iter 1570: loss 1.1427, time 150.1ms\n",
            "[ckpt_lora_dar.pt] iter 1580: loss 1.1602, time 148.7ms\n",
            "[ckpt_lora_dar.pt] iter 1590: loss 1.1164, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 1600: loss 1.1304, time 150.5ms\n",
            "[ckpt_lora_dar.pt] iter 1610: loss 1.1294, time 152.2ms\n",
            "[ckpt_lora_dar.pt] iter 1620: loss 1.1314, time 152.0ms\n",
            "[ckpt_lora_dar.pt] iter 1630: loss 1.1415, time 149.8ms\n",
            "[ckpt_lora_dar.pt] iter 1640: loss 1.1334, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 1650: loss 1.1128, time 149.5ms\n",
            "[ckpt_lora_dar.pt] iter 1660: loss 1.1247, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 1670: loss 1.1582, time 150.2ms\n",
            "[ckpt_lora_dar.pt] iter 1680: loss 1.1295, time 153.8ms\n",
            "[ckpt_lora_dar.pt] iter 1690: loss 1.1238, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 1700: loss 1.0971, time 151.9ms\n",
            "[ckpt_lora_dar.pt] iter 1710: loss 1.1137, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 1720: loss 1.1235, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 1730: loss 1.1211, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 1740: loss 1.0848, time 152.4ms\n",
            "[ckpt_lora_dar.pt] iter 1750: train 0.8640, val 0.9344\n",
            "[ckpt_lora_dar.pt] saving checkpoint at iter 1750 → /content/drive/MyDrive/ECS 189G/out/ckpt_lora_dar.pt\n",
            "[ckpt_lora_dar.pt] iter 1750: loss 1.1454, time 13306.7ms\n",
            "[ckpt_lora_dar.pt] iter 1760: loss 1.1300, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 1770: loss 1.0835, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 1780: loss 1.1161, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 1790: loss 1.0908, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 1800: loss 1.0989, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 1810: loss 1.1270, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 1820: loss 1.1370, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 1830: loss 1.0930, time 148.6ms\n",
            "[ckpt_lora_dar.pt] iter 1840: loss 1.0950, time 151.8ms\n",
            "[ckpt_lora_dar.pt] iter 1850: loss 1.1305, time 149.0ms\n",
            "[ckpt_lora_dar.pt] iter 1860: loss 1.0778, time 136.2ms\n",
            "[ckpt_lora_dar.pt] iter 1870: loss 1.1179, time 151.3ms\n",
            "[ckpt_lora_dar.pt] iter 1880: loss 1.0812, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 1890: loss 1.1128, time 149.8ms\n",
            "[ckpt_lora_dar.pt] iter 1900: loss 1.1012, time 149.4ms\n",
            "[ckpt_lora_dar.pt] iter 1910: loss 1.0734, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 1920: loss 1.1153, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 1930: loss 1.0982, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 1940: loss 1.0848, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 1950: loss 1.0903, time 149.8ms\n",
            "[ckpt_lora_dar.pt] iter 1960: loss 1.1167, time 147.1ms\n",
            "[ckpt_lora_dar.pt] iter 1970: loss 1.0670, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 1980: loss 1.0745, time 149.8ms\n",
            "[ckpt_lora_dar.pt] iter 1990: loss 1.0865, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 2000: train 0.8623, val 0.9359\n",
            "[ckpt_lora_dar.pt] iter 2000: loss 1.0560, time 12808.7ms\n",
            "[ckpt_lora_dar.pt] iter 2010: loss 1.1154, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 2020: loss 1.0514, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 2030: loss 1.0651, time 149.8ms\n",
            "[ckpt_lora_dar.pt] iter 2040: loss 1.0869, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 2050: loss 1.1034, time 151.6ms\n",
            "[ckpt_lora_dar.pt] iter 2060: loss 1.0791, time 149.4ms\n",
            "[ckpt_lora_dar.pt] iter 2070: loss 1.0720, time 149.5ms\n",
            "[ckpt_lora_dar.pt] iter 2080: loss 1.0980, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 2090: loss 1.0810, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 2100: loss 1.0477, time 149.5ms\n",
            "[ckpt_lora_dar.pt] iter 2110: loss 1.0666, time 148.2ms\n",
            "[ckpt_lora_dar.pt] iter 2120: loss 1.0686, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 2130: loss 1.1031, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 2140: loss 1.0790, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 2150: loss 1.0615, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 2160: loss 1.0552, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 2170: loss 1.0646, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 2180: loss 1.0629, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 2190: loss 1.0713, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 2200: loss 1.0813, time 147.8ms\n",
            "[ckpt_lora_dar.pt] iter 2210: loss 1.1089, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 2220: loss 1.0779, time 151.5ms\n",
            "[ckpt_lora_dar.pt] iter 2230: loss 1.0659, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 2240: loss 1.0713, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 2250: train 0.8660, val 0.9368\n",
            "[ckpt_lora_dar.pt] iter 2250: loss 1.0961, time 12945.8ms\n",
            "[ckpt_lora_dar.pt] iter 2260: loss 1.0907, time 149.0ms\n",
            "[ckpt_lora_dar.pt] iter 2270: loss 1.0407, time 149.4ms\n",
            "[ckpt_lora_dar.pt] iter 2280: loss 1.0522, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 2290: loss 1.0865, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 2300: loss 1.0467, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 2310: loss 1.0420, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 2320: loss 1.0595, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 2330: loss 1.0439, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 2340: loss 1.0584, time 151.3ms\n",
            "[ckpt_lora_dar.pt] iter 2350: loss 1.0586, time 152.1ms\n",
            "[ckpt_lora_dar.pt] iter 2360: loss 1.0386, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 2370: loss 1.0467, time 147.7ms\n",
            "[ckpt_lora_dar.pt] iter 2380: loss 1.0633, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 2390: loss 1.0290, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 2400: loss 1.0536, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 2410: loss 1.0967, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 2420: loss 1.0473, time 150.5ms\n",
            "[ckpt_lora_dar.pt] iter 2430: loss 1.0485, time 150.1ms\n",
            "[ckpt_lora_dar.pt] iter 2440: loss 1.0465, time 149.8ms\n",
            "[ckpt_lora_dar.pt] iter 2450: loss 1.0090, time 152.2ms\n",
            "[ckpt_lora_dar.pt] iter 2460: loss 1.0250, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 2470: loss 1.0305, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 2480: loss 1.0475, time 149.0ms\n",
            "[ckpt_lora_dar.pt] iter 2490: loss 1.0188, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 2500: train 0.8649, val 0.9387\n",
            "[ckpt_lora_dar.pt] iter 2500: loss 1.0291, time 12717.8ms\n",
            "[ckpt_lora_dar.pt] iter 2510: loss 1.0727, time 148.7ms\n",
            "[ckpt_lora_dar.pt] iter 2520: loss 1.0456, time 152.6ms\n",
            "[ckpt_lora_dar.pt] iter 2530: loss 1.0204, time 149.1ms\n",
            "[ckpt_lora_dar.pt] iter 2540: loss 1.0803, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 2550: loss 1.0190, time 149.1ms\n",
            "[ckpt_lora_dar.pt] iter 2560: loss 1.0380, time 149.0ms\n",
            "[ckpt_lora_dar.pt] iter 2570: loss 1.0548, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 2580: loss 1.0186, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 2590: loss 1.0136, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 2600: loss 1.0259, time 149.0ms\n",
            "[ckpt_lora_dar.pt] iter 2610: loss 1.0460, time 152.6ms\n",
            "[ckpt_lora_dar.pt] iter 2620: loss 1.0302, time 150.5ms\n",
            "[ckpt_lora_dar.pt] iter 2630: loss 1.0058, time 150.2ms\n",
            "[ckpt_lora_dar.pt] iter 2640: loss 1.0570, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 2650: loss 1.0327, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 2660: loss 1.0295, time 148.7ms\n",
            "[ckpt_lora_dar.pt] iter 2670: loss 1.0288, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 2680: loss 0.9993, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 2690: loss 1.0273, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 2700: loss 1.0259, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 2710: loss 1.0379, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 2720: loss 1.0404, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 2730: loss 1.0236, time 152.0ms\n",
            "[ckpt_lora_dar.pt] iter 2740: loss 1.0252, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 2750: train 0.8622, val 0.9377\n",
            "[ckpt_lora_dar.pt] iter 2750: loss 1.0254, time 12753.5ms\n",
            "[ckpt_lora_dar.pt] iter 2760: loss 1.0171, time 149.1ms\n",
            "[ckpt_lora_dar.pt] iter 2770: loss 1.0166, time 151.4ms\n",
            "[ckpt_lora_dar.pt] iter 2780: loss 0.9971, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 2790: loss 1.0208, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 2800: loss 1.0129, time 148.8ms\n",
            "[ckpt_lora_dar.pt] iter 2810: loss 1.0537, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 2820: loss 1.0316, time 148.1ms\n",
            "[ckpt_lora_dar.pt] iter 2830: loss 1.0117, time 149.0ms\n",
            "[ckpt_lora_dar.pt] iter 2840: loss 1.0145, time 152.4ms\n",
            "[ckpt_lora_dar.pt] iter 2850: loss 1.0156, time 149.5ms\n",
            "[ckpt_lora_dar.pt] iter 2860: loss 1.0068, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 2870: loss 1.0249, time 150.5ms\n",
            "[ckpt_lora_dar.pt] iter 2880: loss 0.9920, time 151.4ms\n",
            "[ckpt_lora_dar.pt] iter 2890: loss 1.0323, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 2900: loss 1.0110, time 151.3ms\n",
            "[ckpt_lora_dar.pt] iter 2910: loss 0.9991, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 2920: loss 1.0066, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 2930: loss 1.0085, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 2940: loss 1.0264, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 2950: loss 1.0369, time 152.2ms\n",
            "[ckpt_lora_dar.pt] iter 2960: loss 1.0083, time 149.0ms\n",
            "[ckpt_lora_dar.pt] iter 2970: loss 0.9945, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 2980: loss 1.0204, time 150.1ms\n",
            "[ckpt_lora_dar.pt] iter 2990: loss 1.0443, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 3000: train 0.8657, val 0.9346\n",
            "[ckpt_lora_dar.pt] iter 3000: loss 1.0284, time 12709.0ms\n",
            "[ckpt_lora_dar.pt] iter 3010: loss 1.0339, time 149.8ms\n",
            "[ckpt_lora_dar.pt] iter 3020: loss 1.0167, time 152.5ms\n",
            "[ckpt_lora_dar.pt] iter 3030: loss 0.9857, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 3040: loss 1.0406, time 150.1ms\n",
            "[ckpt_lora_dar.pt] iter 3050: loss 1.0310, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 3060: loss 0.9907, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 3070: loss 1.0142, time 149.5ms\n",
            "[ckpt_lora_dar.pt] iter 3080: loss 0.9997, time 149.8ms\n",
            "[ckpt_lora_dar.pt] iter 3090: loss 0.9695, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 3100: loss 1.0103, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 3110: loss 1.0199, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 3120: loss 1.0137, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 3130: loss 0.9966, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 3140: loss 1.0093, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 3150: loss 1.0179, time 150.2ms\n",
            "[ckpt_lora_dar.pt] iter 3160: loss 0.9901, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 3170: loss 1.0103, time 151.5ms\n",
            "[ckpt_lora_dar.pt] iter 3180: loss 0.9865, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 3190: loss 0.9818, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 3200: loss 1.0060, time 150.2ms\n",
            "[ckpt_lora_dar.pt] iter 3210: loss 0.9865, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 3220: loss 0.9850, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 3230: loss 0.9816, time 150.5ms\n",
            "[ckpt_lora_dar.pt] iter 3240: loss 0.9558, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 3250: train 0.8657, val 0.9352\n",
            "[ckpt_lora_dar.pt] iter 3250: loss 0.9851, time 12729.4ms\n",
            "[ckpt_lora_dar.pt] iter 3260: loss 1.0177, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 3270: loss 0.9596, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 3280: loss 0.9726, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 3290: loss 0.9797, time 150.2ms\n",
            "[ckpt_lora_dar.pt] iter 3300: loss 0.9741, time 149.5ms\n",
            "[ckpt_lora_dar.pt] iter 3310: loss 1.0070, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 3320: loss 0.9757, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 3330: loss 0.9748, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 3340: loss 0.9809, time 151.6ms\n",
            "[ckpt_lora_dar.pt] iter 3350: loss 0.9644, time 148.0ms\n",
            "[ckpt_lora_dar.pt] iter 3360: loss 0.9684, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 3370: loss 0.9807, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 3380: loss 1.0022, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 3390: loss 1.0031, time 149.1ms\n",
            "[ckpt_lora_dar.pt] iter 3400: loss 0.9934, time 149.1ms\n",
            "[ckpt_lora_dar.pt] iter 3410: loss 0.9942, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 3420: loss 1.0041, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 3430: loss 0.9925, time 150.1ms\n",
            "[ckpt_lora_dar.pt] iter 3440: loss 0.9755, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 3450: loss 0.9413, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 3460: loss 1.0026, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 3470: loss 0.9648, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 3480: loss 0.9830, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 3490: loss 0.9745, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 3500: train 0.8657, val 0.9360\n",
            "[ckpt_lora_dar.pt] iter 3500: loss 0.9626, time 12693.1ms\n",
            "[ckpt_lora_dar.pt] iter 3510: loss 0.9928, time 147.8ms\n",
            "[ckpt_lora_dar.pt] iter 3520: loss 0.9848, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 3530: loss 0.9776, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 3540: loss 0.9753, time 149.1ms\n",
            "[ckpt_lora_dar.pt] iter 3550: loss 0.9651, time 152.2ms\n",
            "[ckpt_lora_dar.pt] iter 3560: loss 0.9667, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 3570: loss 0.9908, time 149.0ms\n",
            "[ckpt_lora_dar.pt] iter 3580: loss 0.9680, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 3590: loss 0.9886, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 3600: loss 0.9949, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 3610: loss 0.9496, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 3620: loss 0.9508, time 150.2ms\n",
            "[ckpt_lora_dar.pt] iter 3630: loss 0.9679, time 149.6ms\n",
            "[ckpt_lora_dar.pt] iter 3640: loss 0.9465, time 149.2ms\n",
            "[ckpt_lora_dar.pt] iter 3650: loss 0.9986, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 3660: loss 0.9755, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 3670: loss 0.9390, time 149.5ms\n",
            "[ckpt_lora_dar.pt] iter 3680: loss 0.9696, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 3690: loss 1.0027, time 151.4ms\n",
            "[ckpt_lora_dar.pt] iter 3700: loss 0.9523, time 151.6ms\n",
            "[ckpt_lora_dar.pt] iter 3710: loss 0.9646, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 3720: loss 0.9747, time 152.3ms\n",
            "[ckpt_lora_dar.pt] iter 3730: loss 0.9546, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 3740: loss 0.9751, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 3750: train 0.8651, val 0.9358\n",
            "[ckpt_lora_dar.pt] iter 3750: loss 0.9709, time 13063.3ms\n",
            "[ckpt_lora_dar.pt] iter 3760: loss 0.9580, time 153.8ms\n",
            "[ckpt_lora_dar.pt] iter 3770: loss 0.9723, time 155.0ms\n",
            "[ckpt_lora_dar.pt] iter 3780: loss 0.9608, time 153.3ms\n",
            "[ckpt_lora_dar.pt] iter 3790: loss 0.9852, time 154.0ms\n",
            "[ckpt_lora_dar.pt] iter 3800: loss 0.9485, time 155.9ms\n",
            "[ckpt_lora_dar.pt] iter 3810: loss 0.9487, time 153.8ms\n",
            "[ckpt_lora_dar.pt] iter 3820: loss 0.9613, time 152.4ms\n",
            "[ckpt_lora_dar.pt] iter 3830: loss 0.9706, time 153.1ms\n",
            "[ckpt_lora_dar.pt] iter 3840: loss 0.9482, time 152.9ms\n",
            "[ckpt_lora_dar.pt] iter 3850: loss 0.9489, time 152.8ms\n",
            "[ckpt_lora_dar.pt] iter 3860: loss 0.9724, time 153.6ms\n",
            "[ckpt_lora_dar.pt] iter 3870: loss 0.9450, time 152.0ms\n",
            "[ckpt_lora_dar.pt] iter 3880: loss 0.9491, time 155.0ms\n",
            "[ckpt_lora_dar.pt] iter 3890: loss 0.9428, time 152.3ms\n",
            "[ckpt_lora_dar.pt] iter 3900: loss 0.9510, time 153.1ms\n",
            "[ckpt_lora_dar.pt] iter 3910: loss 0.9375, time 151.9ms\n",
            "[ckpt_lora_dar.pt] iter 3920: loss 0.9491, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 3930: loss 0.9206, time 153.6ms\n",
            "[ckpt_lora_dar.pt] iter 3940: loss 0.9504, time 151.8ms\n",
            "[ckpt_lora_dar.pt] iter 3950: loss 0.9607, time 152.6ms\n",
            "[ckpt_lora_dar.pt] iter 3960: loss 0.9500, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 3970: loss 0.9371, time 152.9ms\n",
            "[ckpt_lora_dar.pt] iter 3980: loss 0.9655, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 3990: loss 0.9520, time 151.5ms\n",
            "[ckpt_lora_dar.pt] iter 4000: train 0.8649, val 0.9362\n",
            "[ckpt_lora_dar.pt] iter 4000: loss 0.9373, time 12884.6ms\n",
            "[ckpt_lora_dar.pt] iter 4010: loss 0.9949, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 4020: loss 0.9379, time 151.6ms\n",
            "[ckpt_lora_dar.pt] iter 4030: loss 0.9947, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 4040: loss 0.9658, time 153.5ms\n",
            "[ckpt_lora_dar.pt] iter 4050: loss 0.9399, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 4060: loss 0.9422, time 152.3ms\n",
            "[ckpt_lora_dar.pt] iter 4070: loss 0.9935, time 152.3ms\n",
            "[ckpt_lora_dar.pt] iter 4080: loss 0.9468, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 4090: loss 0.9719, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 4100: loss 0.9601, time 154.1ms\n",
            "[ckpt_lora_dar.pt] iter 4110: loss 0.9531, time 151.5ms\n",
            "[ckpt_lora_dar.pt] iter 4120: loss 0.9594, time 151.4ms\n",
            "[ckpt_lora_dar.pt] iter 4130: loss 0.9387, time 152.2ms\n",
            "[ckpt_lora_dar.pt] iter 4140: loss 0.9661, time 154.9ms\n",
            "[ckpt_lora_dar.pt] iter 4150: loss 0.9804, time 151.8ms\n",
            "[ckpt_lora_dar.pt] iter 4160: loss 0.9563, time 154.4ms\n",
            "[ckpt_lora_dar.pt] iter 4170: loss 0.9634, time 152.8ms\n",
            "[ckpt_lora_dar.pt] iter 4180: loss 0.9499, time 152.8ms\n",
            "[ckpt_lora_dar.pt] iter 4190: loss 0.9489, time 153.1ms\n",
            "[ckpt_lora_dar.pt] iter 4200: loss 0.9526, time 152.6ms\n",
            "[ckpt_lora_dar.pt] iter 4210: loss 0.9550, time 152.6ms\n",
            "[ckpt_lora_dar.pt] iter 4220: loss 0.9812, time 152.5ms\n",
            "[ckpt_lora_dar.pt] iter 4230: loss 0.9959, time 152.7ms\n",
            "[ckpt_lora_dar.pt] iter 4240: loss 0.9522, time 152.8ms\n",
            "[ckpt_lora_dar.pt] iter 4250: train 0.8631, val 0.9351\n",
            "[ckpt_lora_dar.pt] iter 4250: loss 0.9216, time 13020.3ms\n",
            "[ckpt_lora_dar.pt] iter 4260: loss 0.9669, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 4270: loss 0.9450, time 151.8ms\n",
            "[ckpt_lora_dar.pt] iter 4280: loss 0.9431, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 4290: loss 0.9817, time 152.4ms\n",
            "[ckpt_lora_dar.pt] iter 4300: loss 0.9290, time 151.5ms\n",
            "[ckpt_lora_dar.pt] iter 4310: loss 0.9749, time 148.9ms\n",
            "[ckpt_lora_dar.pt] iter 4320: loss 0.9177, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 4330: loss 0.9514, time 153.5ms\n",
            "[ckpt_lora_dar.pt] iter 4340: loss 0.9323, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 4350: loss 0.9641, time 151.9ms\n",
            "[ckpt_lora_dar.pt] iter 4360: loss 0.9308, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 4370: loss 0.9418, time 152.1ms\n",
            "[ckpt_lora_dar.pt] iter 4380: loss 0.9357, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 4390: loss 0.9076, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 4400: loss 0.9186, time 152.9ms\n",
            "[ckpt_lora_dar.pt] iter 4410: loss 0.9645, time 149.5ms\n",
            "[ckpt_lora_dar.pt] iter 4420: loss 0.9205, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 4430: loss 0.9607, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 4440: loss 0.9446, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 4450: loss 0.9335, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 4460: loss 0.9319, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 4470: loss 0.9126, time 152.1ms\n",
            "[ckpt_lora_dar.pt] iter 4480: loss 0.9363, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 4490: loss 0.9592, time 148.7ms\n",
            "[ckpt_lora_dar.pt] iter 4500: train 0.8638, val 0.9391\n",
            "[ckpt_lora_dar.pt] iter 4500: loss 0.9516, time 12788.2ms\n",
            "[ckpt_lora_dar.pt] iter 4510: loss 0.9491, time 148.4ms\n",
            "[ckpt_lora_dar.pt] iter 4520: loss 0.9206, time 150.0ms\n",
            "[ckpt_lora_dar.pt] iter 4530: loss 0.9321, time 151.3ms\n",
            "[ckpt_lora_dar.pt] iter 4540: loss 0.9015, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 4550: loss 0.9564, time 152.5ms\n",
            "[ckpt_lora_dar.pt] iter 4560: loss 0.9733, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 4570: loss 0.9255, time 149.7ms\n",
            "[ckpt_lora_dar.pt] iter 4580: loss 0.9230, time 150.1ms\n",
            "[ckpt_lora_dar.pt] iter 4590: loss 0.9474, time 151.6ms\n",
            "[ckpt_lora_dar.pt] iter 4600: loss 0.9461, time 150.1ms\n",
            "[ckpt_lora_dar.pt] iter 4610: loss 0.9244, time 151.9ms\n",
            "[ckpt_lora_dar.pt] iter 4620: loss 0.9429, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 4630: loss 0.9322, time 150.5ms\n",
            "[ckpt_lora_dar.pt] iter 4640: loss 0.9361, time 152.3ms\n",
            "[ckpt_lora_dar.pt] iter 4650: loss 0.9231, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 4660: loss 0.9600, time 149.1ms\n",
            "[ckpt_lora_dar.pt] iter 4670: loss 0.9329, time 148.9ms\n",
            "[ckpt_lora_dar.pt] iter 4680: loss 0.9175, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 4690: loss 0.9538, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 4700: loss 0.9367, time 150.8ms\n",
            "[ckpt_lora_dar.pt] iter 4710: loss 0.9480, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 4720: loss 0.9737, time 151.3ms\n",
            "[ckpt_lora_dar.pt] iter 4730: loss 0.9401, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 4740: loss 0.9447, time 152.5ms\n",
            "[ckpt_lora_dar.pt] iter 4750: train 0.8658, val 0.9354\n",
            "[ckpt_lora_dar.pt] iter 4750: loss 0.9355, time 12864.1ms\n",
            "[ckpt_lora_dar.pt] iter 4760: loss 0.9321, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 4770: loss 0.9556, time 153.0ms\n",
            "[ckpt_lora_dar.pt] iter 4780: loss 0.9318, time 150.5ms\n",
            "[ckpt_lora_dar.pt] iter 4790: loss 0.9566, time 151.0ms\n",
            "[ckpt_lora_dar.pt] iter 4800: loss 0.9363, time 151.2ms\n",
            "[ckpt_lora_dar.pt] iter 4810: loss 0.9541, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 4820: loss 0.9381, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 4830: loss 0.9362, time 150.4ms\n",
            "[ckpt_lora_dar.pt] iter 4840: loss 0.9465, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 4850: loss 0.9184, time 149.9ms\n",
            "[ckpt_lora_dar.pt] iter 4860: loss 0.9365, time 148.2ms\n",
            "[ckpt_lora_dar.pt] iter 4870: loss 0.9312, time 152.0ms\n",
            "[ckpt_lora_dar.pt] iter 4880: loss 0.9634, time 151.1ms\n",
            "[ckpt_lora_dar.pt] iter 4890: loss 0.9416, time 150.1ms\n",
            "[ckpt_lora_dar.pt] iter 4900: loss 0.9370, time 150.7ms\n",
            "[ckpt_lora_dar.pt] iter 4910: loss 0.9425, time 150.2ms\n",
            "[ckpt_lora_dar.pt] iter 4920: loss 0.9334, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 4930: loss 0.9550, time 150.9ms\n",
            "[ckpt_lora_dar.pt] iter 4940: loss 0.9026, time 148.6ms\n",
            "[ckpt_lora_dar.pt] iter 4950: loss 0.9159, time 150.5ms\n",
            "[ckpt_lora_dar.pt] iter 4960: loss 0.9718, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 4970: loss 0.9554, time 150.6ms\n",
            "[ckpt_lora_dar.pt] iter 4980: loss 0.9661, time 149.3ms\n",
            "[ckpt_lora_dar.pt] iter 4990: loss 0.9281, time 150.3ms\n",
            "[ckpt_lora_dar.pt] iter 5000: train 0.8646, val 0.9358\n",
            "[ckpt_lora_dar.pt] final save at iter 5000 → /content/drive/MyDrive/ECS 189G/out/ckpt_lora_dar.pt\n",
            "[ckpt_lora_dar.pt] Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT + PAAF"
      ],
      "metadata": {
        "id": "W-TxMWo2Sw4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell D: NanoGPT + PAAF (no LoRA, no DAR)\n",
        "# Note: we assume Cell 3’s monkey-patch (paaf_forward) is already active in scope.\n",
        "\n",
        "train_and_save_variant(\n",
        "    out_dir       = out_dir,\n",
        "    ckpt_filename = 'ckpt_paaf.pt',\n",
        "    use_lora      = False,\n",
        "    use_paaf      = True,\n",
        "    use_dar       = False,\n",
        "    dar_weight    = 0.0,\n",
        "    max_iters     = max_iters\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcOKt2CQSzrD",
        "outputId": "f31d051e-3aac-4262-aa17-f1df619a91bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 10.66M\n",
            "num decayed parameter tensors: 26, with 10,753,536 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-895d20065530>:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_local = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ckpt_paaf.pt] iter 0: train 0.8636, val 0.9352\n",
            "[ckpt_paaf.pt] iter 0: loss 4.6314, time 12325.8ms\n",
            "[ckpt_paaf.pt] iter 10: loss 3.1884, time 99.8ms\n",
            "[ckpt_paaf.pt] iter 20: loss 2.7122, time 98.2ms\n",
            "[ckpt_paaf.pt] iter 30: loss 2.5010, time 98.1ms\n",
            "[ckpt_paaf.pt] iter 40: loss 2.4286, time 97.7ms\n",
            "[ckpt_paaf.pt] iter 50: loss 2.3981, time 100.6ms\n",
            "[ckpt_paaf.pt] iter 60: loss 2.3952, time 98.3ms\n",
            "[ckpt_paaf.pt] iter 70: loss 2.4069, time 100.5ms\n",
            "[ckpt_paaf.pt] iter 80: loss 2.3557, time 98.7ms\n",
            "[ckpt_paaf.pt] iter 90: loss 2.3608, time 100.2ms\n",
            "[ckpt_paaf.pt] iter 100: loss 2.3520, time 99.5ms\n",
            "[ckpt_paaf.pt] iter 110: loss 2.3225, time 98.1ms\n",
            "[ckpt_paaf.pt] iter 120: loss 2.3336, time 100.4ms\n",
            "[ckpt_paaf.pt] iter 130: loss 2.3058, time 100.9ms\n",
            "[ckpt_paaf.pt] iter 140: loss 2.2767, time 100.3ms\n",
            "[ckpt_paaf.pt] iter 150: loss 2.2573, time 100.6ms\n",
            "[ckpt_paaf.pt] iter 160: loss 2.1937, time 100.9ms\n",
            "[ckpt_paaf.pt] iter 170: loss 2.1657, time 100.8ms\n",
            "[ckpt_paaf.pt] iter 180: loss 2.0802, time 100.8ms\n",
            "[ckpt_paaf.pt] iter 190: loss 2.0602, time 100.1ms\n",
            "[ckpt_paaf.pt] iter 200: loss 1.9983, time 100.9ms\n",
            "[ckpt_paaf.pt] iter 210: loss 1.9697, time 99.2ms\n",
            "[ckpt_paaf.pt] iter 220: loss 1.9685, time 100.5ms\n",
            "[ckpt_paaf.pt] iter 230: loss 1.8876, time 100.5ms\n",
            "[ckpt_paaf.pt] iter 240: loss 1.8755, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 250: train 0.8654, val 0.9347\n",
            "[ckpt_paaf.pt] saving checkpoint at iter 250 → /content/drive/MyDrive/ECS 189G/out/ckpt_paaf.pt\n",
            "[ckpt_paaf.pt] iter 250: loss 1.8415, time 13033.8ms\n",
            "[ckpt_paaf.pt] iter 260: loss 1.8318, time 101.9ms\n",
            "[ckpt_paaf.pt] iter 270: loss 1.8195, time 105.5ms\n",
            "[ckpt_paaf.pt] iter 280: loss 1.7790, time 102.3ms\n",
            "[ckpt_paaf.pt] iter 290: loss 1.7717, time 98.5ms\n",
            "[ckpt_paaf.pt] iter 300: loss 1.7275, time 102.1ms\n",
            "[ckpt_paaf.pt] iter 310: loss 1.6924, time 102.3ms\n",
            "[ckpt_paaf.pt] iter 320: loss 1.7006, time 102.6ms\n",
            "[ckpt_paaf.pt] iter 330: loss 1.6912, time 101.8ms\n",
            "[ckpt_paaf.pt] iter 340: loss 1.6876, time 100.9ms\n",
            "[ckpt_paaf.pt] iter 350: loss 1.6517, time 101.7ms\n",
            "[ckpt_paaf.pt] iter 360: loss 1.6533, time 100.3ms\n",
            "[ckpt_paaf.pt] iter 370: loss 1.6405, time 101.2ms\n",
            "[ckpt_paaf.pt] iter 380: loss 1.6234, time 101.7ms\n",
            "[ckpt_paaf.pt] iter 390: loss 1.6080, time 100.4ms\n",
            "[ckpt_paaf.pt] iter 400: loss 1.6071, time 102.1ms\n",
            "[ckpt_paaf.pt] iter 410: loss 1.5701, time 102.2ms\n",
            "[ckpt_paaf.pt] iter 420: loss 1.5976, time 101.1ms\n",
            "[ckpt_paaf.pt] iter 430: loss 1.5423, time 100.8ms\n",
            "[ckpt_paaf.pt] iter 440: loss 1.5406, time 102.3ms\n",
            "[ckpt_paaf.pt] iter 450: loss 1.5133, time 101.6ms\n",
            "[ckpt_paaf.pt] iter 460: loss 1.5449, time 103.0ms\n",
            "[ckpt_paaf.pt] iter 470: loss 1.5546, time 102.4ms\n",
            "[ckpt_paaf.pt] iter 480: loss 1.4961, time 101.5ms\n",
            "[ckpt_paaf.pt] iter 490: loss 1.5396, time 101.9ms\n",
            "[ckpt_paaf.pt] iter 500: train 0.8651, val 0.9335\n",
            "[ckpt_paaf.pt] saving checkpoint at iter 500 → /content/drive/MyDrive/ECS 189G/out/ckpt_paaf.pt\n",
            "[ckpt_paaf.pt] iter 500: loss 1.4979, time 13053.6ms\n",
            "[ckpt_paaf.pt] iter 510: loss 1.4687, time 101.5ms\n",
            "[ckpt_paaf.pt] iter 520: loss 1.4747, time 103.1ms\n",
            "[ckpt_paaf.pt] iter 530: loss 1.4639, time 101.9ms\n",
            "[ckpt_paaf.pt] iter 540: loss 1.4724, time 100.8ms\n",
            "[ckpt_paaf.pt] iter 550: loss 1.4610, time 101.5ms\n",
            "[ckpt_paaf.pt] iter 560: loss 1.4729, time 100.6ms\n",
            "[ckpt_paaf.pt] iter 570: loss 1.4327, time 102.2ms\n",
            "[ckpt_paaf.pt] iter 580: loss 1.4703, time 102.7ms\n",
            "[ckpt_paaf.pt] iter 590: loss 1.4112, time 101.0ms\n",
            "[ckpt_paaf.pt] iter 600: loss 1.4167, time 101.5ms\n",
            "[ckpt_paaf.pt] iter 610: loss 1.3680, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 620: loss 1.4046, time 101.3ms\n",
            "[ckpt_paaf.pt] iter 630: loss 1.3939, time 100.5ms\n",
            "[ckpt_paaf.pt] iter 640: loss 1.3840, time 100.1ms\n",
            "[ckpt_paaf.pt] iter 650: loss 1.3982, time 96.2ms\n",
            "[ckpt_paaf.pt] iter 660: loss 1.3809, time 101.6ms\n",
            "[ckpt_paaf.pt] iter 670: loss 1.3787, time 101.3ms\n",
            "[ckpt_paaf.pt] iter 680: loss 1.3695, time 101.9ms\n",
            "[ckpt_paaf.pt] iter 690: loss 1.4144, time 100.8ms\n",
            "[ckpt_paaf.pt] iter 700: loss 1.3510, time 101.8ms\n",
            "[ckpt_paaf.pt] iter 710: loss 1.3666, time 101.7ms\n",
            "[ckpt_paaf.pt] iter 720: loss 1.3471, time 101.6ms\n",
            "[ckpt_paaf.pt] iter 730: loss 1.3762, time 102.4ms\n",
            "[ckpt_paaf.pt] iter 740: loss 1.3496, time 101.7ms\n",
            "[ckpt_paaf.pt] iter 750: train 0.8661, val 0.9351\n",
            "[ckpt_paaf.pt] iter 750: loss 1.3008, time 12755.0ms\n",
            "[ckpt_paaf.pt] iter 760: loss 1.3405, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 770: loss 1.3225, time 102.5ms\n",
            "[ckpt_paaf.pt] iter 780: loss 1.2994, time 102.5ms\n",
            "[ckpt_paaf.pt] iter 790: loss 1.3181, time 102.3ms\n",
            "[ckpt_paaf.pt] iter 800: loss 1.2881, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 810: loss 1.3043, time 100.9ms\n",
            "[ckpt_paaf.pt] iter 820: loss 1.3101, time 99.0ms\n",
            "[ckpt_paaf.pt] iter 830: loss 1.2993, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 840: loss 1.2931, time 101.1ms\n",
            "[ckpt_paaf.pt] iter 850: loss 1.2841, time 101.6ms\n",
            "[ckpt_paaf.pt] iter 860: loss 1.2857, time 101.4ms\n",
            "[ckpt_paaf.pt] iter 870: loss 1.2851, time 101.9ms\n",
            "[ckpt_paaf.pt] iter 880: loss 1.3031, time 101.5ms\n",
            "[ckpt_paaf.pt] iter 890: loss 1.2721, time 102.2ms\n",
            "[ckpt_paaf.pt] iter 900: loss 1.2656, time 101.6ms\n",
            "[ckpt_paaf.pt] iter 910: loss 1.2658, time 102.4ms\n",
            "[ckpt_paaf.pt] iter 920: loss 1.2974, time 101.2ms\n",
            "[ckpt_paaf.pt] iter 930: loss 1.2734, time 101.5ms\n",
            "[ckpt_paaf.pt] iter 940: loss 1.2830, time 102.1ms\n",
            "[ckpt_paaf.pt] iter 950: loss 1.2736, time 100.4ms\n",
            "[ckpt_paaf.pt] iter 960: loss 1.2348, time 101.7ms\n",
            "[ckpt_paaf.pt] iter 970: loss 1.2439, time 102.2ms\n",
            "[ckpt_paaf.pt] iter 980: loss 1.2003, time 102.2ms\n",
            "[ckpt_paaf.pt] iter 990: loss 1.2537, time 102.5ms\n",
            "[ckpt_paaf.pt] iter 1000: train 0.8648, val 0.9368\n",
            "[ckpt_paaf.pt] iter 1000: loss 1.2243, time 12857.1ms\n",
            "[ckpt_paaf.pt] iter 1010: loss 1.2159, time 103.0ms\n",
            "[ckpt_paaf.pt] iter 1020: loss 1.2266, time 102.1ms\n",
            "[ckpt_paaf.pt] iter 1030: loss 1.2471, time 100.9ms\n",
            "[ckpt_paaf.pt] iter 1040: loss 1.2376, time 101.0ms\n",
            "[ckpt_paaf.pt] iter 1050: loss 1.2450, time 101.7ms\n",
            "[ckpt_paaf.pt] iter 1060: loss 1.2296, time 102.1ms\n",
            "[ckpt_paaf.pt] iter 1070: loss 1.2439, time 101.8ms\n",
            "[ckpt_paaf.pt] iter 1080: loss 1.2444, time 102.4ms\n",
            "[ckpt_paaf.pt] iter 1090: loss 1.2310, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 1100: loss 1.2422, time 100.7ms\n",
            "[ckpt_paaf.pt] iter 1110: loss 1.2211, time 101.5ms\n",
            "[ckpt_paaf.pt] iter 1120: loss 1.2469, time 101.8ms\n",
            "[ckpt_paaf.pt] iter 1130: loss 1.1854, time 102.6ms\n",
            "[ckpt_paaf.pt] iter 1140: loss 1.1841, time 101.4ms\n",
            "[ckpt_paaf.pt] iter 1150: loss 1.1840, time 102.3ms\n",
            "[ckpt_paaf.pt] iter 1160: loss 1.2551, time 102.5ms\n",
            "[ckpt_paaf.pt] iter 1170: loss 1.2026, time 101.4ms\n",
            "[ckpt_paaf.pt] iter 1180: loss 1.1777, time 103.2ms\n",
            "[ckpt_paaf.pt] iter 1190: loss 1.1629, time 101.9ms\n",
            "[ckpt_paaf.pt] iter 1200: loss 1.1999, time 101.7ms\n",
            "[ckpt_paaf.pt] iter 1210: loss 1.2405, time 103.3ms\n",
            "[ckpt_paaf.pt] iter 1220: loss 1.1951, time 104.7ms\n",
            "[ckpt_paaf.pt] iter 1230: loss 1.2071, time 102.4ms\n",
            "[ckpt_paaf.pt] iter 1240: loss 1.2181, time 99.8ms\n",
            "[ckpt_paaf.pt] iter 1250: train 0.8647, val 0.9338\n",
            "[ckpt_paaf.pt] iter 1250: loss 1.1748, time 12758.7ms\n",
            "[ckpt_paaf.pt] iter 1260: loss 1.1850, time 103.6ms\n",
            "[ckpt_paaf.pt] iter 1270: loss 1.1787, time 102.7ms\n",
            "[ckpt_paaf.pt] iter 1280: loss 1.1584, time 102.2ms\n",
            "[ckpt_paaf.pt] iter 1290: loss 1.1978, time 106.0ms\n",
            "[ckpt_paaf.pt] iter 1300: loss 1.2411, time 101.1ms\n",
            "[ckpt_paaf.pt] iter 1310: loss 1.1707, time 103.1ms\n",
            "[ckpt_paaf.pt] iter 1320: loss 1.1998, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 1330: loss 1.1585, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 1340: loss 1.1851, time 103.0ms\n",
            "[ckpt_paaf.pt] iter 1350: loss 1.1655, time 102.3ms\n",
            "[ckpt_paaf.pt] iter 1360: loss 1.1346, time 102.7ms\n",
            "[ckpt_paaf.pt] iter 1370: loss 1.1545, time 102.6ms\n",
            "[ckpt_paaf.pt] iter 1380: loss 1.1543, time 101.9ms\n",
            "[ckpt_paaf.pt] iter 1390: loss 1.1738, time 102.9ms\n",
            "[ckpt_paaf.pt] iter 1400: loss 1.1488, time 103.3ms\n",
            "[ckpt_paaf.pt] iter 1410: loss 1.1707, time 102.5ms\n",
            "[ckpt_paaf.pt] iter 1420: loss 1.1384, time 103.3ms\n",
            "[ckpt_paaf.pt] iter 1430: loss 1.1202, time 100.7ms\n",
            "[ckpt_paaf.pt] iter 1440: loss 1.1267, time 104.1ms\n",
            "[ckpt_paaf.pt] iter 1450: loss 1.1950, time 102.5ms\n",
            "[ckpt_paaf.pt] iter 1460: loss 1.1753, time 103.7ms\n",
            "[ckpt_paaf.pt] iter 1470: loss 1.1287, time 102.3ms\n",
            "[ckpt_paaf.pt] iter 1480: loss 1.1620, time 102.4ms\n",
            "[ckpt_paaf.pt] iter 1490: loss 1.1660, time 101.6ms\n",
            "[ckpt_paaf.pt] iter 1500: train 0.8644, val 0.9333\n",
            "[ckpt_paaf.pt] saving checkpoint at iter 1500 → /content/drive/MyDrive/ECS 189G/out/ckpt_paaf.pt\n",
            "[ckpt_paaf.pt] iter 1500: loss 1.1657, time 13415.8ms\n",
            "[ckpt_paaf.pt] iter 1510: loss 1.1444, time 103.8ms\n",
            "[ckpt_paaf.pt] iter 1520: loss 1.1170, time 107.2ms\n",
            "[ckpt_paaf.pt] iter 1530: loss 1.1247, time 103.1ms\n",
            "[ckpt_paaf.pt] iter 1540: loss 1.1707, time 102.8ms\n",
            "[ckpt_paaf.pt] iter 1550: loss 1.1457, time 102.9ms\n",
            "[ckpt_paaf.pt] iter 1560: loss 1.1355, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 1570: loss 1.1304, time 72.4ms\n",
            "[ckpt_paaf.pt] iter 1580: loss 1.1428, time 101.8ms\n",
            "[ckpt_paaf.pt] iter 1590: loss 1.1535, time 102.0ms\n",
            "[ckpt_paaf.pt] iter 1600: loss 1.1271, time 101.9ms\n",
            "[ckpt_paaf.pt] iter 1610: loss 1.1369, time 103.1ms\n",
            "[ckpt_paaf.pt] iter 1620: loss 1.1207, time 103.3ms\n",
            "[ckpt_paaf.pt] iter 1630: loss 1.1471, time 103.4ms\n",
            "[ckpt_paaf.pt] iter 1640: loss 1.1280, time 103.5ms\n",
            "[ckpt_paaf.pt] iter 1650: loss 1.1573, time 102.7ms\n",
            "[ckpt_paaf.pt] iter 1660: loss 1.1172, time 102.5ms\n",
            "[ckpt_paaf.pt] iter 1670: loss 1.0723, time 103.5ms\n",
            "[ckpt_paaf.pt] iter 1680: loss 1.1010, time 104.6ms\n",
            "[ckpt_paaf.pt] iter 1690: loss 1.1223, time 102.6ms\n",
            "[ckpt_paaf.pt] iter 1700: loss 1.1348, time 103.4ms\n",
            "[ckpt_paaf.pt] iter 1710: loss 1.1187, time 103.3ms\n",
            "[ckpt_paaf.pt] iter 1720: loss 1.1302, time 104.3ms\n",
            "[ckpt_paaf.pt] iter 1730: loss 1.1038, time 104.2ms\n",
            "[ckpt_paaf.pt] iter 1740: loss 1.1387, time 103.3ms\n",
            "[ckpt_paaf.pt] iter 1750: train 0.8628, val 0.9355\n",
            "[ckpt_paaf.pt] iter 1750: loss 1.1151, time 12932.7ms\n",
            "[ckpt_paaf.pt] iter 1760: loss 1.0596, time 108.7ms\n",
            "[ckpt_paaf.pt] iter 1770: loss 1.1182, time 107.6ms\n",
            "[ckpt_paaf.pt] iter 1780: loss 1.1036, time 105.2ms\n",
            "[ckpt_paaf.pt] iter 1790: loss 1.1010, time 107.8ms\n",
            "[ckpt_paaf.pt] iter 1800: loss 1.1123, time 110.7ms\n",
            "[ckpt_paaf.pt] iter 1810: loss 1.0879, time 107.9ms\n",
            "[ckpt_paaf.pt] iter 1820: loss 1.0889, time 112.9ms\n",
            "[ckpt_paaf.pt] iter 1830: loss 1.1180, time 107.7ms\n",
            "[ckpt_paaf.pt] iter 1840: loss 1.1117, time 108.8ms\n",
            "[ckpt_paaf.pt] iter 1850: loss 1.1086, time 108.2ms\n",
            "[ckpt_paaf.pt] iter 1860: loss 1.1155, time 107.5ms\n",
            "[ckpt_paaf.pt] iter 1870: loss 1.1050, time 112.5ms\n",
            "[ckpt_paaf.pt] iter 1880: loss 1.1050, time 114.3ms\n",
            "[ckpt_paaf.pt] iter 1890: loss 1.0917, time 109.7ms\n",
            "[ckpt_paaf.pt] iter 1900: loss 1.0914, time 113.7ms\n",
            "[ckpt_paaf.pt] iter 1910: loss 1.1294, time 109.4ms\n",
            "[ckpt_paaf.pt] iter 1920: loss 1.0878, time 110.6ms\n",
            "[ckpt_paaf.pt] iter 1930: loss 1.0786, time 111.1ms\n",
            "[ckpt_paaf.pt] iter 1940: loss 1.0838, time 114.4ms\n",
            "[ckpt_paaf.pt] iter 1950: loss 1.1232, time 110.3ms\n",
            "[ckpt_paaf.pt] iter 1960: loss 1.0795, time 110.7ms\n",
            "[ckpt_paaf.pt] iter 1970: loss 1.0780, time 111.3ms\n",
            "[ckpt_paaf.pt] iter 1980: loss 1.1091, time 108.8ms\n",
            "[ckpt_paaf.pt] iter 1990: loss 1.0618, time 109.8ms\n",
            "[ckpt_paaf.pt] iter 2000: train 0.8635, val 0.9346\n",
            "[ckpt_paaf.pt] iter 2000: loss 1.1029, time 13054.7ms\n",
            "[ckpt_paaf.pt] iter 2010: loss 1.0764, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 2020: loss 1.0617, time 110.8ms\n",
            "[ckpt_paaf.pt] iter 2030: loss 1.0983, time 106.9ms\n",
            "[ckpt_paaf.pt] iter 2040: loss 1.0981, time 106.2ms\n",
            "[ckpt_paaf.pt] iter 2050: loss 1.0889, time 107.7ms\n",
            "[ckpt_paaf.pt] iter 2060: loss 1.1174, time 107.5ms\n",
            "[ckpt_paaf.pt] iter 2070: loss 1.0870, time 109.5ms\n",
            "[ckpt_paaf.pt] iter 2080: loss 1.0794, time 107.3ms\n",
            "[ckpt_paaf.pt] iter 2090: loss 1.0694, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 2100: loss 1.0764, time 107.9ms\n",
            "[ckpt_paaf.pt] iter 2110: loss 1.0696, time 108.8ms\n",
            "[ckpt_paaf.pt] iter 2120: loss 1.0806, time 110.2ms\n",
            "[ckpt_paaf.pt] iter 2130: loss 1.0313, time 106.8ms\n",
            "[ckpt_paaf.pt] iter 2140: loss 1.0615, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 2150: loss 1.0679, time 110.4ms\n",
            "[ckpt_paaf.pt] iter 2160: loss 1.0710, time 109.3ms\n",
            "[ckpt_paaf.pt] iter 2170: loss 1.0679, time 108.8ms\n",
            "[ckpt_paaf.pt] iter 2180: loss 1.0892, time 107.2ms\n",
            "[ckpt_paaf.pt] iter 2190: loss 1.0499, time 107.6ms\n",
            "[ckpt_paaf.pt] iter 2200: loss 1.0499, time 108.6ms\n",
            "[ckpt_paaf.pt] iter 2210: loss 1.0688, time 108.2ms\n",
            "[ckpt_paaf.pt] iter 2220: loss 1.0716, time 107.4ms\n",
            "[ckpt_paaf.pt] iter 2230: loss 1.0403, time 107.2ms\n",
            "[ckpt_paaf.pt] iter 2240: loss 1.0456, time 105.6ms\n",
            "[ckpt_paaf.pt] iter 2250: train 0.8644, val 0.9362\n",
            "[ckpt_paaf.pt] iter 2250: loss 1.0231, time 12947.3ms\n",
            "[ckpt_paaf.pt] iter 2260: loss 1.0939, time 106.8ms\n",
            "[ckpt_paaf.pt] iter 2270: loss 1.0625, time 106.0ms\n",
            "[ckpt_paaf.pt] iter 2280: loss 1.0807, time 108.5ms\n",
            "[ckpt_paaf.pt] iter 2290: loss 1.0834, time 107.7ms\n",
            "[ckpt_paaf.pt] iter 2300: loss 1.0408, time 104.2ms\n",
            "[ckpt_paaf.pt] iter 2310: loss 1.0616, time 108.9ms\n",
            "[ckpt_paaf.pt] iter 2320: loss 1.0432, time 109.0ms\n",
            "[ckpt_paaf.pt] iter 2330: loss 1.0433, time 105.0ms\n",
            "[ckpt_paaf.pt] iter 2340: loss 1.0302, time 109.2ms\n",
            "[ckpt_paaf.pt] iter 2350: loss 1.0536, time 109.5ms\n",
            "[ckpt_paaf.pt] iter 2360: loss 1.0329, time 104.7ms\n",
            "[ckpt_paaf.pt] iter 2370: loss 1.0593, time 108.2ms\n",
            "[ckpt_paaf.pt] iter 2380: loss 1.0030, time 105.5ms\n",
            "[ckpt_paaf.pt] iter 2390: loss 1.0509, time 107.8ms\n",
            "[ckpt_paaf.pt] iter 2400: loss 1.0276, time 106.5ms\n",
            "[ckpt_paaf.pt] iter 2410: loss 1.0503, time 106.2ms\n",
            "[ckpt_paaf.pt] iter 2420: loss 1.0423, time 108.6ms\n",
            "[ckpt_paaf.pt] iter 2430: loss 1.0656, time 111.3ms\n",
            "[ckpt_paaf.pt] iter 2440: loss 1.0410, time 106.3ms\n",
            "[ckpt_paaf.pt] iter 2450: loss 1.0698, time 108.4ms\n",
            "[ckpt_paaf.pt] iter 2460: loss 1.0648, time 106.5ms\n",
            "[ckpt_paaf.pt] iter 2470: loss 1.0349, time 109.6ms\n",
            "[ckpt_paaf.pt] iter 2480: loss 1.0509, time 110.6ms\n",
            "[ckpt_paaf.pt] iter 2490: loss 1.0553, time 106.1ms\n",
            "[ckpt_paaf.pt] iter 2500: train 0.8641, val 0.9349\n",
            "[ckpt_paaf.pt] iter 2500: loss 1.0187, time 13022.9ms\n",
            "[ckpt_paaf.pt] iter 2510: loss 1.1058, time 108.9ms\n",
            "[ckpt_paaf.pt] iter 2520: loss 1.0240, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 2530: loss 1.0544, time 107.5ms\n",
            "[ckpt_paaf.pt] iter 2540: loss 1.0039, time 108.5ms\n",
            "[ckpt_paaf.pt] iter 2550: loss 1.0254, time 107.5ms\n",
            "[ckpt_paaf.pt] iter 2560: loss 1.0441, time 107.5ms\n",
            "[ckpt_paaf.pt] iter 2570: loss 1.0357, time 106.8ms\n",
            "[ckpt_paaf.pt] iter 2580: loss 1.0167, time 107.7ms\n",
            "[ckpt_paaf.pt] iter 2590: loss 1.0426, time 109.1ms\n",
            "[ckpt_paaf.pt] iter 2600: loss 1.0415, time 108.9ms\n",
            "[ckpt_paaf.pt] iter 2610: loss 1.0357, time 108.2ms\n",
            "[ckpt_paaf.pt] iter 2620: loss 1.0287, time 105.6ms\n",
            "[ckpt_paaf.pt] iter 2630: loss 1.0440, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 2640: loss 1.0414, time 108.9ms\n",
            "[ckpt_paaf.pt] iter 2650: loss 1.0266, time 111.1ms\n",
            "[ckpt_paaf.pt] iter 2660: loss 1.0346, time 108.0ms\n",
            "[ckpt_paaf.pt] iter 2670: loss 1.0241, time 108.9ms\n",
            "[ckpt_paaf.pt] iter 2680: loss 1.0177, time 108.5ms\n",
            "[ckpt_paaf.pt] iter 2690: loss 1.0135, time 111.6ms\n",
            "[ckpt_paaf.pt] iter 2700: loss 1.0151, time 107.9ms\n",
            "[ckpt_paaf.pt] iter 2710: loss 0.9972, time 107.0ms\n",
            "[ckpt_paaf.pt] iter 2720: loss 1.0149, time 109.3ms\n",
            "[ckpt_paaf.pt] iter 2730: loss 1.0128, time 109.3ms\n",
            "[ckpt_paaf.pt] iter 2740: loss 1.0400, time 107.6ms\n",
            "[ckpt_paaf.pt] iter 2750: train 0.8641, val 0.9346\n",
            "[ckpt_paaf.pt] iter 2750: loss 1.0147, time 13017.8ms\n",
            "[ckpt_paaf.pt] iter 2760: loss 1.0113, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 2770: loss 1.0073, time 109.4ms\n",
            "[ckpt_paaf.pt] iter 2780: loss 1.0164, time 111.2ms\n",
            "[ckpt_paaf.pt] iter 2790: loss 1.0492, time 107.6ms\n",
            "[ckpt_paaf.pt] iter 2800: loss 1.0099, time 109.1ms\n",
            "[ckpt_paaf.pt] iter 2810: loss 1.0246, time 109.1ms\n",
            "[ckpt_paaf.pt] iter 2820: loss 1.0157, time 108.8ms\n",
            "[ckpt_paaf.pt] iter 2830: loss 1.0329, time 107.9ms\n",
            "[ckpt_paaf.pt] iter 2840: loss 1.0448, time 107.5ms\n",
            "[ckpt_paaf.pt] iter 2850: loss 0.9897, time 108.9ms\n",
            "[ckpt_paaf.pt] iter 2860: loss 1.0127, time 106.6ms\n",
            "[ckpt_paaf.pt] iter 2870: loss 1.0472, time 107.4ms\n",
            "[ckpt_paaf.pt] iter 2880: loss 1.0386, time 106.0ms\n",
            "[ckpt_paaf.pt] iter 2890: loss 0.9944, time 108.7ms\n",
            "[ckpt_paaf.pt] iter 2900: loss 0.9971, time 110.3ms\n",
            "[ckpt_paaf.pt] iter 2910: loss 1.0305, time 107.4ms\n",
            "[ckpt_paaf.pt] iter 2920: loss 1.0104, time 105.7ms\n",
            "[ckpt_paaf.pt] iter 2930: loss 1.0488, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 2940: loss 0.9979, time 106.2ms\n",
            "[ckpt_paaf.pt] iter 2950: loss 1.0064, time 109.4ms\n",
            "[ckpt_paaf.pt] iter 2960: loss 0.9894, time 110.2ms\n",
            "[ckpt_paaf.pt] iter 2970: loss 0.9925, time 108.6ms\n",
            "[ckpt_paaf.pt] iter 2980: loss 1.0263, time 109.4ms\n",
            "[ckpt_paaf.pt] iter 2990: loss 1.0174, time 111.2ms\n",
            "[ckpt_paaf.pt] iter 3000: train 0.8645, val 0.9380\n",
            "[ckpt_paaf.pt] iter 3000: loss 1.0241, time 12978.0ms\n",
            "[ckpt_paaf.pt] iter 3010: loss 1.0318, time 107.4ms\n",
            "[ckpt_paaf.pt] iter 3020: loss 0.9998, time 110.3ms\n",
            "[ckpt_paaf.pt] iter 3030: loss 1.0084, time 109.6ms\n",
            "[ckpt_paaf.pt] iter 3040: loss 0.9818, time 107.3ms\n",
            "[ckpt_paaf.pt] iter 3050: loss 1.0035, time 106.0ms\n",
            "[ckpt_paaf.pt] iter 3060: loss 0.9710, time 109.8ms\n",
            "[ckpt_paaf.pt] iter 3070: loss 0.9899, time 111.1ms\n",
            "[ckpt_paaf.pt] iter 3080: loss 0.9818, time 110.8ms\n",
            "[ckpt_paaf.pt] iter 3090: loss 0.9856, time 107.4ms\n",
            "[ckpt_paaf.pt] iter 3100: loss 1.0036, time 105.2ms\n",
            "[ckpt_paaf.pt] iter 3110: loss 1.0184, time 107.6ms\n",
            "[ckpt_paaf.pt] iter 3120: loss 0.9923, time 108.1ms\n",
            "[ckpt_paaf.pt] iter 3130: loss 1.0086, time 109.2ms\n",
            "[ckpt_paaf.pt] iter 3140: loss 0.9953, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 3150: loss 1.0262, time 112.8ms\n",
            "[ckpt_paaf.pt] iter 3160: loss 0.9782, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 3170: loss 0.9925, time 109.4ms\n",
            "[ckpt_paaf.pt] iter 3180: loss 0.9956, time 110.3ms\n",
            "[ckpt_paaf.pt] iter 3190: loss 0.9962, time 111.4ms\n",
            "[ckpt_paaf.pt] iter 3200: loss 1.0094, time 109.3ms\n",
            "[ckpt_paaf.pt] iter 3210: loss 1.0059, time 109.9ms\n",
            "[ckpt_paaf.pt] iter 3220: loss 1.0019, time 108.9ms\n",
            "[ckpt_paaf.pt] iter 3230: loss 0.9864, time 113.1ms\n",
            "[ckpt_paaf.pt] iter 3240: loss 1.0176, time 111.2ms\n",
            "[ckpt_paaf.pt] iter 3250: train 0.8660, val 0.9376\n",
            "[ckpt_paaf.pt] iter 3250: loss 1.0171, time 13006.0ms\n",
            "[ckpt_paaf.pt] iter 3260: loss 1.0270, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 3270: loss 0.9720, time 110.6ms\n",
            "[ckpt_paaf.pt] iter 3280: loss 0.9963, time 106.0ms\n",
            "[ckpt_paaf.pt] iter 3290: loss 0.9638, time 109.8ms\n",
            "[ckpt_paaf.pt] iter 3300: loss 1.0065, time 111.1ms\n",
            "[ckpt_paaf.pt] iter 3310: loss 0.9463, time 106.2ms\n",
            "[ckpt_paaf.pt] iter 3320: loss 0.9908, time 111.5ms\n",
            "[ckpt_paaf.pt] iter 3330: loss 0.9661, time 111.1ms\n",
            "[ckpt_paaf.pt] iter 3340: loss 0.9733, time 106.8ms\n",
            "[ckpt_paaf.pt] iter 3350: loss 0.9328, time 107.6ms\n",
            "[ckpt_paaf.pt] iter 3360: loss 0.9780, time 107.3ms\n",
            "[ckpt_paaf.pt] iter 3370: loss 1.0121, time 105.8ms\n",
            "[ckpt_paaf.pt] iter 3380: loss 1.0262, time 110.2ms\n",
            "[ckpt_paaf.pt] iter 3390: loss 0.9555, time 107.6ms\n",
            "[ckpt_paaf.pt] iter 3400: loss 0.9804, time 108.5ms\n",
            "[ckpt_paaf.pt] iter 3410: loss 1.0119, time 104.8ms\n",
            "[ckpt_paaf.pt] iter 3420: loss 0.9664, time 108.7ms\n",
            "[ckpt_paaf.pt] iter 3430: loss 0.9802, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 3440: loss 0.9835, time 108.8ms\n",
            "[ckpt_paaf.pt] iter 3450: loss 0.9992, time 105.5ms\n",
            "[ckpt_paaf.pt] iter 3460: loss 0.9469, time 108.7ms\n",
            "[ckpt_paaf.pt] iter 3470: loss 1.0028, time 111.5ms\n",
            "[ckpt_paaf.pt] iter 3480: loss 0.9873, time 107.0ms\n",
            "[ckpt_paaf.pt] iter 3490: loss 0.9671, time 108.0ms\n",
            "[ckpt_paaf.pt] iter 3500: train 0.8660, val 0.9324\n",
            "[ckpt_paaf.pt] saving checkpoint at iter 3500 → /content/drive/MyDrive/ECS 189G/out/ckpt_paaf.pt\n",
            "[ckpt_paaf.pt] iter 3500: loss 0.9761, time 13729.1ms\n",
            "[ckpt_paaf.pt] iter 3510: loss 0.9817, time 117.4ms\n",
            "[ckpt_paaf.pt] iter 3520: loss 0.9617, time 113.3ms\n",
            "[ckpt_paaf.pt] iter 3530: loss 0.9833, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 3540: loss 0.9852, time 107.6ms\n",
            "[ckpt_paaf.pt] iter 3550: loss 0.9545, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 3560: loss 0.9791, time 108.8ms\n",
            "[ckpt_paaf.pt] iter 3570: loss 0.9568, time 107.0ms\n",
            "[ckpt_paaf.pt] iter 3580: loss 0.9819, time 107.0ms\n",
            "[ckpt_paaf.pt] iter 3590: loss 0.9584, time 106.6ms\n",
            "[ckpt_paaf.pt] iter 3600: loss 0.9560, time 110.0ms\n",
            "[ckpt_paaf.pt] iter 3610: loss 0.9820, time 75.5ms\n",
            "[ckpt_paaf.pt] iter 3620: loss 0.9827, time 110.9ms\n",
            "[ckpt_paaf.pt] iter 3630: loss 0.9560, time 107.4ms\n",
            "[ckpt_paaf.pt] iter 3640: loss 0.9867, time 107.9ms\n",
            "[ckpt_paaf.pt] iter 3650: loss 0.9716, time 105.2ms\n",
            "[ckpt_paaf.pt] iter 3660: loss 0.9828, time 107.3ms\n",
            "[ckpt_paaf.pt] iter 3670: loss 0.9410, time 107.9ms\n",
            "[ckpt_paaf.pt] iter 3680: loss 0.9631, time 105.9ms\n",
            "[ckpt_paaf.pt] iter 3690: loss 0.9763, time 110.1ms\n",
            "[ckpt_paaf.pt] iter 3700: loss 0.9600, time 106.8ms\n",
            "[ckpt_paaf.pt] iter 3710: loss 0.9164, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 3720: loss 0.9977, time 107.1ms\n",
            "[ckpt_paaf.pt] iter 3730: loss 1.0013, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 3740: loss 0.9676, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 3750: train 0.8667, val 0.9358\n",
            "[ckpt_paaf.pt] iter 3750: loss 0.9629, time 12993.3ms\n",
            "[ckpt_paaf.pt] iter 3760: loss 0.9484, time 110.6ms\n",
            "[ckpt_paaf.pt] iter 3770: loss 0.9516, time 109.6ms\n",
            "[ckpt_paaf.pt] iter 3780: loss 0.9805, time 109.8ms\n",
            "[ckpt_paaf.pt] iter 3790: loss 0.9653, time 111.4ms\n",
            "[ckpt_paaf.pt] iter 3800: loss 0.9461, time 109.2ms\n",
            "[ckpt_paaf.pt] iter 3810: loss 0.9510, time 110.1ms\n",
            "[ckpt_paaf.pt] iter 3820: loss 0.9740, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 3830: loss 0.9640, time 106.6ms\n",
            "[ckpt_paaf.pt] iter 3840: loss 0.9526, time 110.4ms\n",
            "[ckpt_paaf.pt] iter 3850: loss 0.9564, time 107.3ms\n",
            "[ckpt_paaf.pt] iter 3860: loss 0.9707, time 109.7ms\n",
            "[ckpt_paaf.pt] iter 3870: loss 0.9802, time 109.4ms\n",
            "[ckpt_paaf.pt] iter 3880: loss 0.9531, time 110.9ms\n",
            "[ckpt_paaf.pt] iter 3890: loss 0.9633, time 109.6ms\n",
            "[ckpt_paaf.pt] iter 3900: loss 0.9479, time 108.7ms\n",
            "[ckpt_paaf.pt] iter 3910: loss 0.9544, time 110.3ms\n",
            "[ckpt_paaf.pt] iter 3920: loss 0.9297, time 104.9ms\n",
            "[ckpt_paaf.pt] iter 3930: loss 0.9521, time 107.0ms\n",
            "[ckpt_paaf.pt] iter 3940: loss 0.9529, time 107.2ms\n",
            "[ckpt_paaf.pt] iter 3950: loss 0.9621, time 105.9ms\n",
            "[ckpt_paaf.pt] iter 3960: loss 0.9338, time 105.5ms\n",
            "[ckpt_paaf.pt] iter 3970: loss 0.9629, time 108.5ms\n",
            "[ckpt_paaf.pt] iter 3980: loss 0.9327, time 111.6ms\n",
            "[ckpt_paaf.pt] iter 3990: loss 0.9478, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 4000: train 0.8662, val 0.9359\n",
            "[ckpt_paaf.pt] iter 4000: loss 0.9445, time 13128.9ms\n",
            "[ckpt_paaf.pt] iter 4010: loss 0.9422, time 110.8ms\n",
            "[ckpt_paaf.pt] iter 4020: loss 0.9231, time 110.6ms\n",
            "[ckpt_paaf.pt] iter 4030: loss 0.9457, time 107.2ms\n",
            "[ckpt_paaf.pt] iter 4040: loss 0.9642, time 106.2ms\n",
            "[ckpt_paaf.pt] iter 4050: loss 0.9853, time 108.6ms\n",
            "[ckpt_paaf.pt] iter 4060: loss 0.9420, time 110.3ms\n",
            "[ckpt_paaf.pt] iter 4070: loss 0.9488, time 107.8ms\n",
            "[ckpt_paaf.pt] iter 4080: loss 0.9358, time 108.7ms\n",
            "[ckpt_paaf.pt] iter 4090: loss 0.9712, time 108.9ms\n",
            "[ckpt_paaf.pt] iter 4100: loss 0.9641, time 109.1ms\n",
            "[ckpt_paaf.pt] iter 4110: loss 0.9810, time 105.9ms\n",
            "[ckpt_paaf.pt] iter 4120: loss 0.9749, time 104.9ms\n",
            "[ckpt_paaf.pt] iter 4130: loss 0.9479, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 4140: loss 0.9170, time 109.2ms\n",
            "[ckpt_paaf.pt] iter 4150: loss 0.9474, time 107.5ms\n",
            "[ckpt_paaf.pt] iter 4160: loss 0.9468, time 110.1ms\n",
            "[ckpt_paaf.pt] iter 4170: loss 0.9616, time 108.6ms\n",
            "[ckpt_paaf.pt] iter 4180: loss 0.9416, time 105.9ms\n",
            "[ckpt_paaf.pt] iter 4190: loss 0.9749, time 105.0ms\n",
            "[ckpt_paaf.pt] iter 4200: loss 0.9600, time 107.8ms\n",
            "[ckpt_paaf.pt] iter 4210: loss 0.9489, time 105.2ms\n",
            "[ckpt_paaf.pt] iter 4220: loss 0.9484, time 105.8ms\n",
            "[ckpt_paaf.pt] iter 4230: loss 0.9490, time 107.2ms\n",
            "[ckpt_paaf.pt] iter 4240: loss 0.9282, time 107.3ms\n",
            "[ckpt_paaf.pt] iter 4250: train 0.8640, val 0.9355\n",
            "[ckpt_paaf.pt] iter 4250: loss 0.9633, time 12963.8ms\n",
            "[ckpt_paaf.pt] iter 4260: loss 0.9451, time 108.9ms\n",
            "[ckpt_paaf.pt] iter 4270: loss 0.9478, time 107.0ms\n",
            "[ckpt_paaf.pt] iter 4280: loss 0.9476, time 106.2ms\n",
            "[ckpt_paaf.pt] iter 4290: loss 0.9736, time 111.1ms\n",
            "[ckpt_paaf.pt] iter 4300: loss 0.9138, time 106.6ms\n",
            "[ckpt_paaf.pt] iter 4310: loss 0.9315, time 106.1ms\n",
            "[ckpt_paaf.pt] iter 4320: loss 0.9334, time 105.4ms\n",
            "[ckpt_paaf.pt] iter 4330: loss 0.9258, time 107.1ms\n",
            "[ckpt_paaf.pt] iter 4340: loss 0.9690, time 108.7ms\n",
            "[ckpt_paaf.pt] iter 4350: loss 0.9245, time 108.0ms\n",
            "[ckpt_paaf.pt] iter 4360: loss 0.9620, time 107.6ms\n",
            "[ckpt_paaf.pt] iter 4370: loss 0.9587, time 107.7ms\n",
            "[ckpt_paaf.pt] iter 4380: loss 0.9258, time 108.7ms\n",
            "[ckpt_paaf.pt] iter 4390: loss 0.9550, time 104.9ms\n",
            "[ckpt_paaf.pt] iter 4400: loss 0.9535, time 108.3ms\n",
            "[ckpt_paaf.pt] iter 4410: loss 0.9228, time 108.6ms\n",
            "[ckpt_paaf.pt] iter 4420: loss 0.9720, time 108.0ms\n",
            "[ckpt_paaf.pt] iter 4430: loss 0.9381, time 107.1ms\n",
            "[ckpt_paaf.pt] iter 4440: loss 0.9378, time 105.3ms\n",
            "[ckpt_paaf.pt] iter 4450: loss 0.9335, time 106.1ms\n",
            "[ckpt_paaf.pt] iter 4460: loss 0.9272, time 108.2ms\n",
            "[ckpt_paaf.pt] iter 4470: loss 0.9418, time 105.6ms\n",
            "[ckpt_paaf.pt] iter 4480: loss 0.9481, time 107.7ms\n",
            "[ckpt_paaf.pt] iter 4490: loss 0.9334, time 109.0ms\n",
            "[ckpt_paaf.pt] iter 4500: train 0.8644, val 0.9347\n",
            "[ckpt_paaf.pt] iter 4500: loss 0.9107, time 13053.7ms\n",
            "[ckpt_paaf.pt] iter 4510: loss 0.9054, time 109.1ms\n",
            "[ckpt_paaf.pt] iter 4520: loss 0.9535, time 107.1ms\n",
            "[ckpt_paaf.pt] iter 4530: loss 0.9186, time 106.2ms\n",
            "[ckpt_paaf.pt] iter 4540: loss 0.9243, time 105.6ms\n",
            "[ckpt_paaf.pt] iter 4550: loss 0.9508, time 107.2ms\n",
            "[ckpt_paaf.pt] iter 4560: loss 0.9436, time 106.1ms\n",
            "[ckpt_paaf.pt] iter 4570: loss 0.9351, time 104.9ms\n",
            "[ckpt_paaf.pt] iter 4580: loss 0.9307, time 108.5ms\n",
            "[ckpt_paaf.pt] iter 4590: loss 0.9612, time 108.4ms\n",
            "[ckpt_paaf.pt] iter 4600: loss 0.9510, time 111.3ms\n",
            "[ckpt_paaf.pt] iter 4610: loss 0.9235, time 108.5ms\n",
            "[ckpt_paaf.pt] iter 4620: loss 0.9290, time 107.3ms\n",
            "[ckpt_paaf.pt] iter 4630: loss 0.9142, time 110.0ms\n",
            "[ckpt_paaf.pt] iter 4640: loss 0.9246, time 107.3ms\n",
            "[ckpt_paaf.pt] iter 4650: loss 0.9407, time 110.3ms\n",
            "[ckpt_paaf.pt] iter 4660: loss 0.8887, time 110.7ms\n",
            "[ckpt_paaf.pt] iter 4670: loss 0.9221, time 111.7ms\n",
            "[ckpt_paaf.pt] iter 4680: loss 0.9212, time 111.7ms\n",
            "[ckpt_paaf.pt] iter 4690: loss 0.9421, time 107.0ms\n",
            "[ckpt_paaf.pt] iter 4700: loss 0.9377, time 109.0ms\n",
            "[ckpt_paaf.pt] iter 4710: loss 0.9243, time 106.4ms\n",
            "[ckpt_paaf.pt] iter 4720: loss 0.9464, time 107.2ms\n",
            "[ckpt_paaf.pt] iter 4730: loss 0.9258, time 110.5ms\n",
            "[ckpt_paaf.pt] iter 4740: loss 0.9456, time 112.3ms\n",
            "[ckpt_paaf.pt] iter 4750: train 0.8675, val 0.9357\n",
            "[ckpt_paaf.pt] iter 4750: loss 0.9376, time 12975.4ms\n",
            "[ckpt_paaf.pt] iter 4760: loss 0.9481, time 106.3ms\n",
            "[ckpt_paaf.pt] iter 4770: loss 0.9258, time 105.7ms\n",
            "[ckpt_paaf.pt] iter 4780: loss 0.9189, time 106.9ms\n",
            "[ckpt_paaf.pt] iter 4790: loss 0.9471, time 106.3ms\n",
            "[ckpt_paaf.pt] iter 4800: loss 0.9321, time 107.5ms\n",
            "[ckpt_paaf.pt] iter 4810: loss 0.9467, time 106.7ms\n",
            "[ckpt_paaf.pt] iter 4820: loss 0.9476, time 110.9ms\n",
            "[ckpt_paaf.pt] iter 4830: loss 0.9240, time 108.1ms\n",
            "[ckpt_paaf.pt] iter 4840: loss 0.9331, time 108.8ms\n",
            "[ckpt_paaf.pt] iter 4850: loss 0.9423, time 109.0ms\n",
            "[ckpt_paaf.pt] iter 4860: loss 0.9134, time 109.8ms\n",
            "[ckpt_paaf.pt] iter 4870: loss 0.9010, time 106.3ms\n",
            "[ckpt_paaf.pt] iter 4880: loss 0.9366, time 109.1ms\n",
            "[ckpt_paaf.pt] iter 4890: loss 0.9481, time 109.3ms\n",
            "[ckpt_paaf.pt] iter 4900: loss 0.9333, time 108.5ms\n",
            "[ckpt_paaf.pt] iter 4910: loss 0.9342, time 107.5ms\n",
            "[ckpt_paaf.pt] iter 4920: loss 0.9776, time 107.0ms\n",
            "[ckpt_paaf.pt] iter 4930: loss 0.9074, time 106.9ms\n",
            "[ckpt_paaf.pt] iter 4940: loss 0.9413, time 106.2ms\n",
            "[ckpt_paaf.pt] iter 4950: loss 0.9493, time 111.2ms\n",
            "[ckpt_paaf.pt] iter 4960: loss 0.9547, time 106.9ms\n",
            "[ckpt_paaf.pt] iter 4970: loss 0.9280, time 107.0ms\n",
            "[ckpt_paaf.pt] iter 4980: loss 0.9428, time 106.0ms\n",
            "[ckpt_paaf.pt] iter 4990: loss 0.9015, time 107.9ms\n",
            "[ckpt_paaf.pt] iter 5000: train 0.8657, val 0.9361\n",
            "[ckpt_paaf.pt] final save at iter 5000 → /content/drive/MyDrive/ECS 189G/out/ckpt_paaf.pt\n",
            "[ckpt_paaf.pt] Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT + PAAF + LoRA"
      ],
      "metadata": {
        "id": "keZKw5U7S3Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell E: NanoGPT + LoRA + PAAF (still no DAR)\n",
        "\n",
        "train_and_save_variant(\n",
        "    out_dir       = out_dir,\n",
        "    ckpt_filename = 'ckpt_lora_paaf.pt',\n",
        "    use_lora      = True,\n",
        "    use_paaf      = True,\n",
        "    use_dar       = False,\n",
        "    dar_weight    = 0.0,\n",
        "    max_iters     = max_iters\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXcq9aAzS5kZ",
        "outputId": "49c96ea1-b75b-4672-ff80-9221ce41aaaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 10.66M\n",
            "--> Injecting LoRA adapters (rank=8, alpha=16.0)\n",
            "num decayed parameter tensors: 74, with 11,048,448 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-895d20065530>:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_local = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ckpt_lora_paaf.pt] iter 0: train 0.8641, val 0.9347\n",
            "[ckpt_lora_paaf.pt] iter 0: loss 4.6493, time 12628.9ms\n",
            "[ckpt_lora_paaf.pt] iter 10: loss 3.1779, time 148.2ms\n",
            "[ckpt_lora_paaf.pt] iter 20: loss 2.7101, time 147.6ms\n",
            "[ckpt_lora_paaf.pt] iter 30: loss 2.5019, time 147.6ms\n",
            "[ckpt_lora_paaf.pt] iter 40: loss 2.4176, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 50: loss 2.3965, time 148.8ms\n",
            "[ckpt_lora_paaf.pt] iter 60: loss 2.3636, time 150.1ms\n",
            "[ckpt_lora_paaf.pt] iter 70: loss 2.3707, time 149.3ms\n",
            "[ckpt_lora_paaf.pt] iter 80: loss 2.3544, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 90: loss 2.3362, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 100: loss 2.3295, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 110: loss 2.3535, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 120: loss 2.3330, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 130: loss 2.2896, time 149.7ms\n",
            "[ckpt_lora_paaf.pt] iter 140: loss 2.2801, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 150: loss 2.2358, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 160: loss 2.1331, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 170: loss 2.0754, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 180: loss 2.0498, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 190: loss 1.9894, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 200: loss 1.9359, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 210: loss 1.8951, time 149.3ms\n",
            "[ckpt_lora_paaf.pt] iter 220: loss 1.8694, time 151.7ms\n",
            "[ckpt_lora_paaf.pt] iter 230: loss 1.8412, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 240: loss 1.8332, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 250: train 0.8641, val 0.9348\n",
            "[ckpt_lora_paaf.pt] iter 250: loss 1.8064, time 13079.3ms\n",
            "[ckpt_lora_paaf.pt] iter 260: loss 1.8188, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 270: loss 1.7334, time 154.0ms\n",
            "[ckpt_lora_paaf.pt] iter 280: loss 1.7392, time 152.1ms\n",
            "[ckpt_lora_paaf.pt] iter 290: loss 1.7097, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 300: loss 1.7056, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 310: loss 1.7236, time 150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 320: loss 1.6946, time 152.3ms\n",
            "[ckpt_lora_paaf.pt] iter 330: loss 1.6803, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 340: loss 1.6136, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 350: loss 1.6174, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 360: loss 1.6494, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 370: loss 1.6185, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 380: loss 1.5749, time 149.7ms\n",
            "[ckpt_lora_paaf.pt] iter 390: loss 1.5886, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 400: loss 1.5654, time 149.3ms\n",
            "[ckpt_lora_paaf.pt] iter 410: loss 1.5366, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 420: loss 1.5550, time 149.7ms\n",
            "[ckpt_lora_paaf.pt] iter 430: loss 1.5517, time 149.5ms\n",
            "[ckpt_lora_paaf.pt] iter 440: loss 1.5307, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 450: loss 1.5557, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 460: loss 1.5240, time 148.4ms\n",
            "[ckpt_lora_paaf.pt] iter 470: loss 1.5089, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 480: loss 1.5104, time 149.5ms\n",
            "[ckpt_lora_paaf.pt] iter 490: loss 1.4885, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 500: train 0.8651, val 0.9381\n",
            "[ckpt_lora_paaf.pt] iter 500: loss 1.4960, time 12862.0ms\n",
            "[ckpt_lora_paaf.pt] iter 510: loss 1.4910, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 520: loss 1.4959, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 530: loss 1.4716, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 540: loss 1.4708, time 152.3ms\n",
            "[ckpt_lora_paaf.pt] iter 550: loss 1.4653, time 149.4ms\n",
            "[ckpt_lora_paaf.pt] iter 560: loss 1.4892, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 570: loss 1.4134, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 580: loss 1.4055, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 590: loss 1.4509, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 600: loss 1.4570, time 153.9ms\n",
            "[ckpt_lora_paaf.pt] iter 610: loss 1.4802, time 153.2ms\n",
            "[ckpt_lora_paaf.pt] iter 620: loss 1.4172, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 630: loss 1.4227, time 152.0ms\n",
            "[ckpt_lora_paaf.pt] iter 640: loss 1.4123, time 153.0ms\n",
            "[ckpt_lora_paaf.pt] iter 650: loss 1.3992, time 152.6ms\n",
            "[ckpt_lora_paaf.pt] iter 660: loss 1.4151, time 152.1ms\n",
            "[ckpt_lora_paaf.pt] iter 670: loss 1.3792, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 680: loss 1.4023, time 149.2ms\n",
            "[ckpt_lora_paaf.pt] iter 690: loss 1.4031, time 153.2ms\n",
            "[ckpt_lora_paaf.pt] iter 700: loss 1.3745, time 152.5ms\n",
            "[ckpt_lora_paaf.pt] iter 710: loss 1.3910, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 720: loss 1.3872, time 152.9ms\n",
            "[ckpt_lora_paaf.pt] iter 730: loss 1.3557, time 148.7ms\n",
            "[ckpt_lora_paaf.pt] iter 740: loss 1.3662, time 153.0ms\n",
            "[ckpt_lora_paaf.pt] iter 750: train 0.8652, val 0.9332\n",
            "[ckpt_lora_paaf.pt] saving checkpoint at iter 750 → /content/drive/MyDrive/ECS 189G/out/ckpt_lora_paaf.pt\n",
            "[ckpt_lora_paaf.pt] iter 750: loss 1.3786, time 13345.3ms\n",
            "[ckpt_lora_paaf.pt] iter 760: loss 1.3366, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 770: loss 1.3425, time 152.8ms\n",
            "[ckpt_lora_paaf.pt] iter 780: loss 1.3188, time 149.3ms\n",
            "[ckpt_lora_paaf.pt] iter 790: loss 1.3631, time 152.0ms\n",
            "[ckpt_lora_paaf.pt] iter 800: loss 1.3494, time 149.6ms\n",
            "[ckpt_lora_paaf.pt] iter 810: loss 1.3336, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 820: loss 1.3093, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 830: loss 1.3261, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 840: loss 1.3228, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 850: loss 1.3298, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 860: loss 1.2860, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 870: loss 1.3232, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 880: loss 1.3361, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 890: loss 1.2922, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 900: loss 1.2936, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 910: loss 1.2897, time 152.0ms\n",
            "[ckpt_lora_paaf.pt] iter 920: loss 1.2498, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 930: loss 1.3067, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 940: loss 1.2666, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 950: loss 1.2734, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 960: loss 1.2731, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 970: loss 1.2791, time 154.1ms\n",
            "[ckpt_lora_paaf.pt] iter 980: loss 1.2668, time 154.2ms\n",
            "[ckpt_lora_paaf.pt] iter 990: loss 1.2670, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1000: train 0.8636, val 0.9356\n",
            "[ckpt_lora_paaf.pt] iter 1000: loss 1.2627, time 12985.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1010: loss 1.2533, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1020: loss 1.2662, time 152.8ms\n",
            "[ckpt_lora_paaf.pt] iter 1030: loss 1.2380, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 1040: loss 1.2281, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 1050: loss 1.2736, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 1060: loss 1.2143, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1070: loss 1.2821, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 1080: loss 1.2554, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1090: loss 1.2243, time 149.7ms\n",
            "[ckpt_lora_paaf.pt] iter 1100: loss 1.2187, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1110: loss 1.2314, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1120: loss 1.2326, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1130: loss 1.2623, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1140: loss 1.2193, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1150: loss 1.2214, time 153.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1160: loss 1.1907, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1170: loss 1.2462, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 1180: loss 1.2201, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1190: loss 1.2369, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1200: loss 1.2172, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1210: loss 1.2043, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1220: loss 1.1809, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1230: loss 1.2065, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1240: loss 1.1911, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 1250: train 0.8683, val 0.9372\n",
            "[ckpt_lora_paaf.pt] iter 1250: loss 1.2123, time 12889.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1260: loss 1.2153, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 1270: loss 1.2200, time 149.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1280: loss 1.2358, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1290: loss 1.2332, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 1300: loss 1.2310, time 148.6ms\n",
            "[ckpt_lora_paaf.pt] iter 1310: loss 1.1998, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1320: loss 1.1979, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 1330: loss 1.2129, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1340: loss 1.1967, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1350: loss 1.1734, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1360: loss 1.1619, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 1370: loss 1.1931, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1380: loss 1.1493, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1390: loss 1.1596, time 149.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1400: loss 1.1789, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1410: loss 1.1749, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1420: loss 1.1666, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1430: loss 1.1559, time 149.8ms\n",
            "[ckpt_lora_paaf.pt] iter 1440: loss 1.1633, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1450: loss 1.1873, time 148.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1460: loss 1.1587, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 1470: loss 1.1416, time 156.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1480: loss 1.1456, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1490: loss 1.1731, time 152.6ms\n",
            "[ckpt_lora_paaf.pt] iter 1500: train 0.8641, val 0.9345\n",
            "[ckpt_lora_paaf.pt] iter 1500: loss 1.1110, time 12988.6ms\n",
            "[ckpt_lora_paaf.pt] iter 1510: loss 1.1340, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1520: loss 1.1372, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1530: loss 1.1532, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1540: loss 1.1553, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1550: loss 1.1241, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 1560: loss 1.1630, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1570: loss 1.1582, time 152.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1580: loss 1.1401, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1590: loss 1.0924, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1600: loss 1.1566, time 152.0ms\n",
            "[ckpt_lora_paaf.pt] iter 1610: loss 1.1204, time 149.7ms\n",
            "[ckpt_lora_paaf.pt] iter 1620: loss 1.1466, time 150.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1630: loss 1.1088, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1640: loss 1.1575, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1650: loss 1.1545, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1660: loss 1.1077, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1670: loss 1.1441, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1680: loss 1.1356, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 1690: loss 1.1254, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 1700: loss 1.1255, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1710: loss 1.1111, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1720: loss 1.1292, time 150.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1730: loss 1.1481, time 149.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1740: loss 1.1074, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1750: train 0.8670, val 0.9339\n",
            "[ckpt_lora_paaf.pt] iter 1750: loss 1.1137, time 13307.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1760: loss 1.1236, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 1770: loss 1.1117, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1780: loss 1.1055, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1790: loss 1.1174, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1800: loss 1.1330, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 1810: loss 1.0908, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 1820: loss 1.1310, time 153.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1830: loss 1.0926, time 150.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1840: loss 1.1178, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1850: loss 1.0851, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 1860: loss 1.1082, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1870: loss 1.1122, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1880: loss 1.1215, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1890: loss 1.0721, time 150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 1900: loss 1.1012, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1910: loss 1.1106, time 152.1ms\n",
            "[ckpt_lora_paaf.pt] iter 1920: loss 1.1105, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 1930: loss 1.1275, time 152.0ms\n",
            "[ckpt_lora_paaf.pt] iter 1940: loss 1.1063, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 1950: loss 1.1077, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1960: loss 1.1214, time 153.4ms\n",
            "[ckpt_lora_paaf.pt] iter 1970: loss 1.0857, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 1980: loss 1.1024, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 1990: loss 1.0925, time 153.1ms\n",
            "[ckpt_lora_paaf.pt] iter 2000: train 0.8628, val 0.9367\n",
            "[ckpt_lora_paaf.pt] iter 2000: loss 1.1265, time 12954.4ms\n",
            "[ckpt_lora_paaf.pt] iter 2010: loss 1.1189, time 148.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2020: loss 1.0949, time 151.7ms\n",
            "[ckpt_lora_paaf.pt] iter 2030: loss 1.1043, time 152.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2040: loss 1.0898, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2050: loss 1.1071, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2060: loss 1.0990, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2070: loss 1.0814, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2080: loss 1.0901, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 2090: loss 1.1165, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2100: loss 1.0646, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 2110: loss 1.1175, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 2120: loss 1.0843, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 2130: loss 1.0803, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2140: loss 1.0813, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 2150: loss 1.0929, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2160: loss 1.0825, time 148.7ms\n",
            "[ckpt_lora_paaf.pt] iter 2170: loss 1.0694, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2180: loss 1.0920, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2190: loss 1.0781, time 149.7ms\n",
            "[ckpt_lora_paaf.pt] iter 2200: loss 1.0568, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2210: loss 1.0833, time 149.2ms\n",
            "[ckpt_lora_paaf.pt] iter 2220: loss 1.0534, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2230: loss 1.0872, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 2240: loss 1.0846, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 2250: train 0.8648, val 0.9335\n",
            "[ckpt_lora_paaf.pt] iter 2250: loss 1.1009, time 13012.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2260: loss 1.0772, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 2270: loss 1.0393, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2280: loss 1.0931, time 153.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2290: loss 1.0599, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2300: loss 1.1056, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2310: loss 1.0925, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2320: loss 1.0788, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2330: loss 1.1170, time 149.6ms\n",
            "[ckpt_lora_paaf.pt] iter 2340: loss 1.0643, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2350: loss 1.0403, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2360: loss 1.0900, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2370: loss 1.0720, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2380: loss 1.0897, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2390: loss 1.0811, time 152.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2400: loss 1.0893, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 2410: loss 1.0505, time 148.7ms\n",
            "[ckpt_lora_paaf.pt] iter 2420: loss 1.0513, time 153.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2430: loss 1.0616, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2440: loss 1.0795, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2450: loss 1.0822, time 150.1ms\n",
            "[ckpt_lora_paaf.pt] iter 2460: loss 1.0506, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2470: loss 1.0641, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2480: loss 1.0426, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2490: loss 1.0410, time 149.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2500: train 0.8654, val 0.9372\n",
            "[ckpt_lora_paaf.pt] iter 2500: loss 1.0883, time 12944.6ms\n",
            "[ckpt_lora_paaf.pt] iter 2510: loss 1.0775, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2520: loss 1.0603, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 2530: loss 1.0582, time 151.7ms\n",
            "[ckpt_lora_paaf.pt] iter 2540: loss 1.0528, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2550: loss 1.0405, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 2560: loss 1.0490, time 149.7ms\n",
            "[ckpt_lora_paaf.pt] iter 2570: loss 1.0256, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2580: loss 1.0215, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2590: loss 1.0684, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2600: loss 1.0430, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2610: loss 1.0454, time 152.7ms\n",
            "[ckpt_lora_paaf.pt] iter 2620: loss 1.0429, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 2630: loss 1.0738, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2640: loss 1.0348, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 2650: loss 1.0561, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 2660: loss 1.0680, time 149.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2670: loss 1.0409, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 2680: loss 1.0627, time 152.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2690: loss 1.0672, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2700: loss 1.0653, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2710: loss 1.0552, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 2720: loss 0.9942, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2730: loss 1.0219, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2740: loss 1.0233, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2750: train 0.8656, val 0.9349\n",
            "[ckpt_lora_paaf.pt] iter 2750: loss 1.0492, time 12914.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2760: loss 1.0312, time 149.6ms\n",
            "[ckpt_lora_paaf.pt] iter 2770: loss 1.0315, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 2780: loss 1.0413, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2790: loss 1.0243, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2800: loss 1.0215, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 2810: loss 1.0326, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 2820: loss 1.0186, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2830: loss 1.0139, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2840: loss 1.0367, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 2850: loss 1.0303, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2860: loss 1.0355, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 2870: loss 1.0413, time 149.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2880: loss 1.0133, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 2890: loss 1.0553, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2900: loss 1.0289, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 2910: loss 1.0365, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 2920: loss 1.0020, time 152.6ms\n",
            "[ckpt_lora_paaf.pt] iter 2930: loss 1.0352, time 152.1ms\n",
            "[ckpt_lora_paaf.pt] iter 2940: loss 1.0226, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 2950: loss 1.0458, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 2960: loss 1.0078, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 2970: loss 1.0292, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 2980: loss 1.0342, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 2990: loss 1.0178, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3000: train 0.8651, val 0.9359\n",
            "[ckpt_lora_paaf.pt] iter 3000: loss 1.0134, time 13022.4ms\n",
            "[ckpt_lora_paaf.pt] iter 3010: loss 1.0270, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3020: loss 0.9997, time 153.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3030: loss 1.0353, time 151.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3040: loss 1.0320, time 151.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3050: loss 1.0260, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 3060: loss 1.0462, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3070: loss 1.0145, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3080: loss 1.0242, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3090: loss 1.0021, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 3100: loss 0.9862, time 149.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3110: loss 0.9925, time 150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3120: loss 1.0175, time 150.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3130: loss 1.0105, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 3140: loss 0.9975, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 3150: loss 1.0197, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3160: loss 1.0289, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 3170: loss 1.0046, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 3180: loss 1.0258, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3190: loss 1.0290, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 3200: loss 1.0013, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3210: loss 1.0094, time 149.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3220: loss 1.0018, time 148.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3230: loss 1.0076, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3240: loss 1.0154, time 149.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3250: train 0.8653, val 0.9363\n",
            "[ckpt_lora_paaf.pt] iter 3250: loss 0.9987, time 12959.3ms\n",
            "[ckpt_lora_paaf.pt] iter 3260: loss 1.0090, time 149.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3270: loss 0.9807, time 152.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3280: loss 0.9935, time 150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3290: loss 1.0017, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3300: loss 1.0093, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3310: loss 1.0237, time 149.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3320: loss 1.0222, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3330: loss 1.0031, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 3340: loss 0.9886, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 3350: loss 0.9862, time 150.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3360: loss 0.9598, time 151.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3370: loss 1.0177, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3380: loss 1.0082, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3390: loss 1.0067, time 152.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3400: loss 1.0182, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3410: loss 0.9820, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 3420: loss 0.9808, time 150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3430: loss 1.0000, time 153.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3440: loss 0.9915, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 3450: loss 0.9808, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 3460: loss 0.9772, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 3470: loss 0.9941, time 146.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3480: loss 1.0101, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 3490: loss 1.0003, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3500: train 0.8640, val 0.9341\n",
            "[ckpt_lora_paaf.pt] iter 3500: loss 0.9833, time 13125.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3510: loss 1.0033, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 3520: loss 0.9761, time 152.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3530: loss 0.9706, time 152.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3540: loss 0.9981, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3550: loss 0.9901, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 3560: loss 0.9876, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3570: loss 0.9873, time 149.3ms\n",
            "[ckpt_lora_paaf.pt] iter 3580: loss 0.9804, time 152.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3590: loss 0.9784, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 3600: loss 1.0036, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3610: loss 0.9778, time 152.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3620: loss 0.9683, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3630: loss 0.9744, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3640: loss 0.9883, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 3650: loss 0.9910, time 149.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3660: loss 0.9710, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 3670: loss 1.0023, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3680: loss 0.9826, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3690: loss 0.9543, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 3700: loss 0.9963, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3710: loss 0.9645, time 150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3720: loss 0.9844, time 149.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3730: loss 0.9609, time 152.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3740: loss 1.0050, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 3750: train 0.8662, val 0.9344\n",
            "[ckpt_lora_paaf.pt] iter 3750: loss 0.9788, time 13150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3760: loss 0.9720, time 149.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3770: loss 0.9641, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3780: loss 0.9744, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3790: loss 1.0133, time 151.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3800: loss 0.9750, time 152.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3810: loss 0.9651, time 152.0ms\n",
            "[ckpt_lora_paaf.pt] iter 3820: loss 0.9659, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3830: loss 0.9803, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 3840: loss 0.9633, time 150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3850: loss 0.9967, time 152.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3860: loss 0.9661, time 150.1ms\n",
            "[ckpt_lora_paaf.pt] iter 3870: loss 0.9653, time 150.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3880: loss 0.9447, time 149.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3890: loss 0.9703, time 151.7ms\n",
            "[ckpt_lora_paaf.pt] iter 3900: loss 0.9559, time 147.5ms\n",
            "[ckpt_lora_paaf.pt] iter 3910: loss 0.9507, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 3920: loss 1.0260, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3930: loss 0.9779, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 3940: loss 0.9799, time 149.6ms\n",
            "[ckpt_lora_paaf.pt] iter 3950: loss 0.9386, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 3960: loss 0.9896, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 3970: loss 0.9886, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 3980: loss 0.9637, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 3990: loss 0.9619, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4000: train 0.8648, val 0.9350\n",
            "[ckpt_lora_paaf.pt] iter 4000: loss 0.9540, time 13115.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4010: loss 0.9343, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 4020: loss 0.9693, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4030: loss 0.9725, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4040: loss 0.9704, time 150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 4050: loss 0.9502, time 153.5ms\n",
            "[ckpt_lora_paaf.pt] iter 4060: loss 0.9346, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4070: loss 0.9880, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4080: loss 0.9702, time 153.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4090: loss 0.9801, time 150.6ms\n",
            "[ckpt_lora_paaf.pt] iter 4100: loss 0.9669, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4110: loss 0.9508, time 149.1ms\n",
            "[ckpt_lora_paaf.pt] iter 4120: loss 0.9978, time 150.0ms\n",
            "[ckpt_lora_paaf.pt] iter 4130: loss 0.9755, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4140: loss 0.9438, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4150: loss 0.9640, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 4160: loss 0.9592, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4170: loss 0.9369, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4180: loss 0.9581, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4190: loss 0.9707, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4200: loss 0.9768, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4210: loss 0.9650, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4220: loss 0.9508, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4230: loss 0.9401, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4240: loss 0.9509, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4250: train 0.8653, val 0.9357\n",
            "[ckpt_lora_paaf.pt] iter 4250: loss 0.9555, time 13036.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4260: loss 0.9564, time 152.1ms\n",
            "[ckpt_lora_paaf.pt] iter 4270: loss 0.9144, time 149.7ms\n",
            "[ckpt_lora_paaf.pt] iter 4280: loss 0.9794, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4290: loss 0.9859, time 154.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4300: loss 0.9708, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4310: loss 0.9786, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4320: loss 0.9518, time 153.1ms\n",
            "[ckpt_lora_paaf.pt] iter 4330: loss 0.9624, time 152.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4340: loss 0.9503, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4350: loss 0.9585, time 152.0ms\n",
            "[ckpt_lora_paaf.pt] iter 4360: loss 0.9532, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4370: loss 0.9445, time 151.7ms\n",
            "[ckpt_lora_paaf.pt] iter 4380: loss 0.9638, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 4390: loss 0.9338, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 4400: loss 0.9792, time 149.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4410: loss 0.9222, time 152.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4420: loss 0.9477, time 152.1ms\n",
            "[ckpt_lora_paaf.pt] iter 4430: loss 0.9476, time 152.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4440: loss 0.9204, time 154.0ms\n",
            "[ckpt_lora_paaf.pt] iter 4450: loss 0.9724, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4460: loss 0.9513, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 4470: loss 0.9871, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 4480: loss 0.9430, time 152.5ms\n",
            "[ckpt_lora_paaf.pt] iter 4490: loss 0.9670, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4500: train 0.8674, val 0.9361\n",
            "[ckpt_lora_paaf.pt] iter 4500: loss 0.9591, time 13156.7ms\n",
            "[ckpt_lora_paaf.pt] iter 4510: loss 0.9556, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 4520: loss 0.9470, time 152.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4530: loss 0.9360, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4540: loss 0.9702, time 150.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4550: loss 0.9526, time 152.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4560: loss 0.9970, time 149.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4570: loss 0.9333, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 4580: loss 0.9365, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4590: loss 0.9268, time 149.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4600: loss 0.9528, time 148.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4610: loss 0.9333, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4620: loss 0.9816, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4630: loss 0.9674, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4640: loss 0.9305, time 151.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4650: loss 0.9441, time 150.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4660: loss 0.9303, time 148.6ms\n",
            "[ckpt_lora_paaf.pt] iter 4670: loss 0.9232, time 152.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4680: loss 0.9303, time 148.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4690: loss 0.9495, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4700: loss 0.9413, time 149.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4710: loss 0.9412, time 153.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4720: loss 0.9518, time 152.0ms\n",
            "[ckpt_lora_paaf.pt] iter 4730: loss 0.9418, time 152.1ms\n",
            "[ckpt_lora_paaf.pt] iter 4740: loss 0.9725, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4750: train 0.8661, val 0.9353\n",
            "[ckpt_lora_paaf.pt] iter 4750: loss 0.9539, time 13136.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4760: loss 0.9361, time 152.1ms\n",
            "[ckpt_lora_paaf.pt] iter 4770: loss 0.9487, time 149.5ms\n",
            "[ckpt_lora_paaf.pt] iter 4780: loss 0.9503, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 4790: loss 0.9439, time 151.9ms\n",
            "[ckpt_lora_paaf.pt] iter 4800: loss 0.9304, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 4810: loss 0.9236, time 154.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4820: loss 0.9753, time 150.4ms\n",
            "[ckpt_lora_paaf.pt] iter 4830: loss 0.9276, time 151.6ms\n",
            "[ckpt_lora_paaf.pt] iter 4840: loss 0.9582, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4850: loss 0.9453, time 151.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4860: loss 0.9471, time 150.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4870: loss 0.9523, time 151.0ms\n",
            "[ckpt_lora_paaf.pt] iter 4880: loss 0.9349, time 151.8ms\n",
            "[ckpt_lora_paaf.pt] iter 4890: loss 0.9199, time 151.1ms\n",
            "[ckpt_lora_paaf.pt] iter 4900: loss 0.9422, time 153.6ms\n",
            "[ckpt_lora_paaf.pt] iter 4910: loss 0.9478, time 149.6ms\n",
            "[ckpt_lora_paaf.pt] iter 4920: loss 0.9590, time 150.3ms\n",
            "[ckpt_lora_paaf.pt] iter 4930: loss 0.9538, time 149.6ms\n",
            "[ckpt_lora_paaf.pt] iter 4940: loss 0.9572, time 152.1ms\n",
            "[ckpt_lora_paaf.pt] iter 4950: loss 0.9372, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 4960: loss 0.9857, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 4970: loss 0.9436, time 151.2ms\n",
            "[ckpt_lora_paaf.pt] iter 4980: loss 0.9303, time 151.5ms\n",
            "[ckpt_lora_paaf.pt] iter 4990: loss 0.9555, time 150.7ms\n",
            "[ckpt_lora_paaf.pt] iter 5000: train 0.8663, val 0.9348\n",
            "[ckpt_lora_paaf.pt] final save at iter 5000 → /content/drive/MyDrive/ECS 189G/out/ckpt_lora_paaf.pt\n",
            "[ckpt_lora_paaf.pt] Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARTM (Adversarial-Repair Thinking Module)"
      ],
      "metadata": {
        "id": "T6K5rL8KVgpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARTM_FFN: a small two-layer MLP to compute Δh from concatenated features\n",
        "class ARTM_FFN(nn.Module):\n",
        "    \"\"\"\n",
        "    The ARTM feed‐forward network.\n",
        "    Inputs a feature vector of size feat_dim, outputs a repair vector of size hidden_dim (n_embd).\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim: int, feat_dim: int):\n",
        "        super().__init__()\n",
        "        # First linear layer: feat_dim → hidden_dim\n",
        "        self.fc1 = nn.Linear(feat_dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        # Second linear layer: hidden_dim → hidden_dim\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, feat: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        feat: (B, T, feat_dim)  or (B, feat_dim) if single position\n",
        "        Returns: (B, T, hidden_dim) or (B, hidden_dim)\n",
        "        \"\"\"\n",
        "        x = self.fc1(feat)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # this is Δh"
      ],
      "metadata": {
        "id": "EXDpRl-iVg7V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Construct ARTM‐augmented GPT\n",
        "gptconf = GPTConfig(\n",
        "    block_size=block_size,\n",
        "    vocab_size=meta_vocab_size,meta\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    dropout=dropout,\n",
        "    bias=bias,\n",
        ")\n",
        "model = GPT(gptconf)  # uses TransformerBlockWithARTM internally\n",
        "\n",
        "# 2) (Optional) Inject LoRA\n",
        "model = inject_lora(model, rank=8, alpha=16)\n",
        "model.to(device)\n",
        "\n",
        "# 3) Set up optimizer, scaler, etc., then run your training loop:\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "for iter_num in range(max_iters):\n",
        "    # compute x, y batches with get_batch()\n",
        "    logits, loss_ce = model(X, Y)  # this now includes ARTM in each block\n",
        "    if use_dar:\n",
        "        loss = loss_ce + dar_weight * DAR.penalty(model)\n",
        "    else:\n",
        "        loss = loss_ce\n",
        "    scaler.scale(loss).backward()\n",
        "    ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "oVuo8mSJY_0k",
        "outputId": "b62d6304-20da-43b5-f735-9fdb1b654f2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'meta_vocab_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-21240d1c6b42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m gptconf = GPTConfig(\n\u001b[1;32m      3\u001b[0m     \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mn_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mn_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'meta_vocab_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper to Load Checkpoints as Classification Models\n",
        "\n"
      ],
      "metadata": {
        "id": "grB2d3ywQw9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell X: Utility to load a GPTForClassification with optional LoRA/PAAF\n",
        "\n",
        "def load_checkpoint_as_classifier(\n",
        "    checkpoint_path: str,\n",
        "    num_labels: int,\n",
        "    use_lora: bool = False,\n",
        "    use_paaf: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Loads a GPTForClassification from `checkpoint_path`.\n",
        "    2) If use_lora=True, injects LoRA adapters at inference.\n",
        "    3) If use_paaf=True, assumes PAAF monkey-patch is already applied\n",
        "       to CausalSelfAttention.forward (from your earlier cell).\n",
        "    \"\"\"\n",
        "    # a) Load the saved checkpoint\n",
        "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_args = ckpt['model_args']\n",
        "\n",
        "    # b) Reconstruct GPTForClassification\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    classifier_model = GPTForClassification(gptconf, num_labels=num_labels)\n",
        "    # c) Load the state dict (non-strict so we can ignore extra keys like \"optimizer\" if present)\n",
        "    classifier_model.load_state_dict(ckpt['model'], strict=False)\n",
        "\n",
        "    # d) Move to device and set to eval\n",
        "    classifier_model.to(device)\n",
        "    classifier_model.eval()\n",
        "\n",
        "    # e) If LoRA was used during training for this checkpoint, re-inject the adapters now\n",
        "    if use_lora:\n",
        "        classifier_model = inject_lora(classifier_model, rank=8, alpha=16)\n",
        "        classifier_model.to(device)   # <--- make sure LoRA adapters are on GPU\n",
        "\n",
        "    # f) If PAAF was used during training, we assume you have already monkey-patched\n",
        "    #    CausalSelfAttention.forward in an earlier cell (so no extra code is needed here).\n",
        "    return classifier_model\n"
      ],
      "metadata": {
        "id": "_OZv9HbLRNWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load All Six Model Variants"
      ],
      "metadata": {
        "id": "nUpOlykURSE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell Y: Paths to each variant’s checkpoint (adjust filenames if yours differ)\n",
        "ckpt_base       = os.path.join(out_dir, 'ckpt_base.pt')\n",
        "ckpt_lora       = os.path.join(out_dir, 'ckpt_lora.pt')\n",
        "ckpt_paaf       = os.path.join(out_dir, 'ckpt_paaf.pt')\n",
        "ckpt_lora_paaf  = os.path.join(out_dir, 'ckpt_lora_paaf.pt')\n",
        "ckpt_dar        = os.path.join(out_dir, 'ckpt_dar.pt')\n",
        "ckpt_lora_dar   = os.path.join(out_dir, 'ckpt_lora_dar.pt')\n",
        "\n",
        "# Number of labels in AdvGLUE tasks: use 3 to cover SST-2 (2 classes), QQP (2), MNLI (3)\n",
        "num_labels = 3\n",
        "\n",
        "print(\"Loading NanoGPT (base)…\")\n",
        "model_base = load_checkpoint_as_classifier(\n",
        "    ckpt_base,\n",
        "    num_labels=num_labels,\n",
        "    use_lora=False,\n",
        "    use_paaf=False\n",
        ")\n",
        "\n",
        "print(\"Loading NanoGPT + LoRA…\")\n",
        "model_lora = load_checkpoint_as_classifier(\n",
        "    ckpt_lora,\n",
        "    num_labels=num_labels,\n",
        "    use_lora=True,\n",
        "    use_paaf=False\n",
        ")\n",
        "\n",
        "print(\"Loading NanoGPT + PAAF…\")\n",
        "model_paaf = load_checkpoint_as_classifier(\n",
        "    ckpt_paaf,\n",
        "    num_labels=num_labels,\n",
        "    use_lora=False,\n",
        "    use_paaf=True\n",
        ")\n",
        "\n",
        "print(\"Loading NanoGPT + LoRA + PAAF…\")\n",
        "model_lora_paaf = load_checkpoint_as_classifier(\n",
        "    ckpt_lora_paaf,\n",
        "    num_labels=num_labels,\n",
        "    use_lora=True,\n",
        "    use_paaf=True\n",
        ")\n",
        "\n",
        "print(\"Loading NanoGPT + DAR…\")\n",
        "model_dar = load_checkpoint_as_classifier(\n",
        "    ckpt_dar,\n",
        "    num_labels=num_labels,\n",
        "    use_lora=False,\n",
        "    use_paaf=False\n",
        ")\n",
        "\n",
        "print(\"Loading NanoGPT + LoRA + DAR…\")\n",
        "model_lora_dar = load_checkpoint_as_classifier(\n",
        "    ckpt_lora_dar,\n",
        "    num_labels=num_labels,\n",
        "    use_lora=True,\n",
        "    use_paaf=False\n",
        ")\n",
        "\n",
        "print(\"Loading NanoGPT + ARTM + LoRA\")\n",
        "model_artm_lora = load_checkpoint_as_classifier(\n",
        "    os.path.join(out_dir, 'ckpt_artm_lora.pt'),\n",
        "    num_labels=num_labels,\n",
        "    use_lora=True,  # only if that checkpoint used LoRA\n",
        "    use_paaf=False,  # if PAAF was also used\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdQhVV-qRUO9",
        "outputId": "ba0db692-a225-4de4-d689-b88541c3a99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading NanoGPT (base)…\n",
            "number of parameters: 10.66M\n",
            "Loading NanoGPT + LoRA…\n",
            "number of parameters: 10.66M\n",
            "Loading NanoGPT + PAAF…\n",
            "number of parameters: 10.66M\n",
            "Loading NanoGPT + LoRA + PAAF…\n",
            "number of parameters: 10.66M\n",
            "Loading NanoGPT + DAR…\n",
            "number of parameters: 10.66M\n",
            "Loading NanoGPT + LoRA + DAR…\n",
            "number of parameters: 10.66M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate All Variants on AdvGLUE"
      ],
      "metadata": {
        "id": "ed2w2fmiRWn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell Z: Define evaluation loop and run it for all six variants\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_advglue(model, data_loader):\n",
        "    \"\"\"\n",
        "    Returns (correct, total, accuracy) on the provided AdvGLUE DataLoader.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for input_ids, labels in data_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "        logits, _ = model(input_ids, labels=None)  # (B, num_labels)\n",
        "        preds = torch.argmax(logits, dim=-1)        # (B,)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    return correct, total, acc\n",
        "\n",
        "# List of (display name, model)\n",
        "variants = [\n",
        "    (\"NanoGPT (base)\"          , model_base),\n",
        "    (\"NanoGPT + LoRA\"          , model_lora),\n",
        "    (\"NanoGPT + PAAF\"          , model_paaf),\n",
        "    (\"NanoGPT + LoRA + PAAF\"   , model_lora_paaf),\n",
        "    (\"NanoGPT + DAR\"           , model_dar),\n",
        "    (\"NanoGPT + LoRA + DAR\"    , model_lora_dar),\n",
        "]\n",
        "\n",
        "print(\"\\n=== Evaluating All Variants on AdvGLUE ===\")\n",
        "for name, m in variants:\n",
        "    correct, total, acc = evaluate_advglue(m, adv_loader)\n",
        "    print(f\"{name:<25} → {correct:4d}/{total:4d}  = {acc*100:6.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enZ8KLnoRY4E",
        "outputId": "d04ad36e-de5a-453a-c0e7-768cd0cd26d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluating All Variants on AdvGLUE ===\n",
            "NanoGPT (base)            →  154/ 347  =  44.38%\n",
            "NanoGPT + LoRA            →  156/ 347  =  44.96%\n",
            "NanoGPT + PAAF            →   64/ 347  =  18.44%\n",
            "NanoGPT + LoRA + PAAF     →  134/ 347  =  38.62%\n",
            "NanoGPT + DAR             →  143/ 347  =  41.21%\n",
            "NanoGPT + LoRA + DAR      →  148/ 347  =  42.65%\n"
          ]
        }
      ]
    }
  ]
}